{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class tNetConfig:\n",
    "    num_vars: int\n",
    "    embedding_size: int\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    def __init__(self, config: tNetConfig):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.num_vars = config.num_vars\n",
    "        self.n_embd = config.embedding_size\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "\n",
    "        self.conv1 = nn.Conv1d(self.num_vars + 1, self.n_embd, 1)\n",
    "        self.conv2 = nn.Conv1d(self.n_embd, 2 * self.n_embd, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.n_embd, 4 * self.n_embd, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * self.n_embd, 2 * self.n_embd)\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        self.input_batch_norm = nn.GroupNorm(1, self.num_vars + 1)\n",
    "\n",
    "        self.bn1 = nn.GroupNorm(1, self.n_embd)\n",
    "        self.bn2 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn3 = nn.GroupNorm(1, 4 * self.n_embd)\n",
    "        self.bn4 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn5 = nn.GroupNorm(1, self.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features + 1, #points]\n",
    "        :return: logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.n_embd\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class TextDiffusionModel:\n",
    "    def __init__(self, vocab_size, seq_len, device=\"cpu\"):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        # self.noise_schedule = torch.linspace(0.01, 0.1, steps=1000).to(device)  # Noise variance per timestep\n",
    "        self.noise_schedule = torch.linspace(1e-4, 2e-2, steps=1000).to(device)  # Noise variance per timestep\n",
    "\n",
    "    def add_noise(self, tokens, t):\n",
    "        noise_std = self.noise_schedule[t].view(-1, 1, 1)  # Shape: (batch_size, 1, 1)\n",
    "\n",
    "        # Convert tokens to one-hot vectors\n",
    "        one_hot = F.one_hot(tokens.long(), num_classes=self.vocab_size).float()\n",
    "        \n",
    "        # Add Gaussian noise to the one-hot vectors\n",
    "        noise = torch.randn_like(one_hot) * noise_std\n",
    "        noisy_one_hot = one_hot + noise\n",
    "\n",
    "        # Compute softmax to normalize the noisy one-hot vectors\n",
    "        noisy_tokens = F.softmax(noisy_one_hot, dim=-1)\n",
    "        return noisy_tokens, noise\n",
    "\n",
    "    def sample_from_noisy_tokens(self, noisy_tokens):\n",
    "        sampled_tokens = torch.argmax(noisy_tokens, dim=-1)\n",
    "        return sampled_tokens\n",
    "\n",
    "class ReverseProcessModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_vars, seq_len):\n",
    "        super(ReverseProcessModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_vars = num_vars\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Calculate the correct input size for fc1\n",
    "        input_size = embedding_size + (seq_len * vocab_size) + 1  # embeddings + noisy_tokens + timestep\n",
    "\n",
    "        # Define layers for the reverse process model\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, seq_len * vocab_size)  # Output for all tokens in the sequence\n",
    "\n",
    "    def forward(self, noisy_tokens, embeddings, t):\n",
    "        # Flatten noisy tokens to [batch_size, seq_len * vocab_size]\n",
    "        noisy_tokens_flat = noisy_tokens.view(noisy_tokens.size(0), -1)\n",
    "\n",
    "        # Concatenate embeddings, flattened noisy tokens, and timestep information\n",
    "        timestep_embedding = torch.cat([embeddings, noisy_tokens_flat, t.unsqueeze(1).float()], dim=-1)\n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = F.relu(self.fc1(timestep_embedding))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        predicted_noise = self.fc3(x)\n",
    "\n",
    "        # Reshape to [batch_size, seq_len, vocab_size]\n",
    "        predicted_noise = predicted_noise.view(-1, self.seq_len, self.vocab_size)\n",
    "        \n",
    "        return predicted_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenize Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize a formula\n",
    "def tokenize_formula(formula):\n",
    "    token_pattern = r\"[a-zA-Z_][a-zA-Z0-9_]*|[()+\\-*/]|\\d+\\.?\\d*\"\n",
    "    tokens = re.findall(token_pattern, formula)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "folder_path = \"data_symbolic_regression/train\"\n",
    "val_folder_path = \"data_symbolic_regression/val\"\n",
    "\n",
    "# Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "tokenized_formulas = []\n",
    "points_list = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "            if formula_human_readable:\n",
    "                tokens = tokenize_formula(formula_human_readable)\n",
    "                tokenized_formulas.append(tokens)\n",
    "            \n",
    "            points = data.get(\"points\")\n",
    "            if points:\n",
    "                points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                points_list.append(points_tensor)\n",
    "\n",
    "val_tokenized_formulas = []\n",
    "val_points_list = []\n",
    "\n",
    "# Create the vocabulary from the tokens\n",
    "for file_name in os.listdir(val_folder_path):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(val_folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            val_data = json.load(file)\n",
    "\n",
    "            val_formula_human_readable = val_data.get(\"formula_human_readable\", \"\")\n",
    "            if val_formula_human_readable:\n",
    "                val_tokens = tokenize_formula(val_formula_human_readable)\n",
    "                val_tokenized_formulas.append(val_tokens)\n",
    "            \n",
    "            val_points = val_data.get(\"points\")\n",
    "            if val_points:\n",
    "                val_points_array = np.array([val_points[\"var_0\"], val_points[\"var_1\"], val_points[\"var_2\"], val_points[\"target\"]])\n",
    "                val_points_tensor = torch.tensor(val_points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                val_points_list.append(val_points_tensor)\n",
    "\n",
    "all_tokenized_formulas = tokenized_formulas + val_tokenized_formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in all_tokenized_formulas for t in tokens))}\n",
    "vocab_size = len(vocab_mapping)\n",
    "\n",
    "# Define EOS and PAD token IDs\n",
    "eos_token_id = vocab_size  # Assuming the last ID in the vocabulary is for EOS\n",
    "pad_token_id = vocab_size + 1  # Assuming the second-to-last ID in the vocabulary is for PAD\n",
    "\n",
    "# Add EOS and PAD tokens to vocab_mapping if not already present\n",
    "if eos_token_id not in vocab_mapping.values():\n",
    "    vocab_mapping['<EOS>'] = eos_token_id\n",
    "if pad_token_id not in vocab_mapping.values():\n",
    "    vocab_mapping['<PAD>'] = pad_token_id\n",
    "\n",
    "vocab_size = len(vocab_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and map tokens to vocabulary indices\n",
    "token_sequences = [[vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in tokenized_formulas]\n",
    "\n",
    "# Calculate sequence length based on the 95th percentile of formula lengths\n",
    "formula_lengths = [len(tokens) for tokens in tokenized_formulas]\n",
    "seq_len = int(np.percentile(formula_lengths, 100)) + 1 # Use 95th percentile\n",
    "batch_size = 100  # Example batch size\n",
    "\n",
    "# Pad or truncate sequences to seq_len, adding EOS token last\n",
    "token_sequences = [\n",
    "    seq[:seq_len] + [eos_token_id] + [pad_token_id]*max(0, seq_len - len(seq)) \n",
    "    if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "    for seq in token_sequences\n",
    "]\n",
    "\n",
    "# Convert to tensor\n",
    "token_tensor = torch.tensor(token_sequences, device=device)\n",
    "\n",
    "# Pad or truncate sequences to seq_len\n",
    "token_sequences = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in token_sequences]\n",
    "token_tensor = torch.tensor(token_sequences, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and map tokens to vocabulary indices\n",
    "val_token_sequences = [[vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in val_tokenized_formulas]\n",
    "\n",
    "# Calculate sequence length based on the 95th percentile of formula lengths\n",
    "val_formula_lengths = [len(tokens) for tokens in val_tokenized_formulas]\n",
    "val_seq_len = int(np.percentile(val_formula_lengths, 100))  # Use 95th percentile\n",
    "\n",
    "# Pad or truncate sequences to seq_len, adding EOS token last\n",
    "val_token_sequences = [\n",
    "    seq[:seq_len] + [eos_token_id] + [pad_token_id]*max(0, seq_len - len(seq)) \n",
    "    if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "    for seq in val_token_sequences\n",
    "]\n",
    "\n",
    "# Convert to tensor\n",
    "val_token_tensor = torch.tensor(val_token_sequences, device=device)\n",
    "\n",
    "# Pad or truncate sequences to seq_len\n",
    "val_token_sequences = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in val_token_sequences]\n",
    "val_token_tensor = torch.tensor(val_token_sequences, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([747, 4, 100])\n",
      "Output shape: torch.Size([747, 128])\n",
      "Original Tokens shape: torch.Size([747, 28])\n",
      "Noisy Tokens (probabilities) shape: torch.Size([747, 28, 25])\n",
      "Sampled Tokens shape: torch.Size([747, 28])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "diffusion_model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "\n",
    "# Choose random timesteps for each sequence\n",
    "t = torch.randint(0, 1000, (len(token_tensor),), device=device)\n",
    "\n",
    "# Add noise to the tokens\n",
    "noisy_tokens, noise = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "# Sample from noisy tokens\n",
    "sampled_tokens = diffusion_model.sample_from_noisy_tokens(noisy_tokens)\n",
    "\n",
    "# Configuration for tNet\n",
    "num_vars = 3\n",
    "embedding_size = 128  # Example embedding size\n",
    "config = tNetConfig(num_vars=num_vars, embedding_size=embedding_size)\n",
    "\n",
    "# Instantiate the model\n",
    "tnet_model = tNet(config)\n",
    "\n",
    "# Input: batch_size x (num_vars + 1) x num_points\n",
    "batch_size = 1\n",
    "\n",
    "# Generate embeddings\n",
    "# input_tensor = torch.rand(batch_size, num_vars, 100)\n",
    "\n",
    "output_embeddings = []\n",
    "for pt in points_list:\n",
    "    output_embedding = tnet_model(pt)\n",
    "    output_embeddings.append(output_embedding)\n",
    "\n",
    "points_tensors = torch.cat(points_list, dim=0)\n",
    "\n",
    "output_embeddings_tensor = torch.cat(output_embeddings, dim=0)\n",
    "# Print the output\n",
    "print(\"Input shape:\", points_tensors.shape)\n",
    "print(\"Output shape:\", output_embeddings_tensor.shape)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Tokens shape:\", token_tensor.shape)\n",
    "print(\"Noisy Tokens (probabilities) shape:\", noisy_tokens.shape)\n",
    "print(\"Sampled Tokens shape:\", sampled_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reverse model (denoiser)\n",
    "reverse_model = ReverseProcessModel(vocab_size, embedding_size, num_vars, seq_len).to(device)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the reverse process model\n",
    "epochs = 1000  # Define the number of epochs for training\n",
    "batch_size = 100  # Example batch size\n",
    "\n",
    "# Optimizer for the reverse process model\n",
    "optimizer = torch.optim.Adam(reverse_model.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.AdamW(reverse_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=2,factor=0.5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "training_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.0427, Val Loss: 0.0412\n",
      "Epoch [2/1000], Loss: 0.0314, Val Loss: 0.0327\n",
      "Epoch [3/1000], Loss: 0.0264, Val Loss: 0.0295\n",
      "Epoch [4/1000], Loss: 0.0240, Val Loss: 0.0270\n",
      "Epoch [5/1000], Loss: 0.0219, Val Loss: 0.0252\n",
      "Epoch [6/1000], Loss: 0.0206, Val Loss: 0.0237\n",
      "Epoch [7/1000], Loss: 0.0199, Val Loss: 0.0239\n",
      "Epoch [8/1000], Loss: 0.0194, Val Loss: 0.0219\n",
      "Epoch [9/1000], Loss: 0.0187, Val Loss: 0.0214\n",
      "Epoch [10/1000], Loss: 0.0182, Val Loss: 0.0209\n",
      "Epoch [11/1000], Loss: 0.0180, Val Loss: 0.0210\n",
      "Epoch [12/1000], Loss: 0.0178, Val Loss: 0.0206\n",
      "Epoch [13/1000], Loss: 0.0172, Val Loss: 0.0203\n",
      "Epoch [14/1000], Loss: 0.0169, Val Loss: 0.0196\n",
      "Epoch [15/1000], Loss: 0.0168, Val Loss: 0.0198\n",
      "Epoch [16/1000], Loss: 0.0164, Val Loss: 0.0191\n",
      "Epoch [17/1000], Loss: 0.0162, Val Loss: 0.0192\n",
      "Epoch [18/1000], Loss: 0.0161, Val Loss: 0.0189\n",
      "Epoch [19/1000], Loss: 0.0161, Val Loss: 0.0188\n",
      "Epoch [20/1000], Loss: 0.0160, Val Loss: 0.0187\n",
      "Epoch [21/1000], Loss: 0.0159, Val Loss: 0.0187\n",
      "Epoch [22/1000], Loss: 0.0159, Val Loss: 0.0185\n",
      "Epoch [23/1000], Loss: 0.0158, Val Loss: 0.0185\n",
      "Epoch [24/1000], Loss: 0.0158, Val Loss: 0.0186\n",
      "Epoch [25/1000], Loss: 0.0158, Val Loss: 0.0185\n",
      "Epoch [26/1000], Loss: 0.0158, Val Loss: 0.0186\n",
      "Epoch [27/1000], Loss: 0.0158, Val Loss: 0.0185\n",
      "Epoch [28/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [29/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [30/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [31/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [32/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [33/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [34/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [35/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [36/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [37/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [38/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [39/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [40/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [41/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [42/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [43/1000], Loss: 0.0157, Val Loss: 0.0185\n",
      "Epoch [44/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [45/1000], Loss: 0.0157, Val Loss: 0.0184\n",
      "Epoch [46/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [47/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [48/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [49/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [50/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [51/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [52/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [53/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [54/1000], Loss: 0.0156, Val Loss: 0.0183\n",
      "Epoch [55/1000], Loss: 0.0156, Val Loss: 0.0183\n",
      "Epoch [56/1000], Loss: 0.0156, Val Loss: 0.0184\n",
      "Epoch [57/1000], Loss: 0.0156, Val Loss: 0.0183\n",
      "Epoch [58/1000], Loss: 0.0156, Val Loss: 0.0183\n",
      "Epoch [59/1000], Loss: 0.0156, Val Loss: 0.0183\n",
      "Epoch [60/1000], Loss: 0.0156, Val Loss: 0.0183\n",
      "Epoch [61/1000], Loss: 0.0156, Val Loss: 0.0183\n",
      "Epoch [62/1000], Loss: 0.0155, Val Loss: 0.0183\n",
      "Epoch [63/1000], Loss: 0.0155, Val Loss: 0.0183\n",
      "Epoch [64/1000], Loss: 0.0155, Val Loss: 0.0183\n",
      "Epoch [65/1000], Loss: 0.0155, Val Loss: 0.0183\n",
      "Epoch [66/1000], Loss: 0.0155, Val Loss: 0.0183\n",
      "Epoch [67/1000], Loss: 0.0155, Val Loss: 0.0182\n",
      "Epoch [68/1000], Loss: 0.0155, Val Loss: 0.0182\n",
      "Epoch [69/1000], Loss: 0.0155, Val Loss: 0.0183\n",
      "Epoch [70/1000], Loss: 0.0155, Val Loss: 0.0182\n",
      "Epoch [71/1000], Loss: 0.0155, Val Loss: 0.0182\n",
      "Epoch [72/1000], Loss: 0.0155, Val Loss: 0.0182\n",
      "Epoch [73/1000], Loss: 0.0155, Val Loss: 0.0182\n",
      "Epoch [74/1000], Loss: 0.0154, Val Loss: 0.0182\n",
      "Epoch [75/1000], Loss: 0.0154, Val Loss: 0.0182\n",
      "Epoch [76/1000], Loss: 0.0154, Val Loss: 0.0181\n",
      "Epoch [77/1000], Loss: 0.0154, Val Loss: 0.0182\n",
      "Epoch [78/1000], Loss: 0.0154, Val Loss: 0.0181\n",
      "Epoch [79/1000], Loss: 0.0154, Val Loss: 0.0181\n",
      "Epoch [80/1000], Loss: 0.0154, Val Loss: 0.0181\n",
      "Epoch [81/1000], Loss: 0.0154, Val Loss: 0.0181\n",
      "Epoch [82/1000], Loss: 0.0154, Val Loss: 0.0181\n",
      "Epoch [83/1000], Loss: 0.0153, Val Loss: 0.0181\n",
      "Epoch [84/1000], Loss: 0.0153, Val Loss: 0.0180\n",
      "Epoch [85/1000], Loss: 0.0153, Val Loss: 0.0180\n",
      "Epoch [86/1000], Loss: 0.0153, Val Loss: 0.0180\n",
      "Epoch [87/1000], Loss: 0.0153, Val Loss: 0.0180\n",
      "Epoch [88/1000], Loss: 0.0152, Val Loss: 0.0179\n",
      "Epoch [89/1000], Loss: 0.0152, Val Loss: 0.0179\n",
      "Epoch [90/1000], Loss: 0.0152, Val Loss: 0.0179\n",
      "Epoch [91/1000], Loss: 0.0152, Val Loss: 0.0179\n",
      "Epoch [92/1000], Loss: 0.0151, Val Loss: 0.0178\n",
      "Epoch [93/1000], Loss: 0.0151, Val Loss: 0.0178\n",
      "Epoch [94/1000], Loss: 0.0151, Val Loss: 0.0178\n",
      "Epoch [95/1000], Loss: 0.0151, Val Loss: 0.0177\n",
      "Epoch [96/1000], Loss: 0.0151, Val Loss: 0.0178\n",
      "Epoch [97/1000], Loss: 0.0150, Val Loss: 0.0177\n",
      "Epoch [98/1000], Loss: 0.0150, Val Loss: 0.0176\n",
      "Epoch [99/1000], Loss: 0.0150, Val Loss: 0.0177\n",
      "Epoch [100/1000], Loss: 0.0150, Val Loss: 0.0176\n",
      "Epoch [101/1000], Loss: 0.0149, Val Loss: 0.0175\n",
      "Epoch [102/1000], Loss: 0.0149, Val Loss: 0.0176\n",
      "Epoch [103/1000], Loss: 0.0149, Val Loss: 0.0174\n",
      "Epoch [104/1000], Loss: 0.0148, Val Loss: 0.0174\n",
      "Epoch [105/1000], Loss: 0.0148, Val Loss: 0.0174\n",
      "Epoch [106/1000], Loss: 0.0148, Val Loss: 0.0172\n",
      "Epoch [107/1000], Loss: 0.0147, Val Loss: 0.0174\n",
      "Epoch [108/1000], Loss: 0.0147, Val Loss: 0.0172\n",
      "Epoch [109/1000], Loss: 0.0146, Val Loss: 0.0172\n",
      "Epoch [110/1000], Loss: 0.0146, Val Loss: 0.0171\n",
      "Epoch [111/1000], Loss: 0.0145, Val Loss: 0.0170\n",
      "Epoch [112/1000], Loss: 0.0145, Val Loss: 0.0169\n",
      "Epoch [113/1000], Loss: 0.0145, Val Loss: 0.0169\n",
      "Epoch [114/1000], Loss: 0.0144, Val Loss: 0.0169\n",
      "Epoch [115/1000], Loss: 0.0144, Val Loss: 0.0168\n",
      "Epoch [116/1000], Loss: 0.0143, Val Loss: 0.0167\n",
      "Epoch [117/1000], Loss: 0.0143, Val Loss: 0.0167\n",
      "Epoch [118/1000], Loss: 0.0143, Val Loss: 0.0166\n",
      "Epoch [119/1000], Loss: 0.0142, Val Loss: 0.0166\n",
      "Epoch [120/1000], Loss: 0.0141, Val Loss: 0.0165\n",
      "Epoch [121/1000], Loss: 0.0141, Val Loss: 0.0164\n",
      "Epoch [122/1000], Loss: 0.0140, Val Loss: 0.0163\n",
      "Epoch [123/1000], Loss: 0.0139, Val Loss: 0.0163\n",
      "Epoch [124/1000], Loss: 0.0139, Val Loss: 0.0163\n",
      "Epoch [125/1000], Loss: 0.0139, Val Loss: 0.0162\n",
      "Epoch [126/1000], Loss: 0.0138, Val Loss: 0.0161\n",
      "Epoch [127/1000], Loss: 0.0137, Val Loss: 0.0160\n",
      "Epoch [128/1000], Loss: 0.0137, Val Loss: 0.0160\n",
      "Epoch [129/1000], Loss: 0.0137, Val Loss: 0.0159\n",
      "Epoch [130/1000], Loss: 0.0135, Val Loss: 0.0158\n",
      "Epoch [131/1000], Loss: 0.0135, Val Loss: 0.0159\n",
      "Epoch [132/1000], Loss: 0.0134, Val Loss: 0.0158\n",
      "Epoch [133/1000], Loss: 0.0133, Val Loss: 0.0157\n",
      "Epoch [134/1000], Loss: 0.0132, Val Loss: 0.0156\n",
      "Epoch [135/1000], Loss: 0.0132, Val Loss: 0.0155\n",
      "Epoch [136/1000], Loss: 0.0131, Val Loss: 0.0154\n",
      "Epoch [137/1000], Loss: 0.0131, Val Loss: 0.0154\n",
      "Epoch [138/1000], Loss: 0.0130, Val Loss: 0.0153\n",
      "Epoch [139/1000], Loss: 0.0129, Val Loss: 0.0152\n",
      "Epoch [140/1000], Loss: 0.0129, Val Loss: 0.0151\n",
      "Epoch [141/1000], Loss: 0.0128, Val Loss: 0.0151\n",
      "Epoch [142/1000], Loss: 0.0127, Val Loss: 0.0150\n",
      "Epoch [143/1000], Loss: 0.0127, Val Loss: 0.0149\n",
      "Epoch [144/1000], Loss: 0.0126, Val Loss: 0.0149\n",
      "Epoch [145/1000], Loss: 0.0126, Val Loss: 0.0148\n",
      "Epoch [146/1000], Loss: 0.0126, Val Loss: 0.0149\n",
      "Epoch [147/1000], Loss: 0.0125, Val Loss: 0.0147\n",
      "Epoch [148/1000], Loss: 0.0124, Val Loss: 0.0146\n",
      "Epoch [149/1000], Loss: 0.0124, Val Loss: 0.0146\n",
      "Epoch [150/1000], Loss: 0.0123, Val Loss: 0.0145\n",
      "Epoch [151/1000], Loss: 0.0123, Val Loss: 0.0145\n",
      "Epoch [152/1000], Loss: 0.0122, Val Loss: 0.0144\n",
      "Epoch [153/1000], Loss: 0.0122, Val Loss: 0.0144\n",
      "Epoch [154/1000], Loss: 0.0121, Val Loss: 0.0143\n",
      "Epoch [155/1000], Loss: 0.0121, Val Loss: 0.0143\n",
      "Epoch [156/1000], Loss: 0.0121, Val Loss: 0.0142\n",
      "Epoch [157/1000], Loss: 0.0120, Val Loss: 0.0142\n",
      "Epoch [158/1000], Loss: 0.0120, Val Loss: 0.0142\n",
      "Epoch [159/1000], Loss: 0.0120, Val Loss: 0.0141\n",
      "Epoch [160/1000], Loss: 0.0119, Val Loss: 0.0141\n",
      "Epoch [161/1000], Loss: 0.0119, Val Loss: 0.0140\n",
      "Epoch [162/1000], Loss: 0.0118, Val Loss: 0.0140\n",
      "Epoch [163/1000], Loss: 0.0118, Val Loss: 0.0139\n",
      "Epoch [164/1000], Loss: 0.0118, Val Loss: 0.0139\n",
      "Epoch [165/1000], Loss: 0.0118, Val Loss: 0.0139\n",
      "Epoch [166/1000], Loss: 0.0117, Val Loss: 0.0138\n",
      "Epoch [167/1000], Loss: 0.0117, Val Loss: 0.0138\n",
      "Epoch [168/1000], Loss: 0.0117, Val Loss: 0.0138\n",
      "Epoch [169/1000], Loss: 0.0117, Val Loss: 0.0138\n",
      "Epoch [170/1000], Loss: 0.0116, Val Loss: 0.0137\n",
      "Epoch [171/1000], Loss: 0.0116, Val Loss: 0.0137\n",
      "Epoch [172/1000], Loss: 0.0116, Val Loss: 0.0137\n",
      "Epoch [173/1000], Loss: 0.0116, Val Loss: 0.0137\n",
      "Epoch [174/1000], Loss: 0.0115, Val Loss: 0.0136\n",
      "Epoch [175/1000], Loss: 0.0115, Val Loss: 0.0136\n",
      "Epoch [176/1000], Loss: 0.0115, Val Loss: 0.0136\n",
      "Epoch [177/1000], Loss: 0.0115, Val Loss: 0.0136\n",
      "Epoch [178/1000], Loss: 0.0115, Val Loss: 0.0136\n",
      "Epoch [179/1000], Loss: 0.0114, Val Loss: 0.0135\n",
      "Epoch [180/1000], Loss: 0.0114, Val Loss: 0.0135\n",
      "Epoch [181/1000], Loss: 0.0114, Val Loss: 0.0135\n",
      "Epoch [182/1000], Loss: 0.0114, Val Loss: 0.0135\n",
      "Epoch [183/1000], Loss: 0.0114, Val Loss: 0.0135\n",
      "Epoch [184/1000], Loss: 0.0113, Val Loss: 0.0134\n",
      "Epoch [185/1000], Loss: 0.0113, Val Loss: 0.0134\n",
      "Epoch [186/1000], Loss: 0.0113, Val Loss: 0.0134\n",
      "Epoch [187/1000], Loss: 0.0113, Val Loss: 0.0134\n",
      "Epoch [188/1000], Loss: 0.0113, Val Loss: 0.0134\n",
      "Epoch [189/1000], Loss: 0.0112, Val Loss: 0.0134\n",
      "Epoch [190/1000], Loss: 0.0112, Val Loss: 0.0133\n",
      "Epoch [191/1000], Loss: 0.0112, Val Loss: 0.0133\n",
      "Epoch [192/1000], Loss: 0.0112, Val Loss: 0.0133\n",
      "Epoch [193/1000], Loss: 0.0112, Val Loss: 0.0132\n",
      "Epoch [194/1000], Loss: 0.0112, Val Loss: 0.0133\n",
      "Epoch [195/1000], Loss: 0.0111, Val Loss: 0.0132\n",
      "Epoch [196/1000], Loss: 0.0111, Val Loss: 0.0132\n",
      "Epoch [197/1000], Loss: 0.0111, Val Loss: 0.0132\n",
      "Epoch [198/1000], Loss: 0.0111, Val Loss: 0.0132\n",
      "Epoch [199/1000], Loss: 0.0111, Val Loss: 0.0132\n",
      "Epoch [200/1000], Loss: 0.0111, Val Loss: 0.0131\n",
      "Epoch [201/1000], Loss: 0.0110, Val Loss: 0.0131\n",
      "Epoch [202/1000], Loss: 0.0110, Val Loss: 0.0132\n",
      "Epoch [203/1000], Loss: 0.0110, Val Loss: 0.0131\n",
      "Epoch [204/1000], Loss: 0.0110, Val Loss: 0.0131\n",
      "Epoch [205/1000], Loss: 0.0110, Val Loss: 0.0131\n",
      "Epoch [206/1000], Loss: 0.0110, Val Loss: 0.0131\n",
      "Epoch [207/1000], Loss: 0.0110, Val Loss: 0.0130\n",
      "Epoch [208/1000], Loss: 0.0110, Val Loss: 0.0130\n",
      "Epoch [209/1000], Loss: 0.0109, Val Loss: 0.0131\n",
      "Epoch [210/1000], Loss: 0.0109, Val Loss: 0.0130\n",
      "Epoch [211/1000], Loss: 0.0109, Val Loss: 0.0130\n",
      "Epoch [212/1000], Loss: 0.0109, Val Loss: 0.0130\n",
      "Epoch [213/1000], Loss: 0.0109, Val Loss: 0.0130\n",
      "Epoch [214/1000], Loss: 0.0109, Val Loss: 0.0129\n",
      "Epoch [215/1000], Loss: 0.0109, Val Loss: 0.0129\n",
      "Epoch [216/1000], Loss: 0.0108, Val Loss: 0.0129\n",
      "Epoch [217/1000], Loss: 0.0108, Val Loss: 0.0129\n",
      "Epoch [218/1000], Loss: 0.0108, Val Loss: 0.0129\n",
      "Epoch [219/1000], Loss: 0.0108, Val Loss: 0.0128\n",
      "Epoch [220/1000], Loss: 0.0108, Val Loss: 0.0128\n",
      "Epoch [221/1000], Loss: 0.0108, Val Loss: 0.0128\n",
      "Epoch [222/1000], Loss: 0.0108, Val Loss: 0.0128\n",
      "Epoch [223/1000], Loss: 0.0107, Val Loss: 0.0128\n",
      "Epoch [224/1000], Loss: 0.0107, Val Loss: 0.0128\n",
      "Epoch [225/1000], Loss: 0.0107, Val Loss: 0.0127\n",
      "Epoch [226/1000], Loss: 0.0107, Val Loss: 0.0128\n",
      "Epoch [227/1000], Loss: 0.0107, Val Loss: 0.0128\n",
      "Epoch [228/1000], Loss: 0.0107, Val Loss: 0.0127\n",
      "Epoch [229/1000], Loss: 0.0106, Val Loss: 0.0127\n",
      "Epoch [230/1000], Loss: 0.0106, Val Loss: 0.0127\n",
      "Epoch [231/1000], Loss: 0.0106, Val Loss: 0.0127\n",
      "Epoch [232/1000], Loss: 0.0106, Val Loss: 0.0126\n",
      "Epoch [233/1000], Loss: 0.0106, Val Loss: 0.0126\n",
      "Epoch [234/1000], Loss: 0.0106, Val Loss: 0.0126\n",
      "Epoch [235/1000], Loss: 0.0105, Val Loss: 0.0126\n",
      "Epoch [236/1000], Loss: 0.0105, Val Loss: 0.0126\n",
      "Epoch [237/1000], Loss: 0.0105, Val Loss: 0.0126\n",
      "Epoch [238/1000], Loss: 0.0105, Val Loss: 0.0126\n",
      "Epoch [239/1000], Loss: 0.0105, Val Loss: 0.0126\n",
      "Epoch [240/1000], Loss: 0.0105, Val Loss: 0.0126\n",
      "Epoch [241/1000], Loss: 0.0105, Val Loss: 0.0125\n",
      "Epoch [242/1000], Loss: 0.0104, Val Loss: 0.0125\n",
      "Epoch [243/1000], Loss: 0.0104, Val Loss: 0.0125\n",
      "Epoch [244/1000], Loss: 0.0104, Val Loss: 0.0124\n",
      "Epoch [245/1000], Loss: 0.0104, Val Loss: 0.0124\n",
      "Epoch [246/1000], Loss: 0.0104, Val Loss: 0.0124\n",
      "Epoch [247/1000], Loss: 0.0104, Val Loss: 0.0124\n",
      "Epoch [248/1000], Loss: 0.0103, Val Loss: 0.0124\n",
      "Epoch [249/1000], Loss: 0.0103, Val Loss: 0.0123\n",
      "Epoch [250/1000], Loss: 0.0103, Val Loss: 0.0124\n",
      "Epoch [251/1000], Loss: 0.0103, Val Loss: 0.0123\n",
      "Epoch [252/1000], Loss: 0.0102, Val Loss: 0.0123\n",
      "Epoch [253/1000], Loss: 0.0102, Val Loss: 0.0123\n",
      "Epoch [254/1000], Loss: 0.0102, Val Loss: 0.0123\n",
      "Epoch [255/1000], Loss: 0.0102, Val Loss: 0.0122\n",
      "Epoch [256/1000], Loss: 0.0102, Val Loss: 0.0123\n",
      "Epoch [257/1000], Loss: 0.0102, Val Loss: 0.0123\n",
      "Epoch [258/1000], Loss: 0.0102, Val Loss: 0.0122\n",
      "Epoch [259/1000], Loss: 0.0101, Val Loss: 0.0121\n",
      "Epoch [260/1000], Loss: 0.0101, Val Loss: 0.0121\n",
      "Epoch [261/1000], Loss: 0.0101, Val Loss: 0.0121\n",
      "Epoch [262/1000], Loss: 0.0101, Val Loss: 0.0121\n",
      "Epoch [263/1000], Loss: 0.0100, Val Loss: 0.0120\n",
      "Epoch [264/1000], Loss: 0.0100, Val Loss: 0.0120\n",
      "Epoch [265/1000], Loss: 0.0100, Val Loss: 0.0120\n",
      "Epoch [266/1000], Loss: 0.0100, Val Loss: 0.0120\n",
      "Epoch [267/1000], Loss: 0.0100, Val Loss: 0.0120\n",
      "Epoch [268/1000], Loss: 0.0100, Val Loss: 0.0119\n",
      "Epoch [269/1000], Loss: 0.0099, Val Loss: 0.0120\n",
      "Epoch [270/1000], Loss: 0.0099, Val Loss: 0.0119\n",
      "Epoch [271/1000], Loss: 0.0099, Val Loss: 0.0119\n",
      "Epoch [272/1000], Loss: 0.0099, Val Loss: 0.0119\n",
      "Epoch [273/1000], Loss: 0.0099, Val Loss: 0.0119\n",
      "Epoch [274/1000], Loss: 0.0098, Val Loss: 0.0119\n",
      "Epoch [275/1000], Loss: 0.0098, Val Loss: 0.0118\n",
      "Epoch [276/1000], Loss: 0.0098, Val Loss: 0.0117\n",
      "Epoch [277/1000], Loss: 0.0097, Val Loss: 0.0118\n",
      "Epoch [278/1000], Loss: 0.0097, Val Loss: 0.0117\n",
      "Epoch [279/1000], Loss: 0.0097, Val Loss: 0.0117\n",
      "Epoch [280/1000], Loss: 0.0097, Val Loss: 0.0117\n",
      "Epoch [281/1000], Loss: 0.0097, Val Loss: 0.0117\n",
      "Epoch [282/1000], Loss: 0.0096, Val Loss: 0.0117\n",
      "Epoch [283/1000], Loss: 0.0096, Val Loss: 0.0116\n",
      "Epoch [284/1000], Loss: 0.0096, Val Loss: 0.0116\n",
      "Epoch [285/1000], Loss: 0.0096, Val Loss: 0.0115\n",
      "Epoch [286/1000], Loss: 0.0095, Val Loss: 0.0115\n",
      "Epoch [287/1000], Loss: 0.0096, Val Loss: 0.0114\n",
      "Epoch [288/1000], Loss: 0.0095, Val Loss: 0.0114\n",
      "Epoch [289/1000], Loss: 0.0095, Val Loss: 0.0114\n",
      "Epoch [290/1000], Loss: 0.0095, Val Loss: 0.0114\n",
      "Epoch [291/1000], Loss: 0.0094, Val Loss: 0.0114\n",
      "Epoch [292/1000], Loss: 0.0094, Val Loss: 0.0114\n",
      "Epoch [293/1000], Loss: 0.0094, Val Loss: 0.0113\n",
      "Epoch [294/1000], Loss: 0.0094, Val Loss: 0.0113\n",
      "Epoch [295/1000], Loss: 0.0094, Val Loss: 0.0113\n",
      "Epoch [296/1000], Loss: 0.0094, Val Loss: 0.0113\n",
      "Epoch [297/1000], Loss: 0.0093, Val Loss: 0.0112\n",
      "Epoch [298/1000], Loss: 0.0093, Val Loss: 0.0112\n",
      "Epoch [299/1000], Loss: 0.0093, Val Loss: 0.0112\n",
      "Epoch [300/1000], Loss: 0.0093, Val Loss: 0.0112\n",
      "Epoch [301/1000], Loss: 0.0093, Val Loss: 0.0111\n",
      "Epoch [302/1000], Loss: 0.0092, Val Loss: 0.0111\n",
      "Epoch [303/1000], Loss: 0.0092, Val Loss: 0.0111\n",
      "Epoch [304/1000], Loss: 0.0092, Val Loss: 0.0111\n",
      "Epoch [305/1000], Loss: 0.0092, Val Loss: 0.0111\n",
      "Epoch [306/1000], Loss: 0.0091, Val Loss: 0.0111\n",
      "Epoch [307/1000], Loss: 0.0091, Val Loss: 0.0111\n",
      "Epoch [308/1000], Loss: 0.0091, Val Loss: 0.0111\n",
      "Epoch [309/1000], Loss: 0.0091, Val Loss: 0.0110\n",
      "Epoch [310/1000], Loss: 0.0091, Val Loss: 0.0110\n",
      "Epoch [311/1000], Loss: 0.0091, Val Loss: 0.0110\n",
      "Epoch [312/1000], Loss: 0.0090, Val Loss: 0.0109\n",
      "Epoch [313/1000], Loss: 0.0090, Val Loss: 0.0109\n",
      "Epoch [314/1000], Loss: 0.0090, Val Loss: 0.0109\n",
      "Epoch [315/1000], Loss: 0.0090, Val Loss: 0.0109\n",
      "Epoch [316/1000], Loss: 0.0089, Val Loss: 0.0108\n",
      "Epoch [317/1000], Loss: 0.0089, Val Loss: 0.0108\n",
      "Epoch [318/1000], Loss: 0.0089, Val Loss: 0.0108\n",
      "Epoch [319/1000], Loss: 0.0089, Val Loss: 0.0108\n",
      "Epoch [320/1000], Loss: 0.0088, Val Loss: 0.0107\n",
      "Epoch [321/1000], Loss: 0.0088, Val Loss: 0.0107\n",
      "Epoch [322/1000], Loss: 0.0088, Val Loss: 0.0107\n",
      "Epoch [323/1000], Loss: 0.0088, Val Loss: 0.0107\n",
      "Epoch [324/1000], Loss: 0.0088, Val Loss: 0.0107\n",
      "Epoch [325/1000], Loss: 0.0088, Val Loss: 0.0106\n",
      "Epoch [326/1000], Loss: 0.0087, Val Loss: 0.0106\n",
      "Epoch [327/1000], Loss: 0.0087, Val Loss: 0.0106\n",
      "Epoch [328/1000], Loss: 0.0087, Val Loss: 0.0106\n",
      "Epoch [329/1000], Loss: 0.0087, Val Loss: 0.0106\n",
      "Epoch [330/1000], Loss: 0.0087, Val Loss: 0.0106\n",
      "Epoch [331/1000], Loss: 0.0086, Val Loss: 0.0105\n",
      "Epoch [332/1000], Loss: 0.0086, Val Loss: 0.0105\n",
      "Epoch [333/1000], Loss: 0.0086, Val Loss: 0.0105\n",
      "Epoch [334/1000], Loss: 0.0086, Val Loss: 0.0105\n",
      "Epoch [335/1000], Loss: 0.0086, Val Loss: 0.0105\n",
      "Epoch [336/1000], Loss: 0.0086, Val Loss: 0.0104\n",
      "Epoch [337/1000], Loss: 0.0086, Val Loss: 0.0104\n",
      "Epoch [338/1000], Loss: 0.0085, Val Loss: 0.0104\n",
      "Epoch [339/1000], Loss: 0.0085, Val Loss: 0.0104\n",
      "Epoch [340/1000], Loss: 0.0085, Val Loss: 0.0104\n",
      "Epoch [341/1000], Loss: 0.0085, Val Loss: 0.0104\n",
      "Epoch [342/1000], Loss: 0.0085, Val Loss: 0.0104\n",
      "Epoch [343/1000], Loss: 0.0085, Val Loss: 0.0104\n",
      "Epoch [344/1000], Loss: 0.0085, Val Loss: 0.0103\n",
      "Epoch [345/1000], Loss: 0.0085, Val Loss: 0.0104\n",
      "Epoch [346/1000], Loss: 0.0084, Val Loss: 0.0103\n",
      "Epoch [347/1000], Loss: 0.0084, Val Loss: 0.0103\n",
      "Epoch [348/1000], Loss: 0.0084, Val Loss: 0.0103\n",
      "Epoch [349/1000], Loss: 0.0084, Val Loss: 0.0103\n",
      "Epoch [350/1000], Loss: 0.0084, Val Loss: 0.0103\n",
      "Epoch [351/1000], Loss: 0.0084, Val Loss: 0.0102\n",
      "Epoch [352/1000], Loss: 0.0083, Val Loss: 0.0102\n",
      "Epoch [353/1000], Loss: 0.0083, Val Loss: 0.0102\n",
      "Epoch [354/1000], Loss: 0.0083, Val Loss: 0.0102\n",
      "Epoch [355/1000], Loss: 0.0083, Val Loss: 0.0102\n",
      "Epoch [356/1000], Loss: 0.0083, Val Loss: 0.0102\n",
      "Epoch [357/1000], Loss: 0.0083, Val Loss: 0.0101\n",
      "Epoch [358/1000], Loss: 0.0083, Val Loss: 0.0102\n",
      "Epoch [359/1000], Loss: 0.0083, Val Loss: 0.0101\n",
      "Epoch [360/1000], Loss: 0.0082, Val Loss: 0.0101\n",
      "Epoch [361/1000], Loss: 0.0082, Val Loss: 0.0101\n",
      "Epoch [362/1000], Loss: 0.0082, Val Loss: 0.0101\n",
      "Epoch [363/1000], Loss: 0.0082, Val Loss: 0.0101\n",
      "Epoch [364/1000], Loss: 0.0082, Val Loss: 0.0101\n",
      "Epoch [365/1000], Loss: 0.0082, Val Loss: 0.0100\n",
      "Epoch [366/1000], Loss: 0.0082, Val Loss: 0.0100\n",
      "Epoch [367/1000], Loss: 0.0082, Val Loss: 0.0100\n",
      "Epoch [368/1000], Loss: 0.0082, Val Loss: 0.0100\n",
      "Epoch [369/1000], Loss: 0.0081, Val Loss: 0.0100\n",
      "Epoch [370/1000], Loss: 0.0081, Val Loss: 0.0100\n",
      "Epoch [371/1000], Loss: 0.0081, Val Loss: 0.0100\n",
      "Epoch [372/1000], Loss: 0.0081, Val Loss: 0.0100\n",
      "Epoch [373/1000], Loss: 0.0081, Val Loss: 0.0100\n",
      "Epoch [374/1000], Loss: 0.0081, Val Loss: 0.0099\n",
      "Epoch [375/1000], Loss: 0.0081, Val Loss: 0.0099\n",
      "Epoch [376/1000], Loss: 0.0081, Val Loss: 0.0099\n",
      "Epoch [377/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [378/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [379/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [380/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [381/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [382/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [383/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [384/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [385/1000], Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [386/1000], Loss: 0.0080, Val Loss: 0.0098\n",
      "Epoch [387/1000], Loss: 0.0080, Val Loss: 0.0098\n",
      "Epoch [388/1000], Loss: 0.0080, Val Loss: 0.0098\n",
      "Epoch [389/1000], Loss: 0.0080, Val Loss: 0.0098\n",
      "Epoch [390/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [391/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [392/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [393/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [394/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [395/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [396/1000], Loss: 0.0079, Val Loss: 0.0097\n",
      "Epoch [397/1000], Loss: 0.0079, Val Loss: 0.0097\n",
      "Epoch [398/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [399/1000], Loss: 0.0079, Val Loss: 0.0098\n",
      "Epoch [400/1000], Loss: 0.0079, Val Loss: 0.0097\n",
      "Epoch [401/1000], Loss: 0.0079, Val Loss: 0.0097\n",
      "Epoch [402/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [403/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [404/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [405/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [406/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [407/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [408/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [409/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [410/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [411/1000], Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [412/1000], Loss: 0.0078, Val Loss: 0.0096\n",
      "Epoch [413/1000], Loss: 0.0078, Val Loss: 0.0096\n",
      "Epoch [414/1000], Loss: 0.0078, Val Loss: 0.0096\n",
      "Epoch [415/1000], Loss: 0.0078, Val Loss: 0.0096\n",
      "Epoch [416/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [417/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [418/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [419/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [420/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [421/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [422/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [423/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [424/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [425/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [426/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [427/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [428/1000], Loss: 0.0077, Val Loss: 0.0095\n",
      "Epoch [429/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [430/1000], Loss: 0.0077, Val Loss: 0.0096\n",
      "Epoch [431/1000], Loss: 0.0076, Val Loss: 0.0096\n",
      "Epoch [432/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [433/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [434/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [435/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [436/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [437/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [438/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [439/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [440/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [441/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [442/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [443/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [444/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [445/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [446/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [447/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [448/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [449/1000], Loss: 0.0075, Val Loss: 0.0095\n",
      "Epoch [450/1000], Loss: 0.0076, Val Loss: 0.0095\n",
      "Epoch [451/1000], Loss: 0.0075, Val Loss: 0.0095\n",
      "Epoch [452/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [453/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [454/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [455/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [456/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [457/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [458/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [459/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [460/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [461/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [462/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [463/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [464/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [465/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [466/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [467/1000], Loss: 0.0075, Val Loss: 0.0094\n",
      "Epoch [468/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [469/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [470/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [471/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [472/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [473/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [474/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [475/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [476/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [477/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [478/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [479/1000], Loss: 0.0074, Val Loss: 0.0093\n",
      "Epoch [480/1000], Loss: 0.0074, Val Loss: 0.0093\n",
      "Epoch [481/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [482/1000], Loss: 0.0074, Val Loss: 0.0094\n",
      "Epoch [483/1000], Loss: 0.0074, Val Loss: 0.0093\n",
      "Epoch [484/1000], Loss: 0.0074, Val Loss: 0.0093\n",
      "Epoch [485/1000], Loss: 0.0074, Val Loss: 0.0093\n",
      "Epoch [486/1000], Loss: 0.0074, Val Loss: 0.0093\n",
      "Epoch [487/1000], Loss: 0.0074, Val Loss: 0.0093\n",
      "Epoch [488/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [489/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [490/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [491/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [492/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [493/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [494/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [495/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [496/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [497/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [498/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [499/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [500/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [501/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [502/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [503/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [504/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [505/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [506/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [507/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [508/1000], Loss: 0.0073, Val Loss: 0.0093\n",
      "Epoch [509/1000], Loss: 0.0072, Val Loss: 0.0093\n",
      "Epoch [510/1000], Loss: 0.0072, Val Loss: 0.0093\n",
      "Epoch [511/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [512/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [513/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [514/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [515/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [516/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [517/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [518/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [519/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [520/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [521/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [522/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [523/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [524/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [525/1000], Loss: 0.0072, Val Loss: 0.0093\n",
      "Epoch [526/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [527/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [528/1000], Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch [529/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [530/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [531/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [532/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [533/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [534/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [535/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [536/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [537/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [538/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [539/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [540/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [541/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [542/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [543/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [544/1000], Loss: 0.0071, Val Loss: 0.0091\n",
      "Epoch [545/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [546/1000], Loss: 0.0071, Val Loss: 0.0091\n",
      "Epoch [547/1000], Loss: 0.0071, Val Loss: 0.0091\n",
      "Epoch [548/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [549/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [550/1000], Loss: 0.0071, Val Loss: 0.0091\n",
      "Epoch [551/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [552/1000], Loss: 0.0071, Val Loss: 0.0091\n",
      "Epoch [553/1000], Loss: 0.0071, Val Loss: 0.0092\n",
      "Epoch [554/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [555/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [556/1000], Loss: 0.0071, Val Loss: 0.0091\n",
      "Epoch [557/1000], Loss: 0.0070, Val Loss: 0.0092\n",
      "Epoch [558/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [559/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [560/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [561/1000], Loss: 0.0070, Val Loss: 0.0092\n",
      "Epoch [562/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [563/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [564/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [565/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [566/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [567/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [568/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [569/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [570/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [571/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [572/1000], Loss: 0.0070, Val Loss: 0.0091\n",
      "Early stopping triggered. Restoring best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_44040\\3912563837.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reverse_model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    reverse_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training Phase\n",
    "    for batch_idx in range(0, len(points_list), batch_size):\n",
    "        batch_points = points_list[batch_idx:batch_idx + batch_size]\n",
    "        batch_token_tensor = token_tensor[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        # Random timesteps\n",
    "        t_batch = torch.randint(0, 1000, (len(batch_points),), device=device)\n",
    "\n",
    "        # Add noise\n",
    "        noisy_tokens, _ = diffusion_model.add_noise(batch_token_tensor, t_batch)\n",
    "\n",
    "        # Get embeddings\n",
    "        batch_embeddings = [tnet_model(pt) for pt in batch_points]\n",
    "        embeddings_tensor = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "        # Predict logits\n",
    "        logits = reverse_model(noisy_tokens, embeddings_tensor, t_batch)\n",
    "\n",
    "        # Reshape logits and target tokens for CrossEntropyLoss\n",
    "        logits_flat = logits.view(-1, vocab_size)\n",
    "        target_tokens = batch_token_tensor.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits_flat, target_tokens)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation Phase\n",
    "    reverse_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch_idx in range(0, len(val_points_list), batch_size):\n",
    "            val_batch_points = val_points_list[val_batch_idx:val_batch_idx + batch_size]\n",
    "            val_batch_token_tensor = val_token_tensor[val_batch_idx:val_batch_idx + batch_size]\n",
    "\n",
    "            val_t_batch = torch.randint(0, 1000, (len(val_batch_points),), device=device)\n",
    "\n",
    "            val_noisy_tokens, _ = diffusion_model.add_noise(val_batch_token_tensor, val_t_batch)\n",
    "\n",
    "            val_embeddings = [tnet_model(pt) for pt in val_batch_points]\n",
    "            val_embeddings_tensor = torch.cat(val_embeddings, dim=0)\n",
    "\n",
    "            val_logits = reverse_model(val_noisy_tokens, val_embeddings_tensor, val_t_batch)\n",
    "            val_logits_flat = val_logits.view(-1, vocab_size)\n",
    "            val_target_tokens = val_batch_token_tensor.view(-1)\n",
    "\n",
    "            val_loss += loss_fn(val_logits_flat, val_target_tokens).item()\n",
    "\n",
    "    val_loss /= len(val_points_list)\n",
    "\n",
    "    # Store losses for plotting\n",
    "    training_losses.append(total_loss / len(points_list))\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {training_losses[-1]:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    #scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(reverse_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered. Restoring best model...\")\n",
    "        reverse_model.load_state_dict(torch.load('best_model.pth'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAIjCAYAAAB/FZhcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAACfW0lEQVR4nOzdd3hUVeLG8e9MKklIQigJHamhhF4EC6ihi4IKiChlESsWUBexIFZ0VcQVfyL2dUUQRERFICCshUjvAoICoSUQWkggySRzf38cMjAmIC1McvN+nuc+M3PnlnM5CfJ6msOyLAsREREREREpcpy+LoCIiIiIiIgUTIFNRERERESkiFJgExERERERKaIU2ERERERERIooBTYREREREZEiSoFNRERERESkiFJgExERERERKaIU2ERERERERIooBTYREREREZEiSoFNRKQQDRo0iBo1apzXuWPGjMHhcFzcAhUx27dvx+Fw8PHHH1/yezscDsaMGeP5/PHHH+NwONi+ffvfnlujRg0GDRp0UctzIT8rIiJiXwpsIlIiORyOs9oWLVrk66KWeA8++CAOh4OtW7ee9pgnn3wSh8PB2rVrL2HJzt2ePXsYM2YMq1ev9nVRPPJC82uvvebropyVlJQUHn30UWJjYwkJCSE0NJQWLVrwwgsvcPjwYV8XT0TkovP3dQFERHzh008/9fr8n//8h4SEhHz769evf0H3ee+993C73ed17lNPPcXjjz9+Qfe3g/79+/PWW28xefJkRo8eXeAxn3/+OXFxcTRu3Pi873PHHXdw6623EhQUdN7X+Dt79uzh2WefpUaNGjRt2tTruwv5WSkpli1bRrdu3UhPT+f222+nRYsWACxfvpyXX36ZH3/8kXnz5vm4lCIiF5cCm4iUSLfffrvX519//ZWEhIR8+//q2LFjhISEnPV9AgICzqt8AP7+/vj766/pNm3aULt2bT7//PMCA1tiYiLbtm3j5ZdfvqD7+Pn54efnd0HXuBAX8rNSEhw+fJhevXrh5+fHqlWriI2N9fr+xRdf5L333rso98rIyCA0NPSiXEtE5EKpS6SIyGl06NCBRo0asWLFCq6++mpCQkJ44oknAPj666/p3r07lSpVIigoiFq1avH888+Tm5vrdY2/jks6tfvZpEmTqFWrFkFBQbRq1Yply5Z5nVvQGDaHw8GwYcOYOXMmjRo1IigoiIYNGzJnzpx85V+0aBEtW7YkODiYWrVq8e677571uLiffvqJ3r17U61aNYKCgqhatSrDhw/n+PHj+Z4vLCyM3bt307NnT8LCwihfvjyPPvpovj+Lw4cPM2jQICIiIoiMjGTgwIFn3YWtf//+bNq0iZUrV+b7bvLkyTgcDvr160d2djajR4+mRYsWREREEBoaylVXXcXChQv/9h4FjWGzLIsXXniBKlWqEBISwjXXXMOGDRvynXvw4EEeffRR4uLiCAsLIzw8nK5du7JmzRrPMYsWLaJVq1YADB482NPtNm/8XkFj2DIyMnjkkUeoWrUqQUFB1KtXj9deew3LsryOO5efi/O1b98+hgwZQnR0NMHBwTRp0oRPPvkk33FTpkyhRYsWlC5dmvDwcOLi4njzzTc937tcLp599lnq1KlDcHAwZcuW5corryQhIeGM93/33XfZvXs348aNyxfWAKKjo3nqqac8n/86RjHPX8cf5tX7//73P+677z4qVKhAlSpVmD59umd/QWVxOBysX7/es2/Tpk3ccsstREVFERwcTMuWLZk1a5bXeef77CJSsul/3YqInMGBAwfo2rUrt956K7fffjvR0dGA+UdeWFgYI0aMICwsjB9++IHRo0eTlpbGq6+++rfXnTx5MkePHuXuu+/G4XDwr3/9i5tuuok///zzb1tafv75Z2bMmMF9991H6dKl+fe//83NN99MUlISZcuWBWDVqlV06dKFihUr8uyzz5Kbm8tzzz1H+fLlz+q5p02bxrFjx7j33nspW7YsS5cu5a233mLXrl1MmzbN69jc3Fw6d+5MmzZteO2115g/fz6vv/46tWrV4t577wVM8Lnxxhv5+eefueeee6hfvz5fffUVAwcOPKvy9O/fn2effZbJkyfTvHlzr3t/8cUXXHXVVVSrVo3U1FTef/99+vXrx9ChQzl69CgffPABnTt3ZunSpfm6If6d0aNH88ILL9CtWze6devGypUr6dSpE9nZ2V7H/fnnn8ycOZPevXtz2WWXkZKSwrvvvkv79u357bffqFSpEvXr1+e5555j9OjR3HXXXVx11VUAtGvXrsB7W5bFDTfcwMKFCxkyZAhNmzZl7ty5PPbYY+zevZs33njD6/iz+bk4X8ePH6dDhw5s3bqVYcOGcdlllzFt2jQGDRrE4cOHeeihhwBISEigX79+XHfddbzyyisAbNy4kV9++cVzzJgxYxg7dix33nknrVu3Ji0tjeXLl7Ny5Uo6dux42jLMmjWLUqVKccstt1zQs5zOfffdR/ny5Rk9ejQZGRl0796dsLAwvvjiC9q3b+917NSpU2nYsCGNGjUCYMOGDVxxxRVUrlyZxx9/nNDQUL744gt69uzJl19+Sa9evS7o2UWkhLNERMS6//77rb/+ldi+fXsLsCZOnJjv+GPHjuXbd/fdd1shISFWZmamZ9/AgQOt6tWrez5v27bNAqyyZctaBw8e9Oz/+uuvLcD65ptvPPueeeaZfGUCrMDAQGvr1q2efWvWrLEA66233vLs69GjhxUSEmLt3r3bs2/Lli2Wv79/vmsWpKDnGzt2rOVwOKwdO3Z4PR9gPffcc17HNmvWzGrRooXn88yZMy3A+te//uXZl5OTY1111VUWYH300Ud/W6ZWrVpZVapUsXJzcz375syZYwHWu+++67lmVlaW13mHDh2yoqOjrX/84x9e+wHrmWee8Xz+6KOPLMDatm2bZVmWtW/fPiswMNDq3r275Xa7Pcc98cQTFmANHDjQsy8zM9OrXJZl6jooKMjrz2bZsmWnfd6//qzk/Zm98MILXsfdcsstlsPh8PoZONufi4Lk/Uy++uqrpz1m/PjxFmD997//9ezLzs622rZta4WFhVlpaWmWZVnWQw89ZIWHh1s5OTmnvVaTJk2s7t27n7FMBSlTpozVpEmTsz7+r/Wbp3r16l51l1fvV155Zb5y9+vXz6pQoYLX/r1791pOp9OrXq+77jorLi7O63ff7XZb7dq1s+rUqePZd77PLiIlm7pEioicQVBQEIMHD863v1SpUp73R48eJTU1lauuuopjx46xadOmv71u3759KVOmjOdzXmvLn3/++bfnxsfHU6tWLc/nxo0bEx4e7jk3NzeX+fPn07NnTypVquQ5rnbt2nTt2vVvrw/ez5eRkUFqairt2rXDsixWrVqV7/h77rnH6/NVV13l9SyzZ8/G39/f0+IGZszYAw88cFblATPucNeuXfz444+efZMnTyYwMJDevXt7rhkYGAiA2+3m4MGD5OTk0LJlywK7U57J/Pnzyc7O5oEHHvDqRvrwww/nOzYoKAin0/wnNTc3lwMHDhAWFka9evXO+b55Zs+ejZ+fHw8++KDX/kceeQTLsvj++++99v/dz8WFmD17NjExMfTr18+zLyAggAcffJD09HRPt8HIyEgyMjLO2MUvMjKSDRs2sGXLlnMqQ1paGqVLlz6/BzgLQ4cOzTeGsW/fvuzbt89rttjp06fjdrvp27cvYLrD/vDDD/Tp08fzd0FqaioHDhygc+fObNmyhd27dwPn/+wiUrIpsImInEHlypU9AeBUGzZsoFevXkRERBAeHk758uU9E5YcOXLkb69brVo1r8954e3QoUPnfG7e+Xnn7tu3j+PHj1O7du18xxW0ryBJSUkMGjSIqKgoz7i0vG5hf32+4ODgfF0tTy0PwI4dO6hYsSJhYWFex9WrV++sygNw66234ufnx+TJkwHIzMzkq6++omvXrl7h95NPPqFx48aeMULly5fnu+++O6t6OdWOHTsAqFOnjtf+8uXLe90PTDh84403qFOnDkFBQZQrV47y5cuzdu3ac77vqfevVKlSvpCSN3NpXvny/N3PxYXYsWMHderU8YTS05Xlvvvuo27dunTt2pUqVarwj3/8I984uueee47Dhw9Tt25d4uLieOyxx85qOYbw8HCOHj16wc9yOpdddlm+fV26dCEiIoKpU6d69k2dOpWmTZtSt25dALZu3YplWTz99NOUL1/ea3vmmWcA8zsJ5//sIlKyKbCJiJzBqS1NeQ4fPkz79u1Zs2YNzz33HN988w0JCQmeMTtnMzX76WYjtP4ymcTFPvds5Obm0rFjR7777jtGjhzJzJkzSUhI8EyO8dfnu1QzK1aoUIGOHTvy5Zdf4nK5+Oabbzh69Cj9+/f3HPPf//6XQYMGUatWLT744APmzJlDQkIC1157baFOmf/SSy8xYsQIrr76av773/8yd+5cEhISaNiw4SWbqr+wfy7ORoUKFVi9ejWzZs3yjL/r2rWr11jFq6++mj/++IMPP/yQRo0a8f7779O8eXPef//9M147NjaW33//Pd/4wXP118lw8hT0ux4UFETPnj356quvyMnJYffu3fzyyy+e1jU4+fvw6KOPkpCQUOCW9z9KzvfZRaRk06QjIiLnaNGiRRw4cIAZM2Zw9dVXe/Zv27bNh6U6qUKFCgQHBxe40PSZFp/Os27dOn7//Xc++eQTBgwY4Nl/ITPZVa9enQULFpCenu7VyrZ58+Zzuk7//v2ZM2cO33//PZMnTyY8PJwePXp4vp8+fTo1a9ZkxowZXt0Y81o6zrXMAFu2bKFmzZqe/fv378/XajV9+nSuueYaPvjgA6/9hw8fply5cp7PZzND56n3nz9/PkePHvVqZcvrcptXvkuhevXqrF27Frfb7dXKVlBZAgMD6dGjBz169MDtdnPffffx7rvv8vTTT3uCS1RUFIMHD2bw4MGkp6dz9dVXM2bMGO68887TlqFHjx4kJiby5ZdfenXNPJ0yZcrkm4U0OzubvXv3nsuj07dvXz755BMWLFjAxo0bsSzLK7Dl/WwEBAQQHx//t9c7n2cXkZJNLWwiIucoryXj1JaL7Oxs/u///s9XRfLi5+dHfHw8M2fOZM+ePZ79W7duzTfu6XTng/fzWZblNTX7uerWrRs5OTm88847nn25ubm89dZb53Sdnj17EhISwv/93//x/fffc9NNNxEcHHzGsi9ZsoTExMRzLnN8fDwBAQG89dZbXtcbP358vmP9/PzytWRNmzbNM3YpT97aXmeznEG3bt3Izc1lwoQJXvvfeOMNHA7HWY9HvBi6detGcnKyV9fAnJwc3nrrLcLCwjzdZQ8cOOB1ntPp9CxmnpWVVeAxYWFh1K5d2/P96dxzzz1UrFiRRx55hN9//z3f9/v27eOFF17wfK5Vq5bXeEeASZMmnbaF7XTi4+OJiopi6tSpTJ06ldatW3t1n6xQoQIdOnTg3XffLTAM7t+/3/P+fJ9dREo2tbCJiJyjdu3aUaZMGQYOHMiDDz6Iw+Hg008/vaRdz/7OmDFjmDdvHldccQX33nuv5x/+jRo1YvXq1Wc8NzY2llq1avHoo4+ye/duwsPD+fLLLy9oLFSPHj244oorePzxx9m+fTsNGjRgxowZ5zy+KywsjJ49e3rGsZ3aHRLg+uuvZ8aMGfTq1Yvu3buzbds2Jk6cSIMGDUhPTz+ne+WtJzd27Fiuv/56unXrxqpVq/j++++9Ws3y7vvcc88xePBg2rVrx7p16/jss8+8WubAhIjIyEgmTpxI6dKlCQ0NpU2bNgWOn+rRowfXXHMNTz75JNu3b6dJkybMmzePr7/+mocffthrgpGLYcGCBWRmZubb37NnT+666y7effddBg0axIoVK6hRowbTp0/nl19+Yfz48Z4WwDvvvJODBw9y7bXXUqVKFXbs2MFbb71F06ZNPePdGjRoQIcOHWjRogVRUVEsX76c6dOnM2zYsDOWr0yZMnz11Vd069aNpk2bcvvtt9OiRQsAVq5cyeeff07btm09x995553cc8893HzzzXTs2JE1a9Ywd+7cfHX3dwICArjpppuYMmUKGRkZvPbaa/mOefvtt7nyyiuJi4tj6NCh1KxZk5SUFBITE9m1a5dnPb7zfXYRKeF8MTWliEhRc7pp/Rs2bFjg8b/88ot1+eWXW6VKlbIqVapk/fOf/7Tmzp1rAdbChQs9x51uWv+CplDnL9OQn25a//vvvz/fuX+dqtyyLGvBggVWs2bNrMDAQKtWrVrW+++/bz3yyCNWcHDwaf4UTvrtt9+s+Ph4KywszCpXrpw1dOhQzzTxp05JP3DgQCs0NDTf+QWV/cCBA9Ydd9xhhYeHWxEREdYdd9xhrVq16qyn9c/z3XffWYBVsWLFfFPpu91u66WXXrKqV69uBQUFWc2aNbO+/fbbfPVgWX8/rb9lWVZubq717LPPWhUrVrRKlSpldejQwVq/fn2+P+/MzEzrkUce8Rx3xRVXWImJiVb79u2t9u3be93366+/tho0aOBZYiHv2Qsq49GjR63hw4dblSpVsgICAqw6depYr776qtcyA3nPcrY/F3+V9zN5uu3TTz+1LMuyUlJSrMGDB1vlypWzAgMDrbi4uHz1Nn36dKtTp05WhQoVrMDAQKtatWrW3Xffbe3du9dzzAsvvGC1bt3aioyMtEqVKmXFxsZaL774opWdnX3GcubZs2ePNXz4cKtu3bpWcHCwFRISYrVo0cJ68cUXrSNHjniOy83NtUaOHGmVK1fOCgkJsTp37mxt3br1tNP6L1u27LT3TEhIsADL4XBYO3fuLPCYP/74wxowYIAVExNjBQQEWJUrV7auv/56a/r06Rft2UWkZHJYVhH6X8IiIlKoevbsqWnFRUREihGNYRMRsanjx497fd6yZQuzZ8+mQ4cOvimQiIiInDO1sImI2FTFihUZNGgQNWvWZMeOHbzzzjtkZWWxatWqfGuLiYiISNGkSUdERGyqS5cufP755yQnJxMUFETbtm156aWXFNZERESKEbWwiYiIiIiIFFEawyYiIiIiIlJEKbCJiIiIiIgUURrDdp7cbjd79uyhdOnSOBwOXxdHRERERER8xLIsjh49SqVKlXA6L26bmALbedqzZw9Vq1b1dTFERERERKSI2LlzJ1WqVLmo11RgO0+lS5cGTKWEh4f7rBwul4t58+bRqVMnAgICfFYOKXyq65JF9V1yqK5LDtV1yaL6LjlcLhczZ87kzjvv9GSEi0mB7TzldYMMDw/3eWALCQkhPDxcfxnYnOq6ZFF9lxyq65JDdV2yqL5Ljry6BgplqJQmHRERERERESmiFNhERERERESKKAU2ERERERGRIkpj2ERERETEFizLIicnh9zcXF8XBZfLhb+/P5mZmUWiPHJh/Pz88Pf398lyXgpsIiIiIlLsZWdns3fvXo4dO+brogAmPMbExLBz506t2WsTISEhVKxYkcDAwEt6XwU2ERERESnW3G4327Ztw8/Pj0qVKhEYGOjzkOR2u0lPTycsLOyiL6Qsl5ZlWWRnZ7N//362bdtGnTp1LmmdKrCJiIiISLGWnZ2N2+2matWqnunVfc3tdpOdnU1wcLACmw2UKlWKgIAAduzY4anXS0U/PSIiIiJiCwpGUph89fOln2oREREREZEiSoFNRERERESkiFJgExERERGxkRo1ajB+/PizPn7RokU4HA4OHz5caGWS86fAJiIiIiLiAw6H44zbmDFjzuu6y5Yt46677jrr49u1a8fevXuJiIg4r/udLQXD86NZIkVEREREfGDv3r2e91OnTmX06NFs3rzZsy8sLMzz3rIscnNz8ff/+3++ly9f/pzKERgYSExMzDmdI5eOWthERERExHYsCzIyfLNZ1tmVMSYmxrNFRETgcDg8nzdt2kTp0qX5/vvvadGiBUFBQfz888/88ccf3HjjjURHRxMWFkarVq2YP3++13X/2iXS4XDw/vvv06tXL0JCQqhTpw6zZs3yfP/Xlq+PP/6YyMhI5s6dS/369QkLC6NLly5eATMnJ4cHH3yQyMhIypYty8iRIxk4cCA9e/Y83yrj0KFDDBgwgDJlyhASEkLXrl3ZsmWL5/sdO3bQo0cPypQpQ2hoKA0bNmT27Nmec/v370/58uUpVaoUderU4aOPPjrvshQlCmwiIiIiYjvHjkFYmG+2Y8cu3nM8/vjjvPzyy2zcuJHGjRuTnp5Ot27dWLBgAatWraJLly706NGDpKSkM17n2WefpU+fPqxdu5Zu3brRv39/Dh48eIY/v2O89tprfPrpp/z4448kJSXx6KOPer5/5ZVX+Oyzz/joo4/45ZdfSEtLY+bMmRf0rIMGDWL58uXMmjWLxMRELMuiW7duuFwuAO6//36ysrL48ccfWbduHa+88oqnFfLpp5/mt99+4/vvv2fjxo288847lCtX7oLKU1SoS6SIiIiISBH13HPP0bFjR8/nqKgomjRp4vn8/PPP89VXXzFr1iyGDRt22usMGjSIfv36AfDSSy/x73//m6VLl9KlS5cCj3e5XEycOJFatWoBMGzYMJ577jnP92+99RajRo2iV69eAEyYMMHT2nU+tmzZwqxZs/jll19o164dAJ999hlVq1Zl5syZ9O7dm6SkJG6++Wbi4uIAqFmzpuf8pKQkmjVrRsuWLQHTymgXCmzFXE4O/PprRTIzHdxyC/j5+bpEIiIiIr4XEgLp6b6799l2i/w7eQEkT3p6OmPGjOG7775j79695OTkcPz48b9tYWvcuLHnfWhoKOHh4ezbt++0x4eEhHjCGkDFihU9xx85coSUlBRat27t+d7Pz48WLVrgdrvP6fnybNy4EX9/f9q0aePZV7ZsWerVq8fGjRsBePDBB7n33nuZN28e8fHx3HzzzZ7nuvfee7n55ptZuXIlnTp1omfPnp7gV9ypS2Qxl5UFL7/cmltv9Scz09elERERESkaHA4IDfXN5nBcvOcIDQ31+vzoo4/y1Vdf8dJLL/HTTz+xevVq4uLiyM7OPuN1AgIC/vLn4zhjuCroeOtipdDzdOedd/Lnn39yxx13sG7dOlq2bMlbb70FQNeuXdmxYwfDhw9nz549XHfddV5dOIszBbZi7tQWtfP8HxoiIiIiUkz88ssvDBo0iF69ehEXF0dMTAzbt2+/pGWIiIggOjqaZcuWefbl5uaycuXK875m/fr1ycnJYcmSJZ59Bw4cYPPmzTRo0MCzr2rVqtxzzz3MmDGDRx55hPfee8/zXfny5Rk4cCD//e9/GT9+PJMmTTrv8hQl6hJZzDlPidwKbCIiIiL2VqdOHWbMmEGPHj1wOBw8/fTT590N8UI88MADjB07ltq1axMbG8tbb73FoUOHcJxF8+K6desoXbq057PD4aBJkybceOONDB06lHfffZfSpUvz+OOPU7lyZW688UYAHn74Ybp27UrdunU5dOgQCxcupH79+gCMHj2aFi1a0LBhQ7Kysvj222893xV3CmzF3KmBLTfXd+UQERERkcI3btw4/vGPf9CuXTvKlSvHyJEjSUtLu+TlGDlyJMnJyQwYMAA/Pz/uuusuOnfujN9ZTKhw9dVXe3328/MjJyeHjz76iIceeojrr7+e7Oxsrr76ambPnu3pnpmbm8v999/Prl27CA8Pp0uXLrzxxhuAWUtu1KhRbN++nVKlSnHVVVcxZcqUi//gPuCwfN0ZtZhKS0sjIiKCI0eOEB4e7rNyZGe7CAoyP8T794NNZi+VArhcLmbPnk23bt3y9SsX+1F9lxyq65JDdV14MjMz2bZtG5dddhnBwcG+Lg4AbrebtLQ0wsPDcTrtPwrJ7XZTv359+vTpw/PPP+/r4hSK0/2cuVwupk+fzm233VYo2UAtbMXcqa3OamETERERkUthx44dzJs3j/bt25OVlcWECRPYtm0bt912m6+LZjv2j/slgNNp+i1rDJuIiIiIXApOp5OPP/6YVq1accUVV7Bu3Trmz59vm3FjRYla2GzA6TRhTYFNRERERC6FqlWr8ssvv/i6GCWCWthswOEwwxDVJVJERERExF4U2GzA6TSBTS1sIiIiIiL2osBmA3kTj6iFTURERETEXhTYbEAtbCIiIiIi9qTAZgMKbCIiIiIi9qTAZgOadERERERExJ4U2GxALWwiIiIiJVeHDh14+OGHPZ9r1KjB+PHjz3iOw+Fg5syZF3zvi3UdOT0FNhtwnqhFtbCJiIiIFB89evSgS5cuBX73008/4XA4WLt27Tlfd9myZdx1110XWjwvY8aMoWnTpvn27927l65du17Ue/3Vxx9/TGRkZKHeoyjTwtnFXW4Wz9/0JNlZTqzcV4BAX5dIRERERM7CkCFDuPnmm9m1axdVqlTx+u6jjz6iZcuWNG7c+JyvW758+YtVxL8VExNzye5VUqmFrbizcrj/2n8zvOt4rNxsX5dGREREpGiwLMjJ8M1mWWdVxOuvv57y5cvz8ccfe+1PT09n2rRpDBkyhAMHDtCvXz8qV65MSEgIcXFxfP7552e87l+7RG7ZsoWrr76a4OBgGjRoQEJCQr5zRo4cSd26dQkJCaFmzZo8/fTTuFwuwLRwPfvss6xZswaHw4HD4fCU+a9dItetW8e1115LqVKlKFu2LHfddRfp6eme7wcNGkTPnj157bXXqFixImXLluX+++/33Ot8JCUlceONNxIWFkZ4eDh9+vQhJSXF8/2aNWu45pprKF26NOHh4bRo0YLly5cDsGPHDnr06EGZMmUIDQ2lYcOGzJ49+7zLUhjUwlbcOU5WoTtHfSJFREREAMg9Bl+E+ebefdLBWepvD/P392fAgAF8/PHHPPnkkzhOLK47bdo0cnNz6devH+np6bRo0YKRI0cSHh7Od999xx133EGtWrVo3br1397D7XZz0003ER0dzZIlSzhy5IjXeLc8pUuX5uOPP6ZSpUqsW7eOoUOHUrp0af75z3/St29f1q9fz5w5c5g/fz4AERER+a6RkZFB586dadu2LcuWLWPfvn3ceeedDBs2zCuULly4kIoVK7Jw4UK2bt1K3759adq0KUOHDv3b5yno+fLC2v/+9z9ycnK4//776du3L4sWLQKgf//+NGvWjHfeeQc/Pz9Wr15NQEAAAPfffz/Z2dn8+OOPhIaG8ttvvxEW5qOfm9NQYCvuHH6et5Y7x4cFEREREZFz9Y9//INXX32V//3vf3To0AEw3SFvvvlmIiIiiIiI4NFHH/Uc/8ADDzB37ly++OKLswps8+fPZ9OmTcydO5dKlSoB8NJLL+Ubd/bUU0953teoUYNHH32UKVOm8M9//pNSpUoRFhaGv7//GbtATp48mczMTP7zn/8QGhoKwIQJE+jRowevvPIK0dHRAJQpU4YJEybg5+dHbGws3bt3Z8GCBecV2BYsWMC6devYtm0bVatWBeA///kPDRs2ZNmyZbRq1YqkpCQee+wxYmNjAahTp47n/KSkJG6++Wbi4uIAqFmz5jmXobApsBV3pwQ2t2YdERERETH8QkxLl6/ufZbdImNjY2nXrh0ffvghHTp0YOvWrfz0008899xzAOTm5vLSSy/xxRdfsHv3brKzs8nKyiIkJOSsrr9x40aqVq3qCWsAbdu2zXfc1KlT+fe//80ff/xBeno6OTk5hIeHn9U9Tr1XkyZNPGEN4IorrsDtdrN582ZPYGvYsCF+fif/DVuxYkXWrVt3Tvc69Z5Vq1b1hDWABg0aEBkZycaNG2nVqhUjRozgzjvv5NNPPyU+Pp7evXtTq1YtAB588EHuvfde5s2bR3x8PDfffPN5jRssTBrDVtw5HLjdpvnc7VZgExEREQHA4QD/UN9sJ7o2nq0hQ4bw5ZdfcvToUT766CNq1apF+/btAXj11Vd58803GTlyJAsXLmT16tV07tyZ7OyLN3dBYmIi/fv3p1u3bnz77besWrWKJ5988qLe41R53RHzOBwO3IW4PtWYMWPYsGED3bt354cffqBBgwZ89dVXANx55538+eef3HHHHaxbt46WLVvy1ltvFVpZzocCmw3kWub/UFgKbCIiIiLFTp8+fXA6nUyePJn//Oc//OMf//CMZ/vll1+48cYbuf3222nSpAk1a9bk999/P+tr169fn507d7J3717Pvl9//dXrmMWLF1O9enWefPJJWrZsSZ06ddixY4fXMYGBgeT+TW+u+vXrs2bNGjIyMjz7fvnlF5xOJ/Xq1TvrMp+LvOfbuXOnZ99vv/3G4cOHadCggWdf3bp1GT58OPPmzeOmm27io48+8nxXtWpV7rnnHmbMmMEjjzzCe++9VyhlPV8KbDaQ6z4R2NQlUkRERKTYCQsLo2/fvowaNYq9e/cyaNAgz3d16tQhISGBxYsXs3HjRu6++26vGRD/Tnx8PHXr1mXgwIGsWbOGn376iSeffNLrmDp16pCUlMSUKVP4448/+Pe//+1pgcpTo0YNtm3bxurVq0lNTSUrKyvfvfr3709wcDADBw5k/fr1LFy4kAceeIA77rjD0x3yfOXm5rJ69WqvbePGjcTHxxMXF0f//v1ZuXIlS5cuZcCAAbRv356WLVty/Phxhg0bxqJFi9ixYwe//PILy5Yto379+gA8/PDDzJ07l23btrFy5UoWLlzo+a6oUGCzAbda2ERERESKtSFDhnDo0CE6d+7sNd7sqaeeonnz5nTu3JkOHToQExNDz549z/q6TqeTr776iuPHj9O6dWvuvPNOXnzxRa9jbrjhBoYPH86wYcNo2rQpixcv5umnn/Y65uabb6ZLly5cc801lC9fvsClBUJCQpg7dy4HDx6kVatW3HLLLVx33XVMmDDh3P4wCpCenk6zZs28th49euBwOPj6668pU6YMV199NfHx8dSsWZOpU6cC4Ofnx4EDBxgwYAB169alT58+dO3alWeffRYwQfD++++nfv36dOnShbp16/J///d/F1zei8lhWWc5IlK8pKWlERERwZEjR855QObF5HK5yPy0LKWDj/Jzmd+5smudvz9JiiWXy8Xs2bPp1q1bvr7fYj+q75JDdV1yqK4LT2ZmJtu2beOyyy4jODjY18UBzHTzaWlphIeH43SqjcQOTvdz5nK5mD59OrfddluhZAP99NhAXpdId66m9RcRERERsRMFNhtQl0gREREREXsqEoHt7bffpkaNGgQHB9OmTRuWLl16xuOnTZtGbGwswcHBxMXFMXv27NMee8899+BwOBg/frzX/oMHD9K/f3/Cw8OJjIxkyJAhpKf7aK2OC+SZJVKTjoiIiIiI2IrPA9vUqVMZMWIEzzzzDCtXrqRJkyZ07tyZffv2FXj84sWL6devH0OGDGHVqlX07NmTnj17sn79+nzHfvXVV/z6669eAzfz9O/fnw0bNpCQkMC3337Ljz/+yF133XXRn+9ScLtNNVqWApuIiIiIiJ34PLCNGzeOoUOHMnjwYBo0aMDEiRMJCQnhww8/LPD4N998ky5duvDYY49Rv359nn/+eZo3b55v9pndu3fzwAMP8Nlnn+Ub2Ltx40bmzJnD+++/T5s2bbjyyit56623mDJlCnv27Cm0Zy0seV0iUZdIERERKcE0l54UJl/9fPn75K4nZGdns2LFCkaNGuXZ53Q6iY+PJzExscBzEhMTGTFihNe+zp07M3PmTM9nt9vNHXfcwWOPPUbDhg0LvEZkZCQtW7b07IuPj8fpdLJkyRJ69eqV75ysrCyv9SbS0tIAMyuMy+U6uwcuBC6Xixy3qUZXdpZPyyKFK69uVcclg+q75FBdlxyq68JlWRbp6ekEBQX5uijAyX/cW5aF2+32cWnkYkhPT/fU66m/x4X9O+3TwJaamkpubm6+hfSio6PZtGlTgeckJycXeHxycrLn8yuvvIK/vz8PPvjgaa9RoUIFr33+/v5ERUV5XedUY8eO9azXcKp58+YREhJS4DmXSlPLNJRu+X0Tx2cf8GlZpPAlJCT4ughyCam+Sw7Vdcmhui4cpUuXJisri8zMTAIDA3E4HL4uEgAHDujfZsWdZVlkZ2eTmprKoUOH2LJlyyW9v08DW2FYsWIFb775JitXrryov6ijRo3yatlLS0ujatWqdOrUyefrsKV8YLpE1q5Zk2u7XeWzskjhcrlcJCQk0LFjR63fUwKovksO1XXJobouXJZlsW/fPk8vKF+zLIvMzEyCg4OLTHiUC1O+fHkaNmyYrz5dLhdff/11od3Xp4GtXLly+Pn5kZKS4rU/JSWFmJiYAs+JiYk54/E//fQT+/bto1q1ap7vc3NzeeSRRxg/fjzbt28nJiYm36QmOTk5HDx48LT3DQoKKrCJPSAgwOd/6eaNYXM68XlZpPAVhZ85uXRU3yWH6rrkUF0XnipVqpCbm1skup26XC5+/PFHrr76atW3DQQEBODn5+eTe/s0sAUGBtKiRQsWLFhAz549ATP+bMGCBQwbNqzAc9q2bcuCBQt4+OGHPfsSEhJo27YtAHfccQfx8fFe53Tu3Jk77riDwYMHe65x+PBhVqxYQYsWLQD44YcfcLvdtGnT5iI/ZeFzn+gSiWaJFBERkRLOz8/PZ/+w/ms5cnJyCA4OVmCTC+LzLpEjRoxg4MCBtGzZktatWzN+/HgyMjI84WrAgAFUrlyZsWPHAvDQQw/Rvn17Xn/9dbp3786UKVNYvnw5kyZNAqBs2bKULVvW6x4BAQHExMRQr149AOrXr0+XLl0YOnQoEydOxOVyMWzYMG699dYClwAo6txah01ERERExJZ8Htj69u3L/v37GT16NMnJyTRt2pQ5c+Z4JhZJSkrC6Ty5+kC7du2YPHkyTz31FE888QR16tRh5syZNGrU6Jzu+9lnnzFs2DCuu+46nE4nN998M//+978v6rNdKmphExERERGxJ58HNoBhw4adtgvkokWL8u3r3bs3vXv3Puvrb9++Pd++qKgoJk+efNbXKMpyT0zrb2kdNhERERERW/H5wtly4fJa2Cy1sImIiIiI2IoCmw24MWPYHFaOj0siIiIiIiIXkwKbDXha2NQlUkRERETEVhTYbCBvlkhNOiIiIiIiYi8KbDagFjYREREREXtSYLMBt2VmiXSohU1ERERExFYU2GxA67CJiIiIiNiTApsNuPOqUbNEioiIiIjYigKbDVhqYRMRERERsSUFNhvIW4cNFNhEREREROxEgc0GPGPYNEukiIiIiIitKLDZgHWihc2hFjYREREREVtRYLOBvIWzLY1hExERERGxFQU2G7BOVKPWYRMRERERsRcFNhvIG8PmQNP6i4iIiIjYiQKbDeR1idS0/iIiIiIi9qLAZgOWQ10iRURERETsSIHNBjwLZ2uWSBERERERW1Fgs4G8hbPVwiYiIiIiYi8KbDZgeSYdUWATEREREbETBTYbyJvWX5OOiIiIiIjYiwKbDVgnukQ6HZrWX0RERETEThTYbMCtFjYREREREVtSYLOBvC6RGsMmIiIiImIvCmw2kNclUoFNRERERMReFNhswK0WNhERERERW1JgswUFNhERERERO1Jgs4G8FjanApuIiIiIiK0osNlCXgubpvUXEREREbETBTYbsBwnWtgcamETEREREbETBTYb0LT+IiIiIiL2pMBmA5rWX0RERETEnhTYbEBdIkVERERE7EmBzQbUJVJERERExJ4U2GxB0/qLiIiIiNiRApsNWDgAcDo0rb+IiIiIiJ0osNmB80SXSI1hExERERGxFQU2G8ibJVJdIkVERERE7EWBzQbyJh3RLJEiIiIiIvaiwGYHmtZfRERERMSWFNhswDOtvwKbiIiIiIitKLDZgePELJEawyYiIiIiYisKbHZwokukn1PT+ouIiIiI2IkCmx1oDJuIiIiIiC0psNmA5Tgxrb8Cm4iIiIiIrSiw2YDFiTFsCmwiIiIiIraiwGYLJ8awKbCJiIiIiNiKApsdONXCJiIiIiJiR0UisL399tvUqFGD4OBg2rRpw9KlS894/LRp04iNjSU4OJi4uDhmz57t9f2YMWOIjY0lNDSUMmXKEB8fz5IlS7yOqVGjBg6Hw2t7+eWXL/qzXRInxrBplkgREREREXvxeWCbOnUqI0aM4JlnnmHlypU0adKEzp07s2/fvgKPX7x4Mf369WPIkCGsWrWKnj170rNnT9avX+85pm7dukyYMIF169bx888/U6NGDTp16sT+/fu9rvXcc8+xd+9ez/bAAw8U6rMWGoda2ERERERE7MjngW3cuHEMHTqUwYMH06BBAyZOnEhISAgffvhhgce/+eabdOnShccee4z69evz/PPP07x5cyZMmOA55rbbbiM+Pp6aNWvSsGFDxo0bR1paGmvXrvW6VunSpYmJifFsoaGhhfqshcXyrMOmwCYiIiIiYif+vrx5dnY2K1asYNSoUZ59TqeT+Ph4EhMTCzwnMTGRESNGeO3r3LkzM2fOPO09Jk2aREREBE2aNPH67uWXX+b555+nWrVq3HbbbQwfPhx//4L/SLKyssjKyvJ8TktLA8DlcuFyuf72WQuLy+XCcco6bL4sixSuvLpVHZcMqu+SQ3VdcqiuSxbVd8lR2HXs08CWmppKbm4u0dHRXvujo6PZtGlTgeckJycXeHxycrLXvm+//ZZbb72VY8eOUbFiRRISEihXrpzn+wcffJDmzZsTFRXF4sWLGTVqFHv37mXcuHEF3nfs2LE8++yz+fbPmzePkJCQs3reQnNKYPvreD6xn4SEBF8XQS4h1XfJobouOVTXJYvqWy6UTwNbYbrmmmtYvXo1qampvPfee/Tp04clS5ZQoUIFAK9WusaNGxMYGMjdd9/N2LFjCQoKyne9UaNGeZ2TlpZG1apV6dSpE+Hh4YX/QKfhcrn4/P1ZgOkS2a1bN5+VRQqXy+UiISGBjh07EhAQ4OviSCFTfZccquuSQ3Vdsqi+Sw6Xy8XXX39daNf3aWArV64cfn5+pKSkeO1PSUkhJiamwHNiYmLO6vjQ0FBq165N7dq1ufzyy6lTpw4ffPCBV/fLU7Vp04acnBy2b99OvXr18n0fFBRUYJALCAjw/S/hiWn9/Zy5vi+LFLoi8TMnl4zqu+RQXZccquuSRfUtF8qnk44EBgbSokULFixY4NnndrtZsGABbdu2LfCctm3beh0Ppqn5dMefet1Tx6D91erVq3E6nZ4WuOLEcWJa/wA/TesvIiIiImInPu8SOWLECAYOHEjLli1p3bo148ePJyMjg8GDBwMwYMAAKleuzNixYwF46KGHaN++Pa+//jrdu3dnypQpLF++nEmTJgGQkZHBiy++yA033EDFihVJTU3l7bffZvfu3fTu3RswE5csWbKEa665htKlS5OYmMjw4cO5/fbbKVOmjG/+IC6AdWJaf/PB7RnTJiIiIiIixZvPA1vfvn3Zv38/o0ePJjk5maZNmzJnzhzPxCJJSUk4nScDSLt27Zg8eTJPPfUUTzzxBHXq1GHmzJk0atQIAD8/PzZt2sQnn3xCamoqZcuWpVWrVvz00080bNgQMN0bp0yZwpgxY8jKyuKyyy5j+PDh+WafLDZODWhWrgKbiIiIiIhN+DywAQwbNoxhw4YV+N2iRYvy7evdu7enteyvgoODmTFjxhnv17x5c3799ddzLmeR5dXClguon7SIiIiIiB2oKcYGHM6/tLCJiIiIiIgtKLDZgAKbiIiIiIg9KbDZgJWvS6SIiIiIiNiBApsNOJynBDa3pvYXEREREbELBTYbcDqduN0nQpta2EREREREbEOBzQacTotct1k823IrsImIiIiI2IUCmw04HCcDmztXgU1ERERExC4U2Gzg1MCWq8AmIiIiImIbCmw24OcHudaJLpEKbCIiIiIitqHAZgNeLWw5CmwiIiIiInahwGYDDodFTq4/AJam9RcRERERsQ0FNhs4dZZITToiIiIiImIfCmw24HSiwCYiIiIiYkMKbDbgtQ6bApuIiIiIiG0osNmAw3FKC5sWzhYRERERsQ0FNpvIm9ZfXSJFREREROxDgc0m3OoSKSIiIiJiOwpsNpHjNtP6u3M1rb+IiIiIiF0osNmE+0SXSEtj2EREREREbEOBzSY8s0QqsImIiIiI2IYCm01k5wYBYOVm+7gkIiIiIiJysSiw2URWTrB5k3PctwUREREREZGLRoHNJvICm5Wb6eOSiIiIiIjIxaLAZhNZrlLmjQKbiIiIiIhtKLDZRFbuiS6RueoSKSIiIiJiFwpsNpGdN4ZNLWwiIiIiIrahwGYTeWPYHG4FNhERERERu1Bgs4lsdYkUEREREbEdBTabyAtsamETEREREbEPBTabcCmwiYiIiIjYjgKbTWTmmGn9HW51iRQRERERsQsFNptwuU+0sFlqYRMRERERsQsFNptQl0gREREREftRYLOJ7FzTJdKpLpEiIiIiIrahwGYTeV0ineoSKSIiIiJiGwpsNpFjBQEawyYiIiIiYicKbDaRi7pEioiIiIjYjQKbTeQ6NEukiIiIiIjdKLDZhJsTY9hQYBMRERERsQsFNpuwnKZLpJ+lLpEiIiIiInahwGYTuSda2PzUwiYiIiIiYhsKbHbhp8AmIiIiImI3Cmw2kdcl0unIBbfLx6UREREREZGLQYHNJixn8MkPuWplExERERGxAwU2m3D6B538oMAmIiIiImILCmw24R/gIDP7RGjL1UyRIiIiIiJ2oMBmEwEBkOk60S1SLWwiIiIiIragwGYTgYEKbCIiIiIidlMkAtvbb79NjRo1CA4Opk2bNixduvSMx0+bNo3Y2FiCg4OJi4tj9uzZXt+PGTOG2NhYQkNDKVOmDPHx8SxZssTrmIMHD9K/f3/Cw8OJjIxkyJAhpKenX/Rnu1T8/eF4tpkpUl0iRURERETsweeBberUqYwYMYJnnnmGlStX0qRJEzp37sy+ffsKPH7x4sX069ePIUOGsGrVKnr27EnPnj1Zv36955i6desyYcIE1q1bx88//0yNGjXo1KkT+/fv9xzTv39/NmzYQEJCAt9++y0//vgjd911V6E/b2FRC5uIiIiIiP34PLCNGzeOoUOHMnjwYBo0aMDEiRMJCQnhww8/LPD4N998ky5duvDYY49Rv359nn/+eZo3b86ECRM8x9x2223Ex8dTs2ZNGjZsyLhx40hLS2Pt2rUAbNy4kTlz5vD+++/Tpk0brrzySt566y2mTJnCnj17LslzX2wawyYiIiIiYj/+vrx5dnY2K1asYNSoUZ59TqeT+Ph4EhMTCzwnMTGRESNGeO3r3LkzM2fOPO09Jk2aREREBE2aNPFcIzIykpYtW3qOi4+Px+l0smTJEnr16pXvOllZWWRlZXk+p6WlAeByuXC5fLdQdd69nc5cT5fInOyjWD4skxSOvLr25c+bXDqq75JDdV1yqK5LFtV3yVHYdezTwJaamkpubi7R0dFe+6Ojo9m0aVOB5yQnJxd4fHJyste+b7/9lltvvZVjx45RsWJFEhISKFeunOcaFSpU8Dre39+fqKiofNfJM3bsWJ599tl8++fNm0dISMiZH/QS2LZtM5kxpoVt9Ypf2b0mwMclksKSkJDg6yLIJaT6LjlU1yWH6rpkUX3LhfJpYCtM11xzDatXryY1NZX33nuPPn36sGTJknxB7WyNGjXKq2UvLS2NqlWr0qlTJ8LDwy9Wsc+Zy+UiISGBRo3qkbnXBLamjevTpEY3n5VJCkdeXXfs2JGAAAVyu1N9lxyq65JDdV2yqL5LDpfLxddff11o1/dpYCtXrhx+fn6kpKR47U9JSSEmJqbAc2JiYs7q+NDQUGrXrk3t2rW5/PLLqVOnDh988AGjRo0iJiYm36QmOTk5HDx48LT3DQoKIigoKN/+gICAIvFLGBzs9HSJ9CfbDGoTWyoqP3Nyaai+Sw7Vdcmhui5ZVN9yoXw66UhgYCAtWrRgwYIFnn1ut5sFCxbQtm3bAs9p27at1/FgmppPd/yp180bg9a2bVsOHz7MihUrPN//8MMPuN1u2rRpc76P41OaJVJERERExH583iVyxIgRDBw4kJYtW9K6dWvGjx9PRkYGgwcPBmDAgAFUrlyZsWPHAvDQQw/Rvn17Xn/9dbp3786UKVNYvnw5kyZNAiAjI4MXX3yRG264gYoVK5Kamsrbb7/N7t276d27NwD169enS5cuDB06lIkTJ+JyuRg2bBi33norlSpV8s0fxAXy91dgExERERGxG58Htr59+7J//35Gjx5NcnIyTZs2Zc6cOZ6JRZKSknA6TzYEtmvXjsmTJ/PUU0/xxBNPUKdOHWbOnEmjRo0A8PPzY9OmTXzyySekpqZStmxZWrVqxU8//UTDhg091/nss88YNmwY1113HU6nk5tvvpl///vfl/bhL6KAADiqhbNFRERERGzF54ENYNiwYQwbNqzA7xYtWpRvX+/evT2tZX8VHBzMjBkz/vaeUVFRTJ48+ZzKWZQFBuIZw0bOMd8WRkRERERELgqfL5wtF0dAAGRkhZoPORm+LYyIiIiIiFwUCmw2ERiowCYiIiIiYjcKbDbh1cKWqy6RIiIiIiJ2oMBmE+oSKSIiIiJiPwpsNqEukSIiIiIi9qPAZhP+/hbHskLMBwU2ERERERFbUGCzCe8xbApsIiIiIiJ2oMBmE+oSKSIiIiJiPwpsNuE96YhmiRQRERERsQMFNptQC5uIiIiIiP0osNmEVwubOwvcub4tkIiIiIiIXDAFNpvwCmygiUdERERERGxAgc0mAgMhyxWE2+0wO9QtUkRERESk2FNgs4mAAACHxrGJiIiIiNiIAptN+PubV80UKSIiIiJiHwpsNuFw/HVqf7WwiYiIiIgUdwpsNuIV2DTpiIiIiIhIsafAZiNai01ERERExF4U2GxEXSJFREREROxFgc1GAgLgWFaI+aDAJiIiIiJS7Cmw2Yi6RIqIiIiI2IsCm414Tzqiaf1FRERERIo7BTYb0Rg2ERERERF7UWCzEXWJFBERERGxFwU2G1ELm4iIiIiIvSiw2Yha2ERERERE7EWBzUbUwiYiIiIiYi8KbDaiddhEREREROxFgc1GAgMh7Xi4+ZBz1LeFERERERGRC6bAZiMBAXDkeIT54Dri28KIiIiIiMgFU2CzkYAAOHLsRGDLVmATERERESnuFNhsJDDwlMCmFjYRERERkWJPgc1GvFrYco+D2+XbAomIiIiIyAVRYLORgIBTJh0BdYsUERERESnmFNhsJDAQ3JYfWblhZoe6RYqIiIiIFGsKbDZSqpR5PZ6rcWwiIiIiInagwGYjoaHm9ZhLgU1ERERExA4U2GwkL7BlZGtqfxERERERO1Bgs5GwE0PXjmbltbAd9llZRERERETkwimw2UheC1vacbWwiYiIiIjYgQKbjeQFNi2eLSIiIiJiDwpsNpIX2A5lKLCJiIiIiNjBeQW2nTt3smvXLs/npUuX8vDDDzNp0qSLVjA5d3mB7eBRBTYRERERETs4r8B22223sXDhQgCSk5Pp2LEjS5cu5cknn+S55567qAWUs5cX2A6kaQybiIiIiIgdnFdgW79+Pa1btwbgiy++oFGjRixevJjPPvuMjz/++GKWT85BXmBLPaIWNhEREREROzivwOZyuQgKCgJg/vz53HDDDQDExsayd+/ei1c6OSd5gW3fYQU2ERERERE7OK/A1rBhQyZOnMhPP/1EQkICXbp0AWDPnj2ULVv2ohZQzp7GsImIiIiI2Mt5BbZXXnmFd999lw4dOtCvXz+aNGkCwKxZszxdJeXS80zrr3XYRERERERs4bwCW4cOHUhNTSU1NZUPP/zQs/+uu+5i4sSJ53y9t99+mxo1ahAcHEybNm1YunTpGY+fNm0asbGxBAcHExcXx+zZsz3fuVwuRo4cSVxcHKGhoVSqVIkBAwawZ88er2vUqFEDh8Phtb388svnXPaiJDAQ/P21DpuIiIiIiF2cV2A7fvw4WVlZlClTBoAdO3Ywfvx4Nm/eTIUKFc7pWlOnTmXEiBE888wzrFy5kiZNmtC5c2f27dtX4PGLFy+mX79+DBkyhFWrVtGzZ0969uzJ+vXrATh27BgrV67k6aefZuXKlcyYMYPNmzd7xtmd6rnnnmPv3r2e7YEHHjjHP4miJzQUDmWYeiH3OORm+bZAIiIiIiJy3s4rsN1444385z//AeDw4cO0adOG119/nZ49e/LOO++c07XGjRvH0KFDGTx4MA0aNGDixImEhIR4tdyd6s0336RLly489thj1K9fn+eff57mzZszYcIEACIiIkhISKBPnz7Uq1ePyy+/nAkTJrBixQqSkpK8rlW6dGliYmI8W2hen8JiLDQU0o6HY+VVbfZB3xZIRERERETOm//5nLRy5UreeOMNAKZPn050dDSrVq3iyy+/ZPTo0dx7771ndZ3s7GxWrFjBqFGjPPucTifx8fEkJiYWeE5iYiIjRozw2te5c2dmzpx52vscOXIEh8NBZGSk1/6XX36Z559/nmrVqnHbbbcxfPhw/P0L/iPJysoiK+tka1VaWhpgumC6XK4zPWahyrt33mtIiD+W5cTlKEOgdQBXRgr4l/NZ+eTi+Wtdi72pvksO1XXJobouWVTfJUdh1/F5BbZjx45RunRpAObNm8dNN92E0+nk8ssvZ8eOHWd9ndTUVHJzc4mOjvbaHx0dzaZNmwo8Jzk5ucDjk5OTCzw+MzOTkSNH0q9fP8LDwz37H3zwQZo3b05UVBSLFy9m1KhR7N27l3HjxhV4nbFjx/Lss8/m2z9v3jxCQkLO+JyXQkJCAgC5ue2BSNKyQigXeIBff5zNQb+zrxMp+vLqWkoG1XfJobouOVTXJYvqWy7UeQW22rVrM3PmTHr16sXcuXMZPnw4APv27fMKRb7mcrno06cPlmXl66p5aitd48aNCQwM5O6772bs2LGeNeZONWrUKK9z0tLSqFq1Kp06dfLpM7tcLhISEujYsSMBAQH8619+bNsGBFYCdtK2eT2syt18Vj65eP5a12Jvqu+SQ3VdcqiuSxbVd8nhcrn4+uuvC+365xXYRo8e7elCeO2119K2bVvAtDY1a9bsrK9Trlw5/Pz8SElJ8dqfkpJCTExMgefExMSc1fF5YW3Hjh388MMPfxuq2rRpQ05ODtu3b6devXr5vg8KCiowyAUEBBSJX8K8cpxo+CTTXRac4J+bBkWgfHLxFJWfObk0VN8lh+q65FBdlyyqb7lQ5zXpyC233EJSUhLLly9n7ty5nv3XXXedZ2zb2QgMDKRFixYsWLDAs8/tdrNgwQJPCPyrtm3beh0Ppqn51OPzwtqWLVuYP3/+WS3mvXr1apxO5znPclnU5M2bkpETZd5kH/BdYURERERE5IKcVwsb4JlZcdeuXQBUqVLlvBbNHjFiBAMHDqRly5a0bt2a8ePHk5GRweDBgwEYMGAAlStXZuzYsQA89NBDtG/fntdff53u3bszZcoUli9fzqRJkwAT1m655RZWrlzJt99+S25urmd8W1RUFIGBgSQmJrJkyRKuueYaSpcuTWJiIsOHD+f222/3LFVQXOUFtvTsKAgEsjRLpIiIiIhIcXVegc3tdvPCCy/w+uuvk56eDpgp8h955BGefPJJnM6zb7jr27cv+/fvZ/To0SQnJ9O0aVPmzJnjmVgkKSnJ63rt2rVj8uTJPPXUUzzxxBPUqVOHmTNn0qhRIwB2797NrFmzAGjatKnXvRYuXEiHDh0ICgpiypQpjBkzhqysLC677DKGDx+eb/bJ4igvsB3NioIwNK2/iIiIiEgxdl6B7cknn+SDDz7g5Zdf5oorrgDg559/ZsyYMWRmZvLiiy+e0/WGDRvGsGHDCvxu0aJF+fb17t2b3r17F3h8jRo1sCzrjPdr3rw5v/766zmVsbjIC2yHj+d1iVRgExEREREprs4rsH3yySe8//773HDDDZ59jRs3pnLlytx3333nHNjk4vEEtmMnApu6RIqIiIiIFFvnNenIwYMHiY2Nzbc/NjaWgwcVEHwpL7AdOHpiohW1sImIiIiIFFvnFdiaNGnChAkT8u2fMGECjRs3vuBCyfnLC2ypR9UlUkRERESkuDuvLpH/+te/6N69O/Pnz/dMp5+YmMjOnTuZPXv2RS2gnJu8wLbvcF6XSE3rLyIiIiJSXJ1XC1v79u35/fff6dWrF4cPH+bw4cPcdNNNbNiwgU8//fRil1HOQViYeU0+dCKw5aRD2u/wNxOxiIiIiIhI0XPe67BVqlQp3+Qia9as4YMPPvCsiSaXXl5gSzkYcXLnt/Xgqq+gak+flElERERERM7PebWwSdFVurR5PZLm5/3FgSWXvjAiIiIiInJBFNhsJi+wHT0KlL385BdWjk/KIyIiIiIi50+BzWa8AttVX0Kl680OV5rPyiQiIiIiIufnnMaw3XTTTWf8/vDhwxdSFrkI8gLbsWOQG1QJv5jrYM+3CmwiIiIiIsXQOQW2iIiIv/1+wIABF1QguTB5gQ0gPR0iAsLNBwU2EREREZFi55wC20cffVRY5ZCLJCgIAgLA5TLdIk8GtqO+LZiIiIiIiJwzjWGzIa9xbGphExEREREpthTYbEiBTURERETEHhTYbChv8WyvwJajwCYiIiIiUtwosNnQaVvYLMtnZRIRERERkXOnwGZDXoHN/8QHtwvcWT4rk4iIiIiInDsFNhvyDmxhJ7/QODYRERERkWJFgc2GvAKb0+9kaFNgExEREREpVhTYbMgrsIFmihQRERERKaYU2GwoL7Clp5/YocAmIiIiIlIsKbDZkFrYRERERETsQYHNhhTYRERERETsQYHNhhTYRERERETsQYHNhk4b2HKOFni8iIiIiIgUTQpsNpQvsPmrhU1EREREpDhSYLOhsBPLrqlLpIiIiIhI8abAZkMawyYiIiIiYg8KbDaUF9jS8vJZYKR5zUzxRXFEREREROQ8KbDZUFSUec3OhowMIDLO7Di4EizLZ+USEREREZFzo8BmQ2FhEBxs3u/fD5RpCg5/yNoPx5J8WTQRERERETkHCmw25HBA+fLm/b59gF8wlGlidhxY6rNyiYiIiIjIuVFgs6m8wLZ//4kdZVubVwU2EREREZFiQ4HNpvIFtqhW5lWBTURERESk2FBgs6nTtrAdXAmW2ydlEhERERGRc6PAZlP5Alt4PXAGQU46pP/ps3KJiIiIiMjZU2CzKa9JRwCc/hDZyLw/tMYnZRIRERERkXOjwGZTFSqYV08LG5jp/QEOK7CJiIiIiBQHCmw2la9LJEDkian9D60Gt+tSF0lERERERM6RAptNFRjY8tZi2/0NfFEaUjVjpIiIiIhIUabAZlNnbGEDcGfBjs8vaZlEREREROTcKLDZVF5gy8iAY8dO7AyMgOjrTh6UsvCSl0tERERERM6eAptNhYdDQIB579XKdu086LnLvD+8BjJTL3nZRERERETk7Ciw2ZTDcXKmyJSUU79wQkhliDgxxf++RZe6aCIiIiIicpYU2GysWjXzun17AV9GX2te98y+VMUREREREZFzpMBmY7Vqmdc//yzgy2q3mNcdn0PWgUtWJhEREREROXsKbDaWF9j++KOAL8tfCWWaQW4mbH3vkpZLRERERETOTpEIbG+//TY1atQgODiYNm3asHTpmdcHmzZtGrGxsQQHBxMXF8fs2Se79blcLkaOHElcXByhoaFUqlSJAQMGsGfPHq9rHDx4kP79+xMeHk5kZCRDhgwhPT29UJ7PV84Y2BwOqPeQeb/tk0tWJhEREREROXs+D2xTp05lxIgRPPPMM6xcuZImTZrQuXNn9u3bV+Dxixcvpl+/fgwZMoRVq1bRs2dPevbsyfr16wE4duwYK1eu5Omnn2blypXMmDGDzZs3c8MNN3hdp3///mzYsIGEhAS+/fZbfvzxR+66665Cf95L6YxdIgFiTkzxf3QLuF2XpEwiIiIiInL2fB7Yxo0bx9ChQxk8eDANGjRg4sSJhISE8OGHHxZ4/JtvvkmXLl147LHHqF+/Ps8//zzNmzdnwoQJAERERJCQkECfPn2oV68el19+ORMmTGDFihUkJSUBsHHjRubMmcP7779PmzZtuPLKK3nrrbeYMmVKvpa44qxmTfO6cydkZxdwQKlK4BcCVi6kb7+URRMRERERkbPg78ubZ2dns2LFCkaNGuXZ53Q6iY+PJzExscBzEhMTGTFihNe+zp07M3PmzNPe58iRIzgcDiIjIz3XiIyMpGXLlp5j4uPjcTqdLFmyhF69euW7RlZWFllZWZ7PaWlpgOmC6XL5rnUq794FlSEqCkJD/cnIcLBli4u6dfOf7x9WC8eRdeQc/g2rVI1CLq1ciDPVtdiP6rvkUF2XHKrrkkX1XXIUdh37NLClpqaSm5tLdHS01/7o6Gg2bdpU4DnJyckFHp+cnFzg8ZmZmYwcOZJ+/foRHh7uuUaFvEXKTvD39ycqKuq01xk7dizPPvtsvv3z5s0jJCSk4Ae8hBISEgrcX65cBzIyIpg6dTktWuTvZtoyszSVgY1LZ/FnQCEXUi6K09W12JPqu+RQXZccquuSRfUtF8qnga2wuVwu+vTpg2VZvPPOOxd0rVGjRnm17KWlpVG1alU6derkCYK+4HK5SEhIoGPHjgQE5E9cH37ox44dEBXVmm7d3Pm+d65LhE2LaVhmJ/Wr7seqMcBMSCJFzt/VtdiL6rvkUF2XHKrrkkX1XXK4XC6+/vrrQru+TwNbuXLl8PPzIyUlxWt/SkoKMTExBZ4TExNzVsfnhbUdO3bwww8/eIWqmJiYfJOa5OTkcPDgwdPeNygoiKCgoHz7AwICisQv4enK0bQpzJoFa9b4ERDgl//EyFgAnCkJOFMSILQiVO5eyKWVC1FUfubk0lB9lxyq65JDdV2yqL7lQvl00pHAwEBatGjBggULPPvcbjcLFiygbdu2BZ7Ttm1br+PBNDWfenxeWNuyZQvz58+nbNmy+a5x+PBhVqxY4dn3ww8/4Ha7adOmzcV4tCKjdWvzetqVEkr/ZWBb6uJCLY+IiIiIiJw9n3eJHDFiBAMHDqRly5a0bt2a8ePHk5GRweDBgwEYMGAAlStXZuzYsQA89NBDtG/fntdff53u3bszZcoUli9fzqRJkwAT1m655RZWrlzJt99+S25urmdcWlRUFIGBgdSvX58uXbowdOhQJk6ciMvlYtiwYdx6661UqlTJN38QhaRVK/O6cSOkpUG+3pt/DWzHdl+ScomIiIiIyN/z+bT+ffv25bXXXmP06NE0bdqU1atXM2fOHM/EIklJSezdu9dzfLt27Zg8eTKTJk2iSZMmTJ8+nZkzZ9KoUSMAdu/ezaxZs9i1axdNmzalYsWKnm3x4pOtR5999hmxsbFcd911dOvWjSuvvNIT+uykQgWoUQMsC5YvL+CAoLJQ5caTn4/+fqmKJiIiIiIif8PnLWwAw4YNY9iwYQV+t2jRonz7evfuTe/evQs8vkaNGliW9bf3jIqKYvLkyedUzuKqdWvYvt10i7z22r986XDA1TPh0Br4vqkCm4iIiIhIEeLzFjYpfHndIgtsYctTurZ5zTpgNhERERER8TkFthLgRG9RTrO0neEfCiFVzfs0tbKJiIiIiBQFCmwlQN0T84ps3Qq5uWc4MG8CkoR28NurhV4uERERERE5MwW2EqB6dQgMhKwsSEo6w4Hhp8wYue4ZyDle6GUTEREREZHTU2ArAfz8oPaJIWq/n6m3Y81/QPmrzPvc47B3TqGXTURERERETk+BrYSoV8+8bt58hoPKtoSOP0LsI+Zz0heFXi4RERERETk9BbYSIm8c2xlb2PJU62Ned34Jv70C7jMNfBMRERERkcKiwFZCnFULW56yraB6P3C7YPXjMP8qOLiiUMsnIiIiIiL5KbCVEHktbGec2j+PwwHtPoPLP4KAcEhNhDktYc2ThVpGERERERHx5u/rAsilERdnctiuXZCcDDExf3OCwwE1B0H0tSaobf8vbHgJ0v+E9G2QfRAuG3Cy5a1MUxPuqt4CfqUAN2SmgGVB5Imbi4iIiIjIOVFgKyHCw6FhQ1i/HhIToVevszwxtBq0+xRCq8OGF2HHlJPfrX365PtdM83ryhH5r1GpOwSXN+u87f8FDq2GsJqQmQzl2kLscMABKT9A9HWwZzaUrm3C4vE9EF4fctIgeT74hUKlLuBQ47CIiIiI2J8CWwnStq0JbL/+eg6BLU/j58AZZFrWyrYyrWebxkPFjiZ8ZeyAtE2w78eT5/iXNssD7Pku//WO7zavR7fAtv+c+d7+YZCTfvJzWE0o0wxyMyGigfnOlQaVe0BIVdjzvWntq9EPSlWGY7tOXCcE/ELAL1gtfiIiIiJSLCiwlSBt28J775kWtnPmcELc0977YofnPy77sAlFTn/AYVrTtn0KAaXhwFIIKme6UmamQGAkbP8MkqaBlWvC15HfIKKh6XaZe8yEq7ywFh4Lx5NPdMv80+w7NQxu/8y7LKtHQsgpgS2PfxhEtYQj6yAiDmoOhFKVTEtebia4DkNINSgVfR5/UCIiIiIiF48CWwnStq15Xb4cXC4ICCiEmwRGen+Oama206l8PTR/A9zZpvtlzjEzBs6VZrZSMXBkowlUweXAdRT2/Q+ObgW/IDiwzIQ6/9KQPA+O7YZyl4PriGntO7YLHP7g8AN3lrlnTjrsW2Te71t08v2pHE6IjjddM7d/ZsoUWs0EufB6UL0vHFpj9pW/Si12IiIiIlIoFNhKkLp1oVw5SE2F2bPhxht9XaITSp0yA4p/iHkNjDAbQJnGJ78PKG1CXp46955yoVe8r5u+3XTTLN/OdJF055gummmbTNCLqA+7v4XD6+HYDtM90z/MbMf3mACYPM9cy3XEjLk7sNR83vDiyfuUbQMNnzRBsGxrOLIBnIEQE3+ipVFERERE5PzoX5MliNMJQ4bAK6/Aa68VocBWWMJqmC2P0x+cpc0YvLKtzL7oa05+b1knW8qO/mFmxkzbDDVuMy18GTsgIwl2zzITpITXM58PLIEfb8h//8AyUKY5VL/VtNSFx3qHUxERERGRv6HAVsI8+CCMGwc//2zGsuV1kxS8uzWWrgVxz3h/H9XcvMY+BDnHTVfMzBRY+Qjs/9mMzzu0EkKqmLFwWamQssBsYLplVullJkNxuyCqhQlyIiIiIiKnocBWwlSqBHfcAR9+CE88AT/8oOFX58W/lHktFQNXnDLZSU6GGe/mzjGTmiTPhx1fmPF46Vth53Sz5YmMMyGuSk+zlp0qQ0REREROocBWAj3zDHz2GSxaBHPmQNeuvi6RjfiHmle/QNOCFtUCGow0+w6vg9//D1J/MaHu4Aqz7/A6WP8cBMdAxU5QrY8ZA3d8L1S58eRYPhEREREpcRTYSqBq1eC+++CNN+DjjxXYLpnIOGj9zsnPWQfNpCe7voK988ykJtv+470uXUAEXPmFmYnSbV36MouIiIiITzl9XQDxjS5dzOvq1T4tRskWFAU1B8DVX8EtB+HaBVDvIQiINFvpOmZ2yp/7wFcx+M+JI8y903S33PeTGUcnIiIiIramFrYSqmlT87plC6SnQ1iYT4sjfkEQc63Zmr8BWGZikvlXe5YScLjSuJp/4j//XTiyHipcbUKelg4QERERsS21sJVQFSqYCUgsC9at83VpxIvDYRbu9guCq740Y9qa/gt32bYEcBzHkfXmuH0/wvJhJ8fBiYiIiIjtKLCVYHmtbKtW+bQYciYhVeDKqdDgMXKvWcivQU+QG/sYNB9nvt/6LsxubLbVT8CXFeCXfiaJi4iIiEixp8BWguUFNo1jKyYcTlL8W+OOexFih5vJSMo0NTNOAvw2FrL2w44pkDTNp0UVERERkYtDga0Ea9bMvK5c6dtyyHmq1hu6roLeaVDmRGVyYh23ZffC9slqaRMREREp5hTYSrC2bc3rypWQmurbssgFcPpDu8+g8g1w7Tyz9lv2QVjc38wwmbnP1yUUERERkfOkwFaCVa5sukVaFnz/va9LIxckoj60/xpi4qHjLxD3HDj8Yed0mFULtk7ydQlFRERE5DwosJVw119vXr/91rflkIvILwjinoZOi01rW046LL0b1r8AWQd8XToREREROQcKbCVcXmCbMwdSUnxbFrnIyraCzkuhwUjzee3TMCMafugEW9+DnGO+LZ+IiIiI/C0FthKuVSuoUwfS0qBjR7OIttiIwwlNxkKr/zMzSlq5kJwAS++CHzpCbravSygiIiIiZ6DAVsI5nfDddxAdbRbQ/vxzX5dILjqHA+rca2aUvP53aPISBERA6mKYfzWseswsvJ2xQ7NKioiIiBQxCmxCnTpw773m/dy5vi2LFLLwOtBwFFzxOeCAA0tg42tm4e2va8CPN0JOhq9LKSIiIiInKLAJAJ07m9cFCyAnx7dlkUugUlfotgZavwuVuoEzyHSf3P0NLOoObpevSygiIiIiKLDJCa1aQWQkHD4My5b5ujRySUTGQe27oMN3cGsmxP8MAeGw73+w6p/qHikiIiJSBCiwCQB+fhAfb95/8YVvyyI+Ur4tXP6Reb95vBnftvF12D4FXEd9WjQRERGRkkqBTTwGDTKvEybA2rU+LYr4StWboOUE8CsF+3+GVY/C4n4wt7WWARARERHxAQU28ejeHW66yYxhu+km+OMPX5dIfKLu/dD9NzObZLXeEFQO0jbByuFguX1dOhEREZESRYFNvEyYADVqmLDWsiVMmgRu/Ru95AmrYWaTvPILaDfZ7Ns6CeZ3gAMa5CgiIiJyqSiwiZeKFSExEdq0MROQ3H03tG8PO3b4umTiMxU7Quv3wD8U9v9kukcuuROyj/i6ZCIiIiK2p8Am+cTEwM8/wxtvQGioeX/llbB4sSYOLLFq3wnd1sNlAwAH/PGBmZQk66DpJnk8xdclFBEREbElBTYpkL8/PPwwbNgA9evDrl1wxRVQrZqZTbJnT9N9cuZM+P13HxdWLo2wGtD2E4j/HwTHwOG1MO9y0+L2VQyseVqJXkREROQi8/d1AaRoq14dfvwRHnnETPe/a5fZAL7++uRxsbHQsCEcOQJRUVClCmRlma1jR9izB5xOqFcPWrc25+TmmmMdDvM571WKuApXwbXz4Ydr4eiWk/s3vABY0Ph5VaaIiIjIRaLAJn+rXDn45BN4+21YsQJ27jQBLCHBBLRVq2DTJrMV5P33T3/t8HDTKGNZEBcHQUFmv8NhtsBAE/6aNjXvc3NNeUJDYd8+qFrVTI4SHn7RH1vOJLIhXL/ZdI08vgcCy8Dap2HDi+DOgiZjwam/XkREREQulP5FJWctLMxMQJLnn/80rwcPmnFu27aZFrMDB0wrnJ8fZGTA//4HtWubz7/+Crt3n7xGWtrJ94mJ51cuhwPKlIFDh8x7Pz/TmlepkunGefCgCXnVqpnxeaVLe29RUVChggl9ahg6B4GRUP+Rk5/9w8zU/xtfg9REs55bmaa+Kp2IiIiILSiwyQWLioIbbji7Yy0LMjPNGDnLMuPfnE7TcrZ5s3nNOw7g+HH4809YudIEMYfDBMKjR6F8edi61cxgefDgyfPyliHYts1sZysgwAS3ChVMOC1d2ixxULOm2cqVg5AQEw4vu8yUJTPTlCsg4OzvY1uxD0NwNCy9G/b/At83h1bvQJ27fV0yERERkWJLgU0uKYcDSpU6+blRo5Pv4+LO75opKaZ7ZNmy5rPbbYLf6tWwdq1pVTtwwAS7/ftN2Dt1O3jQvLpcpvXv1BbA04mIMN039+0zLXO9epnylyljgl7DhtCgwfk9T7FWox+UbwcrH4Wd02HZPbDhJag9FBo95evSiYiIiBQ7CmxS7EVHm+2vqleHG288u2scP27C3L59Zjt2zHSxzGul+/NPsy7dsWPmuCOnLEGWlmbG+BV0fzChtHVrM/nK5ZeXgG6XodXNgturH4eN/4JjSWZ8W0g1qDnA16UTERERKVZ8Pq3/22+/TY0aNQgODqZNmzYsXbr0jMdPmzaN2NhYgoODiYuLY/bs2V7fz5gxg06dOlG2bFkcDgerV6/Od40OHTrgcDi8tnvuuediPpYUM6VKmTFuLVtCt25wyy0wdCi89BJ8/jksWWK6bO7caQLa2rWwZo1p3Vu4EJ54Am69Fbp3N+Pm/P1Ni96OHfDdd/DMM9CunelKeeONMGoUTJliJmrJ6wZqKw4HNHsFbtgG9U8Mdlx2N+yZ49tyiYiIiBQzPm1hmzp1KiNGjGDixIm0adOG8ePH07lzZzZv3kyFChXyHb948WL69evH2LFjuf7665k8eTI9e/Zk5cqVNDrRty4jI4Mrr7ySPn36MHTo0NPee+jQoTz33HOezyEhIRf/AcWWAgO9u29WqAAdOngfs2+fCXiWZbpmJiaaZRDyQtysWd7n9+ljulnWqWMWKa9V61I8ySUQVgOavARpm2D3LPjxBrj8PxBcASLjILi8r0soIiIiUqT5NLCNGzeOoUOHMnjwYAAmTpzId999x4cffsjjjz+e7/g333yTLl268NhjjwHw/PPPk5CQwIQJE5g4cSIAd9xxBwDbt28/471DQkKIiYm5iE8jclLe5CUAV18NDz5oWuaWL4fffjMLkq9ZY7Z9+8wi5KeKiTGhsHNn0+IXG3vpn+GicfrBldMg8Q5I+gIW9zP7Q6rCdQuhtF3SqYiIiMjF57PAlp2dzYoVKxg1apRnn9PpJD4+nsTTzO+emJjIiBEjvPZ17tyZmTNnnvP9P/vsM/773/8SExNDjx49ePrpp8/YypaVlUVWVpbnc9qJ+ehdLhcul+uc73+x5N3bl2WQs1OqFFx1ldnyuFzwzTcOFi92kJUF69c7WLLEQXKyg+Rks9bdo49C2bIWTZs6KV++HhUr5lC5spkls/iMh3NA60/w8wvFue0jLIcfjmM7sea1I7fZeKwqvcDh5+tCFin63S45VNclh+q6ZFF9lxyFXcc+C2ypqank5uYS/ZfZIqKjo9l0mhWYk5OTCzw+OTn5nO592223Ub16dSpVqsTatWsZOXIkmzdvZsaMGac9Z+zYsTz77LP59s+bN69IdKdMSEjwdRHkPAUHw7XXmvddu8Lx437s3Fma33+PYuXKCqxbV44DB/xYsMAPiGXKFHNsxYrpxMcn0a3bn5QqVUwGwlk3UKpUWyz8uTzrOSKytuP/620cd5RlVeAwjjvL4SKULGeUr0taZOh3u+RQXZccquuSRfUtF6pEzhJ51113ed7HxcVRsWJFrrvuOv744w9qnWbw0KhRo7xa99LS0qhatSqdOnUiPDy80Mt8Oi6Xi4SEBDp27EiAFgOzpawsN+vXWyxd6uY//znIb79Fc/y4g717w/j00wbMmlWfLl0smje3uOEGd/EZ/5bbm9yNL+Hc+g6lXAdom/U8DtxYQdHkdFoDgSU7tOl3u+RQXZccquuSRfVdcrhcLr7++utCu77PAlu5cuXw8/MjJSXFa39KSsppx5bFxMSc0/Fnq02bNgBs3br1tIEtKCiIoKCgfPsDAgKKxC9hUSmHXHwBAWY5gBYtXFSvvoRu3bqRlRXA9Onwwgvwxx8Opk51MHUqjBzpR+fOMHy4Gf9WpAUEQLOXoPFoSByII+kLABxZKQT89iy0fLs49fksNPrdLjlU1yWH6rpkUX3LhfLZtP6BgYG0aNGCBQsWePa53W4WLFhA27ZtCzynbdu2XseDaWY+3fFnK2/q/4oVK17QdUQulbAwGDTIzET500/w7LNmnTeHA+bOhS5d4O67vdeLK7L8gqHdZGj/DbT50Ozb8g581xD++ADc6vsvIiIiJZdPu0SOGDGCgQMH0rJlS1q3bs348ePJyMjwzBo5YMAAKleuzNixYwF46KGHaN++Pa+//jrdu3dnypQpLF++nEmTJnmuefDgQZKSktizZw8AmzdvBkzrXExMDH/88QeTJ0+mW7dulC1blrVr1zJ8+HCuvvpqGjdufIn/BEQujJ+fWQbgyivN5z//hPHjzayTkybBjBlw111QtqyZefKGG0zYK3KcflD5evM+/Q/Y+CqkbYQld8KGl6DO/VC1F4Rd5ttyioiIiFxiPl04u2/fvrz22muMHj2apk2bsnr1aubMmeOZWCQpKYm9e/d6jm/Xrh2TJ09m0qRJNGnShOnTpzNz5kzPGmwAs2bNolmzZnTv3h2AW2+9lWbNmnmm/Q8MDGT+/Pl06tSJ2NhYHnnkEW6++Wa++eabS/jkIoWjZk3497/N7JKxsZCaahb/fuQR6N8fKlc2YS4nx9clPYMmL8BN+6D5OLNeW/qfsOoR+LY+JM/3delERERELimfTzoybNgwhg0bVuB3ixYtyrevd+/e9O7d+7TXGzRoEIMGDTrt91WrVuV///vfuRZTpFi57jpYuxa++gqmTQOnE1auhK1b4YEH4I03TJfJ226DKlV8XdoCBEZA7HCofRf88SFs/y8cWAr/uwEq3wCxD0N4fbMUQEBRbDIUERERuTh82sImIoUnIAD69DGBbepU2LQJ3n4bypUzXSdHjoT69c2+06yk4Xv+oVDvAYj/ESp2gdzjkDQV5rWF6WVgVk048puvSykiIiJSaBTYREoIPz+47z7Yvh0mToSWLSE9HYYNM8GtTx/zXZHkFwQdvjPB7bKBJ3ZakLUf5raBb+rBlolgWT4tpoiIiMjFpsAmUsKEhprukL/+Cq+8YpYM8PMzLXG1asGQISbIFTkOJ1S4Ctp+DD13QvffIKIR5KTD0d9h2b2woANseBm2fQbuYrKYuIiIiMgZKLCJlFB+fvDPf0JiIixbZsa9ud3w4YfQqhWsX+/rEp5BSBWIqA9dV0KnRGj6L3AGwL4fYc0oSLwdfr4FDixTcBMREZFiTYFNRGjWDObPhx9/NDNJbtoErVvDBx8U8V6GzgAodzk0eAx6/AENn4Ia/cEZCLtmwtzWMDsOVgyHNU9BysIi/kAiIiIi3hTYRMTjqqtg1Sqz8Pbx43DnnRAdDddfb9Z1c7t9XcIzCK0KTZ6Hdv+F6xZBpe4QEG7Wc9s8Hja8CAuuhcW3wb6fIWk67P/Fx4UWEREROTOfT+svIkVL+fLw3Xfw6qswZgzs328+f/cdTJ8On3wCFSv6upR/o3xb6PAtZB+BP96HzH1wfC/s+Bx2TDFbnssGQO27IbQGlKoIDofPii0iIiLyVwpsIpKP02mm/X/oIVi9GhYsgBdfNAtyN2liJi2pVg2uuQZq1/Z1ac8gMALqP3Lyc+27YO3TcGynaX07tBq2/cdsAGG1odZgiL4OyrZWeBMRERGfU2ATkdMKDjazSF5+Odx0E/TrB2vWwAsvnPx+8WIzBq5YqHAlxC88+Xnfz7D5Tdj/k1kiIH0rrHkSeBJCq0OpSlD+Kqh2C0Q0BP8QnxVdRERESiYFNhE5K/Xrw5Il8MUX8P33sHYtbNgAN95oFt++/vpi2CBV4UqzAbjSYftnsHcuJCdAxg6zpSbCxn8BDgirCZGNzHICEY0gsiGUrgd+gT59DBEREbEvBTYROWtBQXDHHWY7fNjMJLllC9xwA9x8M7z3HpQp4+tSnqeAMKhzt9lc6SaoZe6DpKlwYIl5n/6H2XZ9ffI8hz+E14Mat0HNwWYcnIiIiMhFosAmIuclMtKs4favf8Ebb8CXX8Iff8DChea7Yi0gDCp2NO8v629eM/fBkQ1weD0cWX/idQO4jpjXNU+aLag8BEd7b6WiIawWVOqmbpUiIiJyThTYROS8lS0Lr7wCvXtD9+5mgpIOHczi282b+7p0F1lwBbNFX3Nyn2XB8d2Q/AP8/hYcXGHGwmXtN6Hur/zDoFw7KNfWrCGHBVGtICcdyraC0GqX7HFERESkeFBgE5EL1rIlzJsH115rJiVp1QoefBBGjy7GXSTPhsMBIVWg5gCzuY5C+p+QmWJa5DJTTm77foKMbZA8z2wFCSoLwRWhUhccFTriZ2WZiVCCwiGk8qV9NhERESkSFNhE5KJo0sRMQjJ8OEyZAuPHw0cfmfFu991nJi2xvYDSUKZJwd9Zbji0xoyNO7DE7Ms5BkfWgV+IWWIg64DZjqzHf+NrXA/wPYADoq+FyDhwOKFsG6hyA/gFm+vkZoEzsBjO+iIiIiJ/R4FNRC6amBj4/HMYOBAeewzWr4cJE8xkJG+8AffcU4IzhcMJUc3Mxn35v8880ZXy8HrY8x3W3gQcmXuxnIE43NmQssBsnuv5m9Y9dzYc32PCXK27oOpNZv05v1LmniIiIlKsKbCJyEXXpQt06gRz58Kbb5rX++4zSwG89JLNu0mer+DyZotoANX7kJOdzYLZn3Ndl14EZKeYbpQZOyA3E3Z9Bcd2Qcb2k+cfXgcrHjAbgMPPLP4d2eREV8sKEHoZlK5lliJw+vnkMUVEROTcKLCJSKFwOqFrVxPeXn0VHn8cJk6ETz6Bbt3gkUegbVtfl7IIczjIckSalrLwOmbL0+JNM9lJxk7Tilaqkllq4I/34PBac4yVa7pfpibmv3ZINTMLpjPIrC13fI/pZlmmGfiHQkilS/KIIiIi8vcU2ESkUDkc8M9/QsOG8MQTppXtyy9h1izT+jZ0KPjrb6JzkzfZSUiVk/vqDTOb5TZj47L2QcoiyEiCrFTITIb0bXD0dziWBH98cJprO6FKT9j3I2BB5Ruh5VtajkBERMRH9M8kEbkkunc3LWsrV8LYsSa03XcfvP66aX0bNEjB7aJwOM06cgFhpvXsr3IzIWkapG83ywlkbIPAMrDzK3ClgTsLds44efyfH8LuWaY1zp0JudlQoT00fMJ0rwyINOEwOAacqkAREZGLTf91FZFLxuGAFi3giy/MJCRjx5rFtocOhXfeMROVdOhgJi+RQuIXDJfdkX9/q4mABTu/hB1TocbtZubJXweYFrpT7fnWbKcKq22u6wyEyEYQVAHSNkF4PSjXptAeR0RExO4U2ETkknM6zRi2e+6BSZPg+edNy1u/fifHvr3zDlSt6uuSliAOB+CAar3NlueGP83MlX7BZss5BhtegtTFZq05LHNc+lZY90wB1/WDBqNMkMtJM10sK1x5KZ5IRETEFhTYRMRnQkPNum39+sG//gWLFsGqVfDdd9CsmQltt9xSgpcCKAoCwqF8O+99V5/oMul2QfZhE8Z+f8ssGp6bCUd+M90kA8IhbTNseOHkuRtfgxr9TZfK43vAdaJbZvmroGzLS/ZYIiIixYUCm4j4XEwMjBtn3m/aBP37mxa3Pn3MgtuDBsGwYRCieS+KFmeAWYoAoNFT+b+3LBPkUn4wXSRzj8GOz2H7Z2Y7lcMJIdXBddgEuuxDULoOVO8H4XUL/VFERESKKq2qKiJFSmwsLF4Mo0dDYCBs3AgjR0L16nDvvbB6ta9LKGfN4YB6D8LVM6HNJGj3X+i4GMJjzcLfEQ0gqhVU6GBmt8zYZoLa7xNMoFs3Br6tBz90htVPwFdV4LtG8Nu/TBgUEREpAdTCJiJFTlAQPPus6S45YwY89xzs2GHWcZs4Ea68Eh54AHr1goAAX5dWzkm5NnD9RtOd0nlK5e2Za1rXHE7Y/S2E1oCDy2HP92bR8OR55rjju2H1SDOurvnrpoXv+F4z06VfsC+eSEREpFApsIlIkRUZCf/4BwwYAD/8AB9+aJYD+Plns0VHQ9++cNtt0Lq1xroVK86/JO1KnU++P3XSk/RtsOX/YN/PUPsus/TAqhGw/VPTvTKwDGTth6DyUPd+iL7GtNo5/fPfQ0REpBhSYBORIs/fHzp1MtuePfDuu2ZLSYF//9tstWqZ4Hb77VBXQ57sI+wyaPaq974yjWHVY3BwhQlrYF7XjTFbnrKtoWIX8CsF1fvCsd0QUR+Cyl6q0ouIiFwwBTYRKVYqVTLdJZ98EhISYPJkmDnTrOf2/PNma9cOBg82k5aEh/u6xHLRRV8DnZfBsZ1maYHStWHX16YrZeovposkwIGlZgNYM8q8+odC+ashqBzExENUM7Pod1A5NdGKiEiRpMAmIsVSYCB07262jAyYNQv++1+YO9dMWrJ4MTz4oBnv1qmTCW9Vqph13sQGHA4IrWY2gJoDzWZZkHUAcjPgj4/MmLdDa+DgMgiIANcR2Pu9OWf7pyevF9kEWk+Ecpdf+mcRERE5AwU2ESn2QkPNWm79+sHevfDpp/DRR2aJgIQEsz32GAQHmwAXH2+2pk3Bz8/XpZeLyuGA4HJAOWg8xuyzLBPUAiLMEgMZO+DoVti3CI5sNJOdHF4D89pCZGNwZ0P126DBSMg9bmavLFUJgiv47rlERKTEUmATEVupWBH++U8T0NauhZ9+Mi1vS5dCZibMn282gDJlTHD7xz/guus046RtORwQGGnex1yX//vMVFj9T9j2Hzi81uxbNxp+ewncOWDlmH1lmkPcGKjS41KUWkREBFBgExGbcjigSROzDRsGOTnw+++wYIEJbIsWwaFDMG2a2UJDzbGxsea1Tx+zoLeUAMHl4PIPIe4ZOLDMtMatffrkWLigspB1EA6thJ96Qr2HzRpylbpBqYq+LLmIiJQACmwiUiL4+0ODBmZ74AET4JYtg6lTTQvcgQMnx74BPPQQRESY49u0gWbNoEYNaNHChDuxodDqZgOoOdh0m3QGmJkqM1PNxCV/vA+bxpljHE6ofS9UvxWCoiCkGgSE+a78IiJiSwpsIlIi+ftD27ZmGzcONm6E9evN67x5kJgIR46Y18TEk+f5+UHz5mYsXLNmULWq2S67TBOa2IrDCeGnrA8RXA5aT4KybczMk4fXwoElsOVts8HJ5QOyD0NMR6hyo1knzj/EJ48gIiL2oMAmIiWe0wkNG5oNYMwY011yzx5YtQqWLIHffoMtW2DnTtMyt2yZ9zWio034a9TIXKd+fTNGrmJFjY2zDYcDat9pNoDkH2DDC5CxE7JSzeQlf35svts1E5bfb1roIpuAww8qXw+Vb4DweuAX5KOHEBGR4kaBTUSkAGXKmK1hQ7MYd56kJDORyeLFZhbK3bthxw6ziPfMmWY7VWCguUaTJtC48clAV7Gilv0q9mKuNRuYmSh3fwMpC80EJ398AMf3gNsFB5ebYw4sMWPjAiKgRn+4bAD4h0FwNZ89goiIFH0KbCIi56BaNejf32x5srJMt8k1a2DDBrP9/jukpUF2tmmlW7XK+zqRkSa4xcZCuXJQq5YZL9ewoflOihmHA6rcYDYwE5hYFhzdAkfWm26S2z41ywdkH4It/2c2wD8gkhbuRjjXJUJwWYhqAeWvMK1zR36D/b+YpQWq9TULfYuISImiwCYicoGCgqBDB7Odyu2G7dtNkFu71mwbNpiulYcPwy+/mO2vKlaEhg39qFKlFpblIDjYdKuMiDAtdepiWUw4HGYcXN5YuFr/AMtt1oLbOgn2zgMrF4frMFX4GTb9fPLcgEjz6jp8ct+m8dD4Wah998llCkRExPYU2EREConTCTVrmq1Xr5P7MzNh82YT3rZuhdRU8/m332DXLrP49969TqARH3/sfc2oKNMqV7UqxMWZa9eoAbVrm5Y6dbMs4hxOiIk3G4DlJmfPD2xe/Cmx1cPwy06FfT9CZrL53i8EyrUx71MWwurHTbfK8ldCxa5QqStENFTFi4jYmAKbiMglFhx8co24vzpyxMxUuWRJLv/5TypOZ3lycpxkZ5sgd/DgyaUHpk71PjcyEqpXhwoV4KqroGlTCAsz++PizMyYUsQ4nFgV2rM1MIO6zbrhFxAA7lwzE6UzAMo0Ma+W20xosukN08UyZeGJAPdPM3Nl9X5mge9Slc3slP6lfP1kIiJykeg/3yIiRUhEBFx+ObRo4aZGjV/p1q0bAQFmvYC8teP27jXdKn/7zUx48uefpmXu8GGzASQkeF83KMiEuZo1zRIE9eubwBgVZQJdxYpmyQIpApx+UL6t9z6H03SprPUPsz7cnu/Ntm+hmczkwJKTx4bVNLNRlq4NZZqBXzCEx2p5ARGRYkqBTUSkmMhbO64gmZnwxx8muP35J/zvf2bik6wsM5PlkSPm8++/F3x+qVJm0pO4OLM1aGB62VWrZrpgqsddEVK6NtR7wGzHk+G3f8GxHeAMhn2LIP1P2Dze+xyHEyrfCNVugeAYCI6GkEpmnTgRESnSFNhERGwgONh7Lbl77z35ndttWuK2bTPbn3/C6tVm3NyRI6ZV7vhxWLHCbH9VsSJccQWULWta5+rWhTp1zLi54OBL8XRyWqVioMW4k59dR03XyWM74eAKSN8GOUfNOnG7vjLbqaJamiUGyrUzE5wc32Na46JamK6YIiLicwpsIiI253SaoHXZZQV/n5trQty6dWZbv96sMedwmK6Xe/fC9On5z8trgcsLcHXrntyqV9eYOZ8IKG1a3v7q8Ab4fQKkbTITmmSmmOUFDi4/uU6c13UioFJ3qNzDTHoSGAX+oeBUpYqIXGr6m1dEpITz8zOBq04duOkm7+8yM80ac6tXw6FDJtjlda08csS03O3YkX/MXECAGS9Xt65ZLPzaa82YufLlL9ljyakiG0Lrd7z3Ze6DHV9A0hTISDJhLzjGrBWXdQB2TDbbqQIiILw+VO8DkU3M8gKla5vZLHPStdyAiEghUGATEZHTCg6Ga64x26ksyyxHkBfetmzxfp+3dMHmzfDNNzB2rDmvbFkz4clft2rVNE7ukguuAPWGme1U7lwzicmur8xMlIfXgttlvnMdgQO/mi2Pww+cgZB73IS4St1MS1ypilDhGjj6u1kIPKjspXs2EREbUWATEZFz5nCY1rLy5c34tlO53Waik99/N4Ht11/h55/NIuIHDpj3P//sfU5UlBl/V6kStGwJnTuba0dGapzcJef0g/LtzAYmnbuzwJUOWfshOQH2zoX0PyD7sOlemXvcHHt4jdnyXTPQLDcQ1RL8SkGZpiYElm0JAeGX6slERIolnwe2t99+m1dffZXk5GSaNGnCW2+9RevWrU97/LRp03j66afZvn07derU4ZVXXqFbt26e72fMmMHEiRNZsWIFBw8eZNWqVTRt2tTrGpmZmTzyyCNMmTKFrKwsOnfuzP/93/8RHR1dWI8pIlJiOJ1mYe+qVeG66+C++8z+Y8dMgNu48eT222+mRe7gQfjpJ3Pc1Knw2GPmfVCQ6UqZ99fz1VdDt25mPF4pLTV2aTgcZmkAv2AILgcR9aHegye/z0gygS2wjAlyyQvMhCUpCyFjG4RUN69J08x2qoAIKNfWtMZV7mGWIcg5Cs4gCK1m7ul2aQIUESnRfBrYpk6dyogRI5g4cSJt2rRh/PjxdO7cmc2bN1OhQoV8xy9evJh+/foxduxYrr/+eiZPnkzPnj1ZuXIljRo1AiAjI4Mrr7ySPn36MHTo0ALvO3z4cL777jumTZtGREQEw4YN46abbuKXX34p1OcVESnJQkKgWTOznSo724yR277djJH76Sf44QfTrTIrC5YuPXnsN9+YMFeqFNxwA9x2m1mGIDraXF98ILTayfeX3WE2ONEylw1+QXBoNWz7r2mhy0qFI7+Zhb6P7YK9c8zxf37kfV3/UAitAUc2mC6VMZ0grBYEnxgIGdnYdLN0+Ks/rYjYmk8D27hx4xg6dCiDBw8GYOLEiXz33Xd8+OGHPP744/mOf/PNN+nSpQuPnfhfr88//zwJCQlMmDCBiRMnAnDHHeY/FNu3by/wnkeOHOGDDz5g8uTJXHvttQB89NFH1K9fn19//ZXLL7/8Yj+miIicQWAgtG5tNoDHHzfdKh0O061y/XrTApeRAdOmwYYNZsKTqVPNlqd6dejTx6xV16KFaeHTv+N9yOEwYQ1MF8gyTb2/t9yQPB+O7TbdKJMTIO13CIyA3CwzicmRDebY/b+YrSClKptA5x8G2QfMGLqKXcwSBw4HlK6r2S1FpFjz2d9g2dnZrFixglGjRnn2OZ1O4uPjSUxMLPCcxMRERowY4bWvc+fOzJw586zvu2LFClwuF/Hx8Z59sbGxVKtWjcTExNMGtqysLLKysjyf09LSAHC5XLhcrrO+/8WWd29flkEuDdV1yaL6NmrWNFue++83DTerVsHkyU6++87Jrl2QleVgxw549dWTxwYHW1SpAlWrWlSrBi1aWFSubNG4sUX16pf+WU6nRNd1uROz2VS7HRpjKtfhAMvCcXApZGzHiozDmTwPR9pGSP8TR/YB03J3dAsOLDi+G5K+OHnNXV973cLyD8OKam26VzoDsSIaYVW4FtyZWCHVIKyOOdB1CALKFGrKL9F1XQKpvkuOwq5jnwW21NRUcnNz840bi46OZtOmTQWek5ycXODxycnJZ33f5ORkAgMDiYyMPKfrjB07lmeffTbf/nnz5hFSBPrhJPx1Tm2xLdV1yaL6Pr1rrzWbZUFGhj9r1lRg5coK/PlnBElJ4WRmOtm6FbZuNf8A/+QTc57TadG48X6iozMoUyaLuLhU6tc/gNPpw4dBdV2w0sB2oO6J7SRnSBZ+ZBGZ+welrZ34W2bikxo5cwm0jpJDKfzIxj8nHce+H06euHsm/PaC52MWEbgd/pSyDnDcEYWf5SLDGcN+v8YEWMdwOULJcoST5qzOcUc5gq1D5BLIUWdV3ARi4TznkKe6LllU33Kh1EfgLI0aNcqrdS8tLY2qVavSqVMnwsN9N8OVy+UiISGBjh07EhCgQdl2prouWVTf565Pn5Pvs7Nz2b07l127HCQlwZYtDlaudJCc7GD1agerV58cJz1lClSubNGunUVkpMWQIW6aN7905VZdX3xuwAlYVi6uIxtwHFwGOHDkZuDY/zOOA7+aCU8ythHkPgKWOa+UdRCAQPdRyri3nPEeFk4cuLGCY0yLXfYBHJnJWOENsMq0wAqKgtCaWMHROA6vASxcIfWYl7iDjp06qa5LAP1ulxwul4uvv/767w88Tz4LbOXKlcPPz4+UlBSv/SkpKcTExBR4TkxMzDkdf7prZGdnc/jwYa9Wtr+7TlBQEEFBQfn2BwQEFIlfwqJSDil8quuSRfV9fgICzKLddevm/27DBrOswN69sHWrmchk924H06aZVpL33vOjZk1o3NjMRlm3Llx+OdSuDWFhhVlm1fXFFwDlW5jN45ShFblZcHAF5GRAVHM4stFMdpKyANL/hMCyZu25zGTY95N5H1IVXGk4Mk2vHEdmMo6kkwuMOw6vgaTPCyyNP9CFcAK/c+DwD4PwumZiFb9SULqOeT2200ykElEfQqubpRNCqkJYTdOtUwMzix39bsuF8llgCwwMpEWLFixYsICePXsC4Ha7WbBgAcOGDSvwnLZt27JgwQIefvhhz76EhATatm171vdt0aIFAQEBLFiwgJtvvhmAzZs3k5SUdE7XERGR4qlhQ7PlycqCuXNNeFu50rS4/fmn2f4qKsqsEzdgAHTtaj5LMeYXdHK9OYAKV5rXqGYFH583xg7g+In/gZy62EyOUqqSmbUyNREydpj16dL/NDNhRjQAv1CsQ6sIcqeBCxP+ju8+t/I6A03LoH+omW0zPNbc98gGE+5K14XStSAg0iyFEFod/EIg+6ApW5nmYOWaspWKOblcgmWZSWCcfudWHhG5JHzaJXLEiBEMHDiQli1b0rp1a8aPH09GRoZn1sgBAwZQuXJlxo4dC8BDDz1E+/btef311+nevTtTpkxh+fLlTJo0yXPNgwcPkpSUxJ49ewATxsC0rMXExBAREcGQIUMYMWIEUVFRhIeH88ADD9C2bVvNECkiUgIFBZklAvK8/TYsWWJmqNy2Ddatg2XL4PBhM1vlvHlmAzMzZUYGVKsGgwZBv35msW9/DTiwp1Nbt0qdGFNftZfZ8lS50fucU0JezvFD/PL9h1zRvhMB1nE4st6EJ1eaWYjcnQ3BMWbJgyO/mbAXGGmCX06G+T5rv9kADi4/eZ/0PyDlB86aX7AJfznHIPeYCXJRLU2Is9wm7IFZJy+kCmQfMiETh7l/6XomLLqzISDMBMOcdAiMgrDLTKh0BpjZOo/tNK2VpbTercj58Ol/Uvr27cv+/fsZPXo0ycnJNG3alDlz5ngmFklKSsJ5yijwdu3aMXnyZJ566imeeOIJ6tSpw8yZMz1rsAHMmjXLE/gAbr31VgCeeeYZxowZA8Abb7yB0+nk5ptv9lo4W0REJCICOnUy26nS0kyAmzHDbOvXw44d5rvUVNM69+CJ9aTj4qBnT+jdGypXNvvUGldCnRry/MM44lfbtIwFBEC51md3Dcttgo/ryIktHQLCzVIIrqNQvq1ZHuHoFhPcXOngzjRBKee4CX0ZO04ELsDhhNxMs53q1AB4YIl5PXUGznN6bn8oXRuO/m7K73BC+asgpBoElTOtee4cE07dOYDbdAsNrgj+pSCoArizzD5XmnkfVgtwmOUbMveZLqTh9c2yDUe3mGDpH3p+5RUpwhyWZVm+LkRxlJaWRkREBEeOHPH5pCOzZ8+mW7du6h9tc6rrkkX1XfTt3w+bN0NoqBkT98EHsGbN6Y+Pi4PwcBPcbrjBtMRVqABly7rYuPF7bryxq+ra5nz6e+3OMd0onQEQWAaO/gG5x8E/xLSOWTmw70fT7dLhgGN7zPf7/mdazgIizHlWrnk9tNq0zDkDzfc5GSYsHd9rWuNOFRBxMixebA5/E0izUk23z9CakLHNhMTAKLPQusNpgqvDz3R3LV3HPHPWfrP2X2YKVO5uxhPmHj+xDmCGCcJ+pcARYP6cco+bkOsfZloX/QJPliPnmLmnf6gnpOvv8ZLD5XIxffp0brvttkLJBuq0ISIich7KlzcbQLNm8MADphUuPR0WLjTLCCxZYvaB6VqZ55tvTr1SACEhXbnlFj9uusm08DkcUKcOVKp0qZ5GbM/pb8at5Qmvk/+Yy27Pv6/h4+d2H8syrWHH98DhDVCmCYRWg7QtsP/nE106UwHLhC2H/8lumEfWn2hBPHoiXPrD0a0mXPkFnTgPE6KCo82ELK7DZr/DCVkHzJYn+yCkb/Uu38FlBZc7ZcG5PafDH0pVNF1LLbfptoplurRW6gpY+O/8mq6uHPznVgHcJgRH1DfnBZUzrYnB5U2QdOeYsjn84P/bu/fgKKv7DeDPu8nuZpOQZMmdhHAzcicFomkKlBEoEBkqllbrRCfa/oYBg8XWOl6mAo4XmLba1tbGSlV0cKRiB0UK2AgaK3INt0gigtwCIQm5b257yZ7fH192N0siBAzZze7zmTmT3fc9efe8OcTm6TnvOal3ygbyWoh8juqQn1FYkgTiyOEyvVQpT1DuPJLbeg6o3QsMnOyZ2kr9GgMbERFRL4mKkpKbKwUA7HbAYgG2bJFZcCUlwN69QGsrUF0NVFQotLTo8dZbwFtveV9v2jTg+eeBqVP7/l6IroumSYiJHC7FJSq9+5B4NR1WCSuuaZyAXB+QwNJaLgHFnAFUfiwjaTHjIFMn6yQgKiUhr6NVVgVtOSPP3hliZKEWnQE4u8EzQqYLBaADanbKV9Uh3+sKjvYm+f7W8q7tba8ETr4hPwoABgBoKvOcr9179Xs+9dbV6xjjJRjbm6Rd4Sky0tl+UUZLXUIHyM8uJExWG40ZB1jrZFTR9fOMy5YQ2V4pYdkYL0HSGA+EJUggbj0PxIyVEBk+uNNiNnEy4tp8Ut6HJUiwPLFGRi3D0+T7o0dLOwAZ1XTa5XouzSelH8JTr37vQYiBjYiI6AbS62Ua5L2XBi/uvtv7vNXqwAsv7EZ5+Q+wc2cIHA7A4ZBVK//3PwltmZkympeUBGRkeIrZ3Pf3Q9SnQjptqeQKai6aJqN3EWny/vIFX7qT9tPuj496uOdtUkpCYnulBErlkGfpQiOAi/8DavYAHW1wJMzEZ3uP4Ye33IzQEE2mUTaVXRoJrJFRSGutBEVAtnmwNQAXdwLKLkHR6ZCQ1NEmo2thCZe+76KnPR2t8gyf5wcj12r6GnBY5JC9UUJa5+cUXRquMJf7emgh0vajz0pblEOmp7pGJOuKpd7AW4CW0zKVtekrCdVjHpcQGp4q9+i8FD7bq2Qqqj5K7l8XCsRPlbrR44CkGb17D36GgY2IiMiHdDpg7Ng6PPqoE3q9Z1n18+eBp58G1q4F9nfzNxYgq1TOnw/cdx8wbhwQHt43bSYKapoGRAyWcrlBOZemRALKbodF1waVOEP+nxvg0ujfd2Srl8AInbSh/aK8D42QqaKGaAk21lqpq5wS6ppPAvVHLj3rlyYBydEiU1UdzbKSp8EsYdJaLddtr5ZFYMLTJFRBk2mmTruMrllrJRSGJQCmFKDpmHyWKcWzbUVImIy+2epcP0D54pqe6gqfHW1AyYqe/xy+eU2+pi9hYCMiIqK+l5ICvPoq8MwzMp1SKVmV8vBhKadPy/u//U2KpgEjRgAjRwLz5smUTB+uiUVEN4rBLMVFHyX7713OGCvFxfw9YPBPutbrvC1FT3Xek7DDdmnaqgbYGoGGEplmWbVDpp3GTLg0slgjATLme5emp+4HosdK6IseDXzzBnDxMyB6vIxeGuMlVKoOef7S0SojasaBMq2z4bDUGTj52tvfzzCwERER+bHERKDTbjVujY3AF18Aa9bI1MmaGplGeeIE8J//yCIoEyYAQ4fKPnGXl4QEGd0jIrpmnRc56bxapiHaswF98o88x83f63oNc4b3+4xneq15gYaBjYiIqB+KjgZycqQAQFUVcPQoUFwMvP468NVXwMGDUrpjNEqgmz1brhEb61mdkkGOiMh/MLAREREFgMREKTNmAI8+KlMmjxwBysuBs2e9S0UFYLUC+/ZJee45z3USEmShk8mTPSU29ls/loiIbjAGNiIiogA0dKiU7tjt8vzb7t3Au+/KVgONjUB7u2w18O9/S3FJSwOysoDhw2Wj8AEDZI+48ePlmTmOyBER3TgMbEREREFGrwduukmKa7sBALDZgF27ZJ+44mIpJ054Rua6k5QE/PCH8jU5GRgzRjYSdzplJM9sllDX+ZEXIiLqOQY2IiIiAgAYDMD06VJcGhuBAweAPXtk9K2lRY65VqysrJRRuitJTwduvVWC25Qp8j4lhSNzREQ9wcBGRERE3yo6GrjtNimXs9mAoiJZ7KSqSp6XKykBSkvlfGqqHD9+XEpnRqNMsRwxAhg2TOp2LoMGAWFhXT+TiCjYMLARERHRdTEYgB/9SEpnNpvnfHOz7CN35oxMtTx4UBZEsVqBsjIp3yY+Xp7Di4oCbr5ZVrVMSfGUhAROtSSiwMfARkRERL3K0GlbpshI4K67vM87HPJM3DffeJ6RO3fOU86fB9ragIsXpQDA9u1dPycyUkbioqMl1EVHy6jdqFGyYqZeD0yaJMGPiKi/YmAjIiKiPhUaKsFq+PCuo3MAoBRQXy+jcmfPAk1NwKFDEu7On5dSVSWjd19/ffXPGzFCwlxIiCyOkpUFmEzA4MEyUhcRIcfj4qQOEZE/YWAjIiIiv6JpwMCBUiZOlGP33eddx2YDTp6UEbjGRgl1tbUS6srKJPBZLMCxYzKS19mHH3b/uTqdrGoZGyufnZgo+9AZDFLS0oCxY2Vbg/BwKQYDp2US0Y3FwEZERET9jsEgUx9Hjbpyvfp62Z7AZpOtBo4fl1UvOzokyNXWStirqZHztbVSXD744MrXj4mRUTqbTZ6pGzTIUxITZdpmeLiM4kVHA1arDjabTNckIuoJBjYiIiIKWGYzMGvW1es5HBLaamuBujopp05JuAsJkU3Fjx+XKZgtLRLuAKChQQrQdSXMrvQA5gOQrQ3i4iTIpaZKO2NipFit0p6xY2WhFbNZppHq9VI4okcUXBjYiIiIKOiFhspzbElJV6+rFGC3S3A7d06ep9PrZXpmRYWnuPata22VrxUVChaLpK3utjroCb1eRhXj4jzTMiMiZCQvNVVW1bTbJWBOmyavnU4ZBYyNvfbPIyLfY2AjIiIiugaa5nmuzWwGxo/v2ffZbA5s2PBfTJ8+G6WlerS0yHTMigp5Dq++XkbrXKNorj3t7HbPNex2OX49kpPlukpJGTZMRvA0TcJcfLyMJoaEeEb7Li9hYRzhI+prDGxEREREfUDTgMhIBxITZTSsJ5xOCWl2u0yTrK8HvvpKAl5rq2f0rqlJNi4/dUoCl8Mh+95FRcn76mrgwgXva1dWArt2Xds9GAyebRQiI2UBlm/7GhV15WI0MvwR9QQDGxEREZGf0ukk2BiN8j4mRkbGekIpTyCqqZFtEnQ6OeZ0ylYJZ89Kvdpaz8IrdrsEQtfzeQ0N8t7plMVVOu+P913o9VcPdd2ViAgJoCYTMHq0LCATEyP3FREh01ttNqmTnMytGqj/Y2AjIiIiCkCdR6/i4qR0NmlSz6/ldMq+d64AZ7HI++bm7l83NcnrpqauxWKRa9rtXVfl/K6MRtmSobJSgmh8vExZNZkkyLW3A+PGyaIvZrME2AsXZORw5EgZGaytleskJ8tefSZT77WP6HowsBERERHRFel0nhGutLTvdi1X+OsuzPWkWCzyzF1DA3D6tIzUtbfLta1Wz9RPTZORwB07vD//o4+urb1GowS6iAjZ7L2qSkLhhAkS8BwOmQba0iLvMzNl8ZqWFg379ycgOlrDsGFy306nZ4sHPg9IPcXARkRERER9pnP46y1Wq4yonT8vQW7wYJkmuWuXLOrS1iYjejqdbNVQUSH17HZZQbO9HTh8WKZSJiZK/fPn5RlBq1WKxSIjd4Ds4bdv39VaFQogG88+2/1ZvV6CW3S0BD7Xap8REZ7XCQkyMhoWJsExLMxTwsNl9C8iQu5Br5dA29EhgZFhMHAwsBERERFRv+Z6xm/ECO/j06df/zWV8kz/tNnk9YkTEoYuXJDQ1tIiz8hZLBKcqqqAI0dc0yoVbLZGOBzRqKrSEBIiIaq11bM1RE2NlN7mCnJ6vQQ/15YViYnShrY2qRcbK4EwNlbCrFIySvhthRu++wYDGxERERHRZTRNnnMzmz3HMjN7/v12uwNbthTh9ttvh75T0nFNCW1s9JSWFinNzZ7XFosEw8ZGGQHsXNrapLS2eq7lotN5zrtcz55/3TEauw9ySskoZEQEMGaMhFHXM42RkUBWlowWHj8u55KSZMGY9HSZUnrihHxNTZXRwrCw3mlvoGBgIyIiIiLqI52nhA4e3DvXtFgkNEVGypTI8nIJdq4VPysrpVRVSRANC/OsDupaIdS1qqjF0rVYrfI5rumhVxoV/M9/uh577bVru5/4eAlvqakyAuia3hkZKVNIY2I800lvvlmeJwxkDGxERERERP3YgAGe1zqdLI7Sm+z27oOcq7i2n6ipAb7+WqZkuvbkq6wEDh6UBWBGjJBj5eWyKXxFhVx/8GAJkefOyciga+uIgwev3rb/+z9gzZrevV9/w8BGRERERETfSq+XlTEHDuzd61qtcm2dTt4rBdTVSXA7d06CXX29Z+9A1/RP196AjY0ytTLQMbAREREREVGfcy0W46JpsgBKbCyQkeGbNvkjna8bQERERERERN1jYCMiIiIiIvJTDGxERERERER+ioGNiIiIiIjITzGwERERERER+SkGNiIiIiIiIj/FwEZEREREROSnGNiIiIiIiIj8FAMbERERERGRn2JgIyIiIiIi8lMMbERERERERH6KgY2IiIiIiMhPMbARERERERH5KQY2IiIiIiIiP8XARkRERERE5KcY2IiIiIiIiPwUAxsREREREZGfYmAjIiIiIiLyU6G+bkB/pZQCADQ1Nfm0HXa7Ha2trWhqaoJer/dpW+jGYl8HF/Z38GBfBw/2dXBhfwcPV18DnozQmxjYrpPFYgEADB482MctISIiIiIif2CxWBAdHd2r19TUjYiBQcDpdKKiogIDBgyApmk+a0dTUxMGDx6M8vJyREVF+awddOOxr4ML+zt4sK+DB/s6uLC/g4err0tLSzFy5EjodL371BlH2K6TTqdDamqqr5vhFhUVxf8YBAn2dXBhfwcP9nXwYF8HF/Z38EhJSen1sAZw0REiIiIiIiK/xcBGRERERETkpxjY+jmj0YgVK1bAaDT6uil0g7Gvgwv7O3iwr4MH+zq4sL+Dx43uay46QkRERERE5Kc4wkZEREREROSnGNiIiIiIiIj8FAMbERERERGRn2JgIyIiIiIi8lMMbP3Yyy+/jKFDhyIsLAxZWVnYu3evr5tE1+Gzzz7D/PnzMWjQIGiahvfff9/rvFIKy5cvR3JyMkwmE2bNmoXjx4971amrq0Nubi6ioqIQExODX/7yl2hubu7Du6CrWbVqFW655RYMGDAACQkJWLBgAY4dO+ZVp729Hfn5+YiNjUVkZCQWLlyIqqoqrzpnz57FvHnzEB4ejoSEBDz66KNwOBx9eSvUAwUFBZgwYYJ7w9zs7Gxs3brVfZ59HbhWr14NTdPw8MMPu4+xvwPHypUroWmaVxk1apT7PPs6sJw/fx733nsvYmNjYTKZMH78eOzfv999vq/+RmNg66f+9a9/4Te/+Q1WrFiBAwcOICMjA3PmzEF1dbWvm0bXqKWlBRkZGXj55Ze7Pf/73/8eL730El555RXs2bMHERERmDNnDtrb2911cnNzcfToURQWFmLz5s347LPPsGjRor66BeqBoqIi5OfnY/fu3SgsLITdbsfs2bPR0tLirvPrX/8aH374ITZs2ICioiJUVFTgJz/5ift8R0cH5s2bB5vNhi+++AJvvvkm1q5di+XLl/vilugKUlNTsXr1ahQXF2P//v2YMWMG7rjjDhw9ehQA+zpQ7du3D//4xz8wYcIEr+Ps78AyduxYXLhwwV0+//xz9zn2deCor6/HlClToNfrsXXrVpSWluKFF16A2Wx21+mzv9EU9Uu33nqrys/Pd7/v6OhQgwYNUqtWrfJhq+i7AqA2btzofu90OlVSUpL6wx/+4D7W0NCgjEajeuedd5RSSpWWlioAat++fe46W7duVZqmqfPnz/dZ2+naVFdXKwCqqKhIKSX9qtfr1YYNG9x1ysrKFAC1a9cupZRSW7ZsUTqdTlVWVrrrFBQUqKioKGW1Wvv2Buiamc1m9c9//pN9HaAsFotKT09XhYWFavr06WrZsmVKKf5uB5oVK1aojIyMbs+xrwPLY489pqZOnfqt5/vybzSOsPVDNpsNxcXFmDVrlvuYTqfDrFmzsGvXLh+2jHrbqVOnUFlZ6dXX0dHRyMrKcvf1rl27EBMTg8zMTHedWbNmQafTYc+ePX3eZuqZxsZGAMDAgQMBAMXFxbDb7V59PWrUKKSlpXn19fjx45GYmOiuM2fOHDQ1NblHbsj/dHR0YP369WhpaUF2djb7OkDl5+dj3rx5Xv0K8Hc7EB0/fhyDBg3C8OHDkZubi7NnzwJgXweaTZs2ITMzEz/72c+QkJCAiRMnYs2aNe7zffk3GgNbP1RTU4OOjg6vX3YASExMRGVlpY9aRTeCqz+v1NeVlZVISEjwOh8aGoqBAwfy34OfcjqdePjhhzFlyhSMGzcOgPSjwWBATEyMV93L+7q7fwuuc+RfSkpKEBkZCaPRiMWLF2Pjxo0YM2YM+zoArV+/HgcOHMCqVau6nGN/B5asrCysXbsW27ZtQ0FBAU6dOoVp06bBYrGwrwPMyZMnUVBQgPT0dHz00UdYsmQJfvWrX+HNN98E0Ld/o4V+lxshIqJrl5+fjy+//NLruQcKPCNHjsShQ4fQ2NiI9957D3l5eSgqKvJ1s6iXlZeXY9myZSgsLERYWJivm0M3WE5Ojvv1hAkTkJWVhSFDhuDdd9+FyWTyYcuotzmdTmRmZuL5558HAEycOBFffvklXnnlFeTl5fVpWzjC1g/FxcUhJCSky6pDVVVVSEpK8lGr6EZw9eeV+jopKanLYjMOhwN1dXX89+CHli5dis2bN+OTTz5Bamqq+3hSUhJsNhsaGhq86l/e1939W3CdI/9iMBhw0003YfLkyVi1ahUyMjLwl7/8hX0dYIqLi1FdXY1JkyYhNDQUoaGhKCoqwksvvYTQ0FAkJiayvwNYTEwMbr75Zpw4cYK/2wEmOTkZY8aM8To2evRo9xTYvvwbjYGtHzIYDJg8eTK2b9/uPuZ0OrF9+3ZkZ2f7sGXU24YNG4akpCSvvm5qasKePXvcfZ2dnY2GhgYUFxe76+zYsQNOpxNZWVl93mbqnlIKS5cuxcaNG7Fjxw4MGzbM6/zkyZOh1+u9+vrYsWM4e/asV1+XlJR4/ce/sLAQUVFRXf5HhfyP0+mE1WplXweYmTNnoqSkBIcOHXKXzMxM5Obmul+zvwNXc3MzvvnmGyQnJ/N3O8BMmTKly/Y7X3/9NYYMGQKgj/9Gu/Y1U8gfrF+/XhmNRrV27VpVWlqqFi1apGJiYrxWHaL+wWKxqIMHD6qDBw8qAOrFF19UBw8eVGfOnFFKKbV69WoVExOjPvjgA3XkyBF1xx13qGHDhqm2tjb3NebOnasmTpyo9uzZoz7//HOVnp6u7rnnHl/dEnVjyZIlKjo6Wn366afqwoUL7tLa2uqus3jxYpWWlqZ27Nih9u/fr7Kzs1V2drb7vMPhUOPGjVOzZ89Whw4dUtu2bVPx8fHqiSee8MUt0RU8/vjjqqioSJ06dUodOXJEPf7440rTNPXf//5XKcW+DnSdV4lUiv0dSB555BH16aefqlOnTqmdO3eqWbNmqbi4OFVdXa2UYl8Hkr1796rQ0FD13HPPqePHj6u3335bhYeHq3Xr1rnr9NXfaAxs/dhf//pXlZaWpgwGg7r11lvV7t27fd0kug6ffPKJAtCl5OXlKaVk2dinnnpKJSYmKqPRqGbOnKmOHTvmdY3a2lp1zz33qMjISBUVFaUeeOABZbFYfHA39G2662MA6o033nDXaWtrUw8++KAym80qPDxc3XnnnerChQte1zl9+rTKyclRJpNJxcXFqUceeUTZ7fY+vhu6ml/84hdqyJAhymAwqPj4eDVz5kx3WFOKfR3oLg9s7O/Acffdd6vk5GRlMBhUSkqKuvvuu9WJEyfc59nXgeXDDz9U48aNU0ajUY0aNUq9+uqrXuf76m80TSmlrnGEkIiIiIiIiPoAn2EjIiIiIiLyUwxsREREREREfoqBjYiIiIiIyE8xsBEREREREfkpBjYiIiIiIiI/xcBGRERERETkpxjYiIiIiIiI/BQDGxERERERkZ9iYCMiIuoDmqbh/fff93UziIion2FgIyKigHf//fdD07QuZe7cub5uGhER0RWF+roBREREfWHu3Ll44403vI4ZjUYftYaIiKhnOMJGRERBwWg0IikpyauYzWYAMl2xoKAAOTk5MJlMGD58ON577z2v7y8pKcGMGTNgMpkQGxuLRYsWobm52avO66+/jrFjx8JoNCI5ORlLly71Ol9TU4M777wT4eHhSE9Px6ZNm9zn6uvrkZubi/j4eJhMJqSnp3cJmEREFHwY2IiIiAA89dRTWLhwIQ4fPozc3Fz8/Oc/R1lZGQCgpaUFc+bMgdlsxr59+7BhwwZ8/PHHXoGsoKAA+fn5WLRoEUpKSrBp0ybcdNNNXp/x9NNP46677sKRI0dw++23Izc3F3V1de7PLy0txdatW1FWVoaCggLExcX13Q+AiIj8kqaUUr5uBBER0Y10//33Y926dQgLC/M6/uSTT+LJJ5+EpmlYvHgxCgoK3Oe+//3vY9KkSfj73/+ONWvW4LHHHkN5eTkiIiIAAFu2bMH8+fNRUVGBxMREpKSk4IEHHsCzzz7bbRs0TcPvfvc7PPPMMwAkBEZGRmLr1q2YO3cufvzjHyMuLg6vv/76DfopEBFRf8Rn2IiIKCjcdtttXoEMAAYOHOh+nZ2d7XUuOzsbhw4dAgCUlZUhIyPDHdYAYMqUKXA6nTh27Bg0TUNFRQVmzpx5xTZMmDDB/ToiIgJRUVGorq4GACxZsgQLFy7EgQMHMHv2bCxYsAA/+MEPruteiYgocDCwERFRUIiIiOgyRbG3mEymHtXT6/Ve7zVNg9PpBADk5OTgzJkz2LJlCwoLCzFz5kzk5+fjj3/8Y6+3l4iI+g8+w0ZERARg9+7dXd6PHj0aADB69GgcPnwYLS0t7vM7d+6ETqfDyJEjMWDAAAwdOhTbt2//Tm2Ij49HXl4e1q1bhz//+c949dVXv9P1iIio/+MIGxERBQWr1YrKykqvY6Ghoe6FPTZs2IDMzExMnToVb7/9Nvbu3YvXXnsNAJCbm4sVK1YgLy8PK1euxMWLF/HQQw/hvvvuQ2JiIgBg5cqVWLx4MRISEpCTkwOLxYKdO3fioYce6lH7li9fjsmTJ2Ps2LGwWq3YvHmzOzASEVHwYmAjIqKgsG3bNiQnJ3sdGzlyJL766isAsoLj+vXr8eCDDyI5ORnvvPMOxowZAwAIDw/HRx99hGXLluGWW25BeHg4Fi5ciBdffNF9rby8PLS3t+NPf/oTfvvb3yIuLg4//elPe9w+g8GAJ554AqdPn4bJZMK0adOwfv36XrhzIiLqz7hKJBERBT1N07Bx40YsWLDA100hIiLywmfYiIiIiIiI/BQDGxERERERkZ/iM2xERBT0+HQAERH5K46wERERERER+SkGNiIiIiIiIj/FwEZEREREROSnGNiIiIiIiIj8FAMbERERERGRn2JgIyIiIiIi8lMMbERERERERH6KgY2IiIiIiMhP/T9dhK0mnoNSyAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_diffusion_model(test_folder, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device):\n",
    "    reverse_vocab_mapping = {idx: token for token, idx in vocab_mapping.items()}\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file_name in os.listdir(test_folder):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(test_folder, file_name)\n",
    "\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "                tokens = tokenize_formula(formula_human_readable)\n",
    "\n",
    "                # Convert tokens to indices\n",
    "                token_indices = [vocab_mapping.get(token, 0) for token in tokens]\n",
    "\n",
    "                # Pad or truncate to seq_len\n",
    "                token_indices = token_indices[:seq_len] + [0] * max(0, seq_len - len(token_indices))\n",
    "                token_tensor = torch.tensor(token_indices, device=device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                points = data.get(\"points\")\n",
    "                if points:\n",
    "                    points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                    points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                    # Generate embeddings using tNet model\n",
    "                    embedding = tnet_model(points_tensor)\n",
    "\n",
    "                    # Choose random timestep\n",
    "                    t = torch.randint(0, 1000, (1,), device=device)\n",
    "\n",
    "                    # Add noise to the tokens\n",
    "                    noisy_tokens, _ = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "                    # Use reverse model to reconstruct the clean tokens\n",
    "                    reconstructed_noise = reverse_model(noisy_tokens, embedding, t)\n",
    "                    # print(f\"Reconstructed Noise Shape: {reconstructed_noise.shape}\")\n",
    "                    # Convert reconstructed noise to token indices\n",
    "                    # reconstructed_tokens = torch.argmax(reconstructed_noise, dim=-1).squeeze(0)\n",
    "\n",
    "                    # Ensure reconstructed_tokens is a list\n",
    "                    reconstructed_tokens = torch.argmax(reconstructed_noise, dim=-1)\n",
    "                    # print(reconstructed_tokens)\n",
    "                    if reconstructed_tokens.dim() == 2:  # Case: (batch_size, seq_len)\n",
    "                        reconstructed_tokens = reconstructed_tokens.squeeze(0)  # Remove batch dimension\n",
    "                    elif reconstructed_tokens.dim() == 1:  # Case: (seq_len,)\n",
    "                        pass  # Already correct\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected shape for reconstructed_tokens: {reconstructed_tokens.shape}\")\n",
    "\n",
    "                    # print(reconstructed_tokens)\n",
    "                    # Map token indices back to tokens\n",
    "                    reconstructed_formula = \"\".join(\n",
    "                        reverse_vocab_mapping[idx] if idx in reverse_vocab_mapping else \"<UNK>\" for idx in reconstructed_tokens.tolist()\n",
    "                    )\n",
    "                    \n",
    "                    reconstructed_formula = reconstructed_formula.replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\")\n",
    "                    \n",
    "                    actual_formula = \"\".join(tokens)\n",
    "\n",
    "                    results.append((actual_formula, reconstructed_formula))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Formula: (((var_2*C_0)*cos(var_1))*gaussian(reverse(var_0)))\n",
      "Reconstructed Formula: (((var_0+C_0)+sqrt(var_1)))gaussian(((var_2)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "folder_path_test = \"data_symbolic_regression/test\"\n",
    "\n",
    "# Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "tokenized_formulas_test = []\n",
    "points_list_test = []\n",
    "\n",
    "for file_name in os.listdir(folder_path_test):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            formula_human_readable_test = data.get(\"formula_human_readable\", \"\")\n",
    "            if formula_human_readable_test:\n",
    "                tokens_test = tokenize_formula(formula_human_readable_test)\n",
    "                tokenized_formulas_test.append(tokens_test)\n",
    "                \n",
    "            points_test = data.get(\"points\")\n",
    "            if points_test:\n",
    "                points_array_test = np.array([points_test[\"var_0\"], points_test[\"var_1\"], points_test[\"var_2\"], points_test[\"target\"]])\n",
    "                points_tensor_test = torch.tensor(points_array_test, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                points_list_test.append(points_tensor_test)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_diffusion_model(folder_path_test, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device)\n",
    "\n",
    "# Display example results\n",
    "example_idx = 11\n",
    "\n",
    "if results:\n",
    "    actual, reconstructed = results[example_idx]\n",
    "    print(f\"Actual Formula: {actual}\")\n",
    "    print(f\"Reconstructed Formula: {reconstructed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
