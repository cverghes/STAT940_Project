{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.0\n",
      "Epoch 2/50, Train Loss: 0.0\n",
      "Epoch 3/50, Train Loss: 0.0\n",
      "Epoch 4/50, Train Loss: 0.0\n",
      "Early stopping occurred at epoch 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 271\u001b[0m\n\u001b[0;32m    268\u001b[0m preprocessed_data, vocab_embeddings, model, performance_metrics_DICT \u001b[38;5;241m=\u001b[39m preprocess_and_tokenize_dataset(file_path)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Save the preprocessed data, vocab embeddings, and trained Word2Vec model\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m \u001b[43msave_preprocessed_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData/preprocessed_data_with_embeddings.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m save_JSON(vocab_embeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/vocab_embeddings.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing complete with embeddings. Data saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 243\u001b[0m, in \u001b[0;36msave_preprocessed_data\u001b[1;34m(data, output_path)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m--> 243\u001b[0m         \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_to_serializable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants for tokenization\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "NOISE_LEVEL = 0.1\n",
    "EMBEDDING_DIM = 100  # Set the dimension of the embeddings\n",
    "EPOCHS = 50  # Number of epochs for Word2Vec training\n",
    "PATIENCE = 3  # Patience for early stopping (number of epochs to wait for improvement)\n",
    "MIN_DELTA = 0.0001  # Minimum change in loss to count as an improvement\n",
    "\n",
    "def tokenize_skeleton(skeleton_str):\n",
    "    \"\"\"Tokenize the skeleton string into a sequence of symbols.\"\"\"\n",
    "    skeleton_str = skeleton_str.replace(\"**\", \"^\")  # Replace '**' with '^'\n",
    "    pattern = r'[a-zA-Z_][a-zA-Z0-9_]*|[+\\-*/^(),.]|C|sin|cos|log|exp|sqrt'\n",
    "    tokens = re.findall(pattern, skeleton_str)\n",
    "    return tokens\n",
    "\n",
    "def train_word2vec_embeddings(dataset, embedding_dim=EMBEDDING_DIM, epochs=EPOCHS, patience_num_epochs=PATIENCE):\n",
    "    \"\"\"Train Word2Vec embeddings on the dataset over multiple epochs with early stopping.\"\"\"\n",
    "    sentences = []\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        sentences.append(tokens)\n",
    "    \n",
    "    # Initialize the Word2Vec model\n",
    "    model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, epochs=1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    num_epochs_without_improvement = 0\n",
    "    early_stopping = False\n",
    "\n",
    "    performance_metrics_DICT = {\n",
    "        \"epoch_list\": [],\n",
    "        \"train_loss_list\": [],\n",
    "    }\n",
    "\n",
    "    # Train Word2Vec model with early stopping\n",
    "    for epoch in range(epochs-1):\n",
    "        # Train for one epoch\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "        # Get the current loss (Word2Vec model loss is stored in the 'trainables' attribute)\n",
    "        current_loss = model.get_latest_training_loss()\n",
    "\n",
    "        # Record loss\n",
    "        performance_metrics_DICT['epoch_list'].append(epoch+1)\n",
    "        performance_metrics_DICT['train_loss_list'].append(current_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {current_loss}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            model.save(\"Data/embeddings_model.model\")\n",
    "            num_epochs_without_improvement = 0\n",
    "            save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "        else:\n",
    "            num_epochs_without_improvement += 1\n",
    "\n",
    "        if num_epochs_without_improvement >= patience_num_epochs:\n",
    "            print(f'Early stopping occurred at epoch {epoch + 1}')\n",
    "            early_stopping = True\n",
    "            break\n",
    "        \n",
    "    if early_stopping == False:\n",
    "        model.save(\"Data/embeddings_model.model\")\n",
    "        save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "    return model, performance_metrics_DICT\n",
    "\n",
    "def build_vocab_and_embeddings(dataset, model=None):\n",
    "    \"\"\"Build vocabulary and convert tokens to embeddings using Word2Vec model.\"\"\"\n",
    "    token_set = set()\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        token_set.update(tokens)\n",
    "    \n",
    "    # Convert tokens to embeddings using the Word2Vec model\n",
    "    vocab_embeddings = {}\n",
    "    if model:\n",
    "        for token in token_set:\n",
    "            if token in model.wv:\n",
    "                embedding = model.wv[token]\n",
    "                # Normalize the embedding to have a unit norm (length = 1)\n",
    "                embedding = normalize([embedding])[0]\n",
    "                vocab_embeddings[token] = embedding\n",
    "            else:\n",
    "                # If the token isn't in the model, use a random vector\n",
    "                vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    else:\n",
    "        # If no pre-trained model is provided, use random embeddings for all tokens\n",
    "        for token in token_set:\n",
    "            vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    \n",
    "    # Add PAD_TOKEN to the vocabulary and set its embedding to zero\n",
    "    vocab_embeddings[PAD_TOKEN] = np.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    return vocab_embeddings\n",
    "\n",
    "def tokenize_dataset_with_embeddings(dataset, vocab_embeddings):\n",
    "    \"\"\"Tokenize the dataset and convert tokens to embeddings.\"\"\"\n",
    "    tokenized_data = []\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokenized_skeleton = [vocab_embeddings[token] for token in tokenize_skeleton(skeleton_str)]\n",
    "        \n",
    "        for data_point in entry[\"data\"]:\n",
    "            inputs = data_point[\"inputs\"]\n",
    "            output = data_point[\"output\"]\n",
    "            tokenized_data.append({\n",
    "                \"tokens\": tokenized_skeleton,\n",
    "                \"inputs\": inputs,\n",
    "                \"output\": output\n",
    "            })\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "def normalize_data(data, range_vals=(-1, 1)):\n",
    "    \"\"\"Normalize the inputs and outputs to the specified range.\"\"\"\n",
    "    input_keys = []\n",
    "    for dp in data:\n",
    "        input_keys.extend(dp[\"inputs\"].keys())\n",
    "    input_keys = sorted(set(input_keys), key=lambda x: (int(x[1:]), x))\n",
    "    \n",
    "    all_inputs = [dp[\"inputs\"] for dp in data]\n",
    "    all_outputs = [dp[\"output\"] for dp in data]\n",
    "    \n",
    "    input_mins = {key: min([inputs.get(key, float('inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    input_maxs = {key: max([inputs.get(key, float('-inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    output_min = min(all_outputs)\n",
    "    output_max = max(all_outputs)\n",
    "    \n",
    "    for dp in data:\n",
    "        normalized_inputs = {}\n",
    "        for key in input_keys:\n",
    "            if key in dp[\"inputs\"]:\n",
    "                normalized_inputs[key] = 2 * (dp[\"inputs\"][key] - input_mins[key]) / (input_maxs[key] - input_mins[key]) - 1\n",
    "            else:\n",
    "                normalized_inputs[key] = np.nan  \n",
    "        \n",
    "        normalized_output = 2 * (dp[\"output\"] - output_min) / (output_max - output_min) - 1\n",
    "        \n",
    "        dp[\"inputs\"] = normalized_inputs\n",
    "        dp[\"output\"] = normalized_output\n",
    "    \n",
    "    return data\n",
    "\n",
    "def add_masking_to_data(data):\n",
    "    \"\"\"Adds a mask to the data where 1 indicates valid data and 0 indicates NaN.\"\"\"\n",
    "    for dp in data:\n",
    "        dp[\"mask\"] = {key: 1 if not np.isnan(value) else 0 for key, value in dp[\"inputs\"].items()}\n",
    "    return data\n",
    "\n",
    "def add_noise_to_data(data, noise_type=\"gaussian\", noise_level=NOISE_LEVEL):\n",
    "    \"\"\"Add noise to the dataset while ensuring NaN values are ignored.\"\"\"\n",
    "    for dp in data:\n",
    "        for key, value in dp[\"inputs\"].items():\n",
    "            if not np.isnan(value):\n",
    "                if noise_type == \"gaussian\":\n",
    "                    dp[\"inputs\"][key] += np.random.normal(0, noise_level)\n",
    "                elif noise_type == \"uniform\":\n",
    "                    dp[\"inputs\"][key] += np.random.uniform(-noise_level, noise_level)\n",
    "        if not np.isnan(dp[\"output\"]):\n",
    "            if noise_type == \"gaussian\":\n",
    "                dp[\"output\"] += np.random.normal(0, noise_level)\n",
    "            elif noise_type == \"uniform\":\n",
    "                dp[\"output\"] += np.random.uniform(-noise_level, noise_level)\n",
    "    return data\n",
    "\n",
    "def pad_sequences(tokenized_data, max_length, pad_embedding, pad_token=PAD_TOKEN):\n",
    "    \"\"\"Pad tokenized sequences to a fixed length.\"\"\" \n",
    "    for dp in tokenized_data:\n",
    "        token_length = len(dp[\"tokens\"])\n",
    "        if token_length < max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"] + [pad_embedding]*(max_length - token_length)\n",
    "        elif token_length > max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"][:max_length]\n",
    "    return tokenized_data\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "def preprocess_and_tokenize_dataset(file_path, model=None, noise_type=\"gaussian\", noise_level=NOISE_LEVEL, max_length='max_length'):\n",
    "    # Step 1: Load dataset\n",
    "    dataset = load_dataset(file_path)\n",
    "\n",
    "    # Step 2: Train Word2Vec model if not provided\n",
    "    if model is None:\n",
    "        model, performance_metrics_DICT = train_word2vec_embeddings(dataset)\n",
    "\n",
    "    # Step 3: Build vocabulary with embeddings\n",
    "    vocab_embeddings = build_vocab_and_embeddings(dataset, model)\n",
    "\n",
    "    # Step 4: Tokenize dataset and convert tokens to embeddings\n",
    "    tokenized_data = tokenize_dataset_with_embeddings(dataset, vocab_embeddings)\n",
    "\n",
    "    # Step 5: Normalize the data and determine the maximum input length\n",
    "    normalized_data = normalize_data(tokenized_data)\n",
    "\n",
    "    masked_data = add_masking_to_data(normalized_data)\n",
    "    \n",
    "    # Step 6: Add noise to the data\n",
    "    noisy_data = add_noise_to_data(normalized_data, noise_type, noise_level)\n",
    "\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in noisy_data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    \n",
    "    # Step 7: Pad sequences to a fixed length\n",
    "    padded_data = pad_sequences(noisy_data,MAX_LENGTH,vocab_embeddings['<PAD>'])\n",
    "    \n",
    "    return padded_data, vocab_embeddings, model, performance_metrics_DICT\n",
    "\n",
    "# Save preprocessed data\n",
    "def save_preprocessed_data(data, output_path):\n",
    "    \"\"\"Save the preprocessed data to a file with JSON serialization.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        if isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        for dp in data:\n",
    "            json.dump(convert_to_serializable(dp), file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def save_JSON(data, filename):\n",
    "    \"\"\"Save data to a JSON file with support for NumPy arrays.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(convert_to_serializable(data), f)\n",
    "        \n",
    "def load_JSON(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Main script execution\n",
    "file_path = \"Data/combined_dataset_5_variables_dynamic_seed20777980.json\"\n",
    "\n",
    "# Process the data and train Word2Vec embeddings\n",
    "preprocessed_data, vocab_embeddings, model, performance_metrics_DICT = preprocess_and_tokenize_dataset(file_path)\n",
    "\n",
    "# Save the preprocessed data, vocab embeddings, and trained Word2Vec model\n",
    "save_preprocessed_data(preprocessed_data, \"Data/preprocessed_data_with_embeddings.json\")\n",
    "save_JSON(vocab_embeddings, \"Data/vocab_embeddings.json\")\n",
    "\n",
    "print(\"Preprocessing complete with embeddings. Data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Performance Metrics:\n",
      "{'epoch_list': [1], 'train_loss_list': [0.0]}\n"
     ]
    }
   ],
   "source": [
    "performance_metrics_DICT = load_JSON('Data/embeddings_performance_metrics_DICT.json')\n",
    "\n",
    "model = Word2Vec.load(\"Data/embeddings_model.model\")\n",
    "\n",
    "print(\"Embeddings Performance Metrics:\")\n",
    "print(performance_metrics_DICT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
