{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.0\n",
      "Epoch 2/50, Train Loss: 0.0\n",
      "Epoch 3/50, Train Loss: 0.0\n",
      "Epoch 4/50, Train Loss: 0.0\n",
      "Early stopping occurred at epoch 4\n",
      "Preprocessing complete with embeddings. Data saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants for tokenization\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "NOISE_LEVEL = 0.1\n",
    "EMBEDDING_DIM = 100  # Set the dimension of the embeddings\n",
    "EPOCHS = 50  # Number of epochs for Word2Vec training\n",
    "PATIENCE = 3  # Patience for early stopping (number of epochs to wait for improvement)\n",
    "MIN_DELTA = 0.0001  # Minimum change in loss to count as an improvement\n",
    "\n",
    "def tokenize_skeleton(skeleton_str):\n",
    "    \"\"\"Tokenize the skeleton string into a sequence of symbols.\"\"\"\n",
    "    skeleton_str = skeleton_str.replace(\"**\", \"^\")  # Replace '**' with '^'\n",
    "    pattern = r'[a-zA-Z_][a-zA-Z0-9_]*|[+\\-*/^(),.]|C|sin|cos|log|exp|sqrt'\n",
    "    tokens = re.findall(pattern, skeleton_str)\n",
    "    return tokens\n",
    "\n",
    "def train_word2vec_embeddings(dataset, embedding_dim=EMBEDDING_DIM, epochs=EPOCHS, patience_num_epochs=PATIENCE):\n",
    "    \"\"\"Train Word2Vec embeddings on the dataset over multiple epochs with early stopping.\"\"\"\n",
    "    sentences = []\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        sentences.append(tokens)\n",
    "    \n",
    "    # Initialize the Word2Vec model\n",
    "    model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, epochs=1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    num_epochs_without_improvement = 0\n",
    "    early_stopping = False\n",
    "\n",
    "    performance_metrics_DICT = {\n",
    "        \"epoch_list\": [],\n",
    "        \"train_loss_list\": [],\n",
    "    }\n",
    "\n",
    "    # Train Word2Vec model with early stopping\n",
    "    for epoch in range(epochs-1):\n",
    "        # Train for one epoch\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "        # Get the current loss (Word2Vec model loss is stored in the 'trainables' attribute)\n",
    "        current_loss = model.get_latest_training_loss()\n",
    "\n",
    "        # Record loss\n",
    "        performance_metrics_DICT['epoch_list'].append(epoch+1)\n",
    "        performance_metrics_DICT['train_loss_list'].append(current_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {current_loss}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            model.save(\"Data/embeddings_model.model\")\n",
    "            num_epochs_without_improvement = 0\n",
    "            save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "        else:\n",
    "            num_epochs_without_improvement += 1\n",
    "\n",
    "        if num_epochs_without_improvement >= patience_num_epochs:\n",
    "            print(f'Early stopping occurred at epoch {epoch + 1}')\n",
    "            early_stopping = True\n",
    "            break\n",
    "        \n",
    "    if early_stopping == False:\n",
    "        model.save(\"Data/embeddings_model.model\")\n",
    "        save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "    return model, performance_metrics_DICT\n",
    "\n",
    "def build_vocab_and_embeddings(dataset, model=None):\n",
    "    \"\"\"Build vocabulary and convert tokens to embeddings using Word2Vec model.\"\"\"\n",
    "    token_set = set()\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        token_set.update(tokens)\n",
    "    \n",
    "    # Convert tokens to embeddings using the Word2Vec model\n",
    "    vocab_embeddings = {}\n",
    "    if model:\n",
    "        for token in token_set:\n",
    "            if token in model.wv:\n",
    "                embedding = model.wv[token]\n",
    "                # Normalize the embedding to have a unit norm (length = 1)\n",
    "                embedding = normalize([embedding])[0]\n",
    "                vocab_embeddings[token] = embedding\n",
    "            else:\n",
    "                # If the token isn't in the model, use a random vector\n",
    "                vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    else:\n",
    "        # If no pre-trained model is provided, use random embeddings for all tokens\n",
    "        for token in token_set:\n",
    "            vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    \n",
    "    # Add PAD_TOKEN to the vocabulary and set its embedding to zero\n",
    "    vocab_embeddings[PAD_TOKEN] = np.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    return vocab_embeddings\n",
    "\n",
    "def standardize_data(data):\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    data_std = np.std(data, axis=0)    \n",
    "    data_standardized = (data - data_mean)/data_std\n",
    "    return data_standardized\n",
    "\n",
    "def tokenize_dataset_with_embeddings(dataset, vocab_embeddings):\n",
    "    \"\"\"Tokenize the dataset and convert tokens to embeddings.\"\"\"\n",
    "    tokenized_data = []\n",
    "        \n",
    "    max_num_features  = max(len(entry['data']['x']) for entry in dataset)\n",
    "    \n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokenized_skeleton = [vocab_embeddings[token] for token in tokenize_skeleton(skeleton_str)]\n",
    "        \n",
    "        data = entry[\"data\"]\n",
    "        x_DICT = data['x'] \n",
    "                    \n",
    "        num_data_points = len(x_DICT[list(x_DICT.keys())[0]])\n",
    "        num_features = len(list(x_DICT.keys()))\n",
    "                \n",
    "        x_with_values_MAT = np.array(list(x_DICT.values())).T\n",
    "        x_with_values_standardized_MAT = standardize_data(x_with_values_MAT)\n",
    "        nan_MAT = np.full((num_data_points, max_num_features-num_features), np.nan)\n",
    "        x_standardized_MAT = np.concatenate((x_with_values_standardized_MAT, nan_MAT), axis=1)        \n",
    "        \n",
    "        y = np.array(data['y'])\n",
    "        y_standardized = standardize_data(y)\n",
    "        \n",
    "        mask = np.array([1]*num_features + [0]*(max_num_features-num_features))        \n",
    "        \n",
    "        data_DICT = {'x': x_standardized_MAT, 'y': y_standardized, 'mask': mask}\n",
    "        \n",
    "        tokenized_data.append({\n",
    "            \"tokens\": tokenized_skeleton,\n",
    "            \"data\": data_DICT,\n",
    "            \"skeleton\": skeleton_str\n",
    "        })\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "def pad_sequences(tokenized_data, max_length, pad_embedding, pad_token=PAD_TOKEN):\n",
    "    \"\"\"Pad tokenized sequences to a fixed length.\"\"\" \n",
    "    for dp in tokenized_data:\n",
    "        token_length = len(dp[\"tokens\"])\n",
    "        if token_length < max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"] + [pad_embedding]*(max_length - token_length)\n",
    "        elif token_length > max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"][:max_length]\n",
    "    return tokenized_data\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "def preprocess_and_tokenize_dataset(file_path, model=None, noise_type=\"gaussian\", noise_level=NOISE_LEVEL, max_length='max_length'):\n",
    "    # Step 1: Load dataset\n",
    "    dataset = load_dataset(file_path)\n",
    "\n",
    "    # Step 2: Train Word2Vec model if not provided\n",
    "    if model is None:\n",
    "        model, performance_metrics_DICT = train_word2vec_embeddings(dataset)\n",
    "\n",
    "    # Step 3: Build vocabulary with embeddings\n",
    "    vocab_embeddings = build_vocab_and_embeddings(dataset, model)\n",
    "\n",
    "    # Step 4: Tokenize dataset and convert tokens to embeddings\n",
    "    tokenized_data = tokenize_dataset_with_embeddings(dataset, vocab_embeddings)\n",
    "\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in tokenized_data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    \n",
    "    # Step 7: Pad sequences to a fixed length\n",
    "    padded_data = pad_sequences(tokenized_data,MAX_LENGTH,vocab_embeddings['<PAD>'])\n",
    "    \n",
    "    return padded_data, vocab_embeddings, model, performance_metrics_DICT\n",
    "\n",
    "# Save preprocessed data\n",
    "def save_preprocessed_data(data, output_path):\n",
    "    \"\"\"Save the preprocessed data to a file with JSON serialization.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        if isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        for dp in data:\n",
    "            json.dump(convert_to_serializable(dp), file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def save_JSON(data, filename):\n",
    "    \"\"\"Save data to a JSON file with support for NumPy arrays.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(convert_to_serializable(data), f)\n",
    "        \n",
    "def load_JSON(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Main script execution\n",
    "file_path = \"Data/combined_dataset_5_variables_dynamic_seed20777980.json\"\n",
    "\n",
    "# Process the data and train Word2Vec embeddings\n",
    "preprocessed_data, vocab_embeddings, model, performance_metrics_DICT = preprocess_and_tokenize_dataset(file_path)\n",
    "\n",
    "# Save the preprocessed data, vocab embeddings, and trained Word2Vec model\n",
    "save_preprocessed_data(preprocessed_data, \"Data/preprocessed_data_with_embeddings.json\")\n",
    "save_JSON(vocab_embeddings, \"Data/vocab_embeddings.json\")\n",
    "\n",
    "print(\"Preprocessing complete with embeddings. Data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Performance Metrics:\n",
      "{'epoch_list': [1], 'train_loss_list': [0.0]}\n"
     ]
    }
   ],
   "source": [
    "performance_metrics_DICT = load_JSON('Data/embeddings_performance_metrics_DICT.json')\n",
    "model = Word2Vec.load(\"Data/embeddings_model.model\")\n",
    "\n",
    "print(\"Embeddings Performance Metrics:\")\n",
    "print(performance_metrics_DICT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
