{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.0\n",
      "Epoch 2/50, Train Loss: 0.0\n",
      "Epoch 3/50, Train Loss: 0.0\n",
      "Epoch 4/50, Train Loss: 0.0\n",
      "Early stopping occurred at epoch 4\n",
      "Preprocessing complete with embeddings. Data saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import pdb\n",
    "\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants for tokenization\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "NOISE_LEVEL = 0.1\n",
    "EMBEDDING_DIM = 100  # Set the dimension of the embeddings\n",
    "EPOCHS = 50  # Number of epochs for Word2Vec training\n",
    "PATIENCE = 3  # Patience for early stopping (number of epochs to wait for improvement)\n",
    "MIN_DELTA = 0.0001  # Minimum change in loss to count as an improvement\n",
    "\n",
    "@dataclass\n",
    "class tNetConfig:\n",
    "    num_vars: int\n",
    "    embedding_size: int\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    def __init__(self, config: tNetConfig):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.num_vars = config.num_vars\n",
    "        self.n_embd = config.embedding_size\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(self.num_vars, self.n_embd, 1)\n",
    "        self.conv2 = nn.Conv1d(self.n_embd, 2*self.n_embd, 1)\n",
    "        self.conv3 = nn.Conv1d(2*self.n_embd, 4*self.n_embd, 1)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(4*self.n_embd, 2*self.n_embd)\n",
    "        self.fc2 = nn.Linear(2*self.n_embd, self.n_embd)\n",
    "\n",
    "        # Corrected GroupNorm initialization\n",
    "        self.input_batch_norm = nn.GroupNorm(1, self.num_vars)  # Corrected to match input channels\n",
    "        \n",
    "        # Define other GroupNorm layers\n",
    "        self.bn1 = nn.GroupNorm(1, self.n_embd)\n",
    "        self.bn2 = nn.GroupNorm(1, 2*self.n_embd)\n",
    "        self.bn3 = nn.GroupNorm(1, 4*self.n_embd)\n",
    "        self.bn4 = nn.GroupNorm(1, 2*self.n_embd)\n",
    "        self.bn5 = nn.GroupNorm(1, self.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply normalization and convolutions\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "\n",
    "        # Global max pooling\n",
    "        x, _ = torch.max(x, dim=2)  # Reducing along the sequence dimension (index 2)\n",
    "        assert x.size(1) == 4 * self.n_embd  # Ensure correct output size\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "        return x\n",
    "\n",
    "def tokenize_skeleton(skeleton_str):\n",
    "    \"\"\"Tokenize the skeleton string into a sequence of symbols.\"\"\"\n",
    "    skeleton_str = skeleton_str.replace(\"**\", \"^\")  # Replace '**' with '^'\n",
    "    pattern = r'[a-zA-Z_][a-zA-Z0-9_]*|[+\\-*/^(),.]|C|sin|cos|log|exp|sqrt'\n",
    "    tokens = re.findall(pattern, skeleton_str)\n",
    "    return tokens\n",
    "\n",
    "def train_word2vec_embeddings(dataset, embedding_dim=EMBEDDING_DIM, epochs=EPOCHS, patience_num_epochs=PATIENCE):\n",
    "    \"\"\"Train Word2Vec embeddings on the dataset over multiple epochs with early stopping.\"\"\"\n",
    "    sentences = []\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        sentences.append(tokens)\n",
    "    \n",
    "    # Initialize the Word2Vec model\n",
    "    model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, epochs=1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    num_epochs_without_improvement = 0\n",
    "    early_stopping = False\n",
    "\n",
    "    performance_metrics_DICT = {\n",
    "        \"epoch_list\": [],\n",
    "        \"train_loss_list\": [],\n",
    "    }\n",
    "\n",
    "    # Train Word2Vec model with early stopping\n",
    "    for epoch in range(epochs-1):\n",
    "        # Train for one epoch\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "        # Get the current loss (Word2Vec model loss is stored in the 'trainables' attribute)\n",
    "        current_loss = model.get_latest_training_loss()\n",
    "\n",
    "        # Record loss\n",
    "        performance_metrics_DICT['epoch_list'].append(epoch+1)\n",
    "        performance_metrics_DICT['train_loss_list'].append(current_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {current_loss}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            model.save(\"Data/embeddings_model.model\")\n",
    "            num_epochs_without_improvement = 0\n",
    "            save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "        else:\n",
    "            num_epochs_without_improvement += 1\n",
    "\n",
    "        if num_epochs_without_improvement >= patience_num_epochs:\n",
    "            print(f'Early stopping occurred at epoch {epoch + 1}')\n",
    "            early_stopping = True\n",
    "            break\n",
    "        \n",
    "    if early_stopping == False:\n",
    "        model.save(\"Data/embeddings_model.model\")\n",
    "        save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "    return model, performance_metrics_DICT\n",
    "\n",
    "def build_vocab_and_embeddings(dataset, model=None):\n",
    "    \"\"\"Build vocabulary and convert tokens to embeddings using Word2Vec model.\"\"\"\n",
    "    token_set = set()\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        token_set.update(tokens)\n",
    "    \n",
    "    # Convert tokens to embeddings using the Word2Vec model\n",
    "    vocab_embeddings = {}\n",
    "    if model:\n",
    "        for token in token_set:\n",
    "            if token in model.wv:\n",
    "                embedding = model.wv[token]\n",
    "                # Normalize the embedding to have a unit norm (length = 1)\n",
    "                embedding = normalize([embedding])[0]\n",
    "                vocab_embeddings[token] = embedding\n",
    "            else:\n",
    "                # If the token isn't in the model, use a random vector\n",
    "                vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    else:\n",
    "        # If no pre-trained model is provided, use random embeddings for all tokens\n",
    "        for token in token_set:\n",
    "            vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    \n",
    "    # Add PAD_TOKEN to the vocabulary and set its embedding to zero\n",
    "    vocab_embeddings[PAD_TOKEN] = np.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    return vocab_embeddings\n",
    "\n",
    "def standardize_data(data):\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    data_std = np.std(data, axis=0)    \n",
    "    data_standardized = (data - data_mean)/data_std\n",
    "    return data_standardized\n",
    "\n",
    "def tokenize_dataset_with_embeddings(dataset, vocab_embeddings):\n",
    "    \"\"\"Tokenize the dataset and convert tokens to embeddings.\"\"\"\n",
    "    tokenized_data = []\n",
    "        \n",
    "    max_num_features  = max(len(entry['data']['x']) for entry in dataset)\n",
    "    \n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokenized_skeleton = [vocab_embeddings[token] for token in tokenize_skeleton(skeleton_str)]\n",
    "        \n",
    "        data = entry[\"data\"]\n",
    "        x_DICT = data['x'] \n",
    "                    \n",
    "        num_data_points = len(x_DICT[list(x_DICT.keys())[0]])\n",
    "        num_features = len(list(x_DICT.keys()))\n",
    "                \n",
    "        x_with_values_MAT = np.array(list(x_DICT.values())).T\n",
    "        x_with_values_standardized_MAT = standardize_data(x_with_values_MAT)\n",
    "        nan_MAT = np.full((num_data_points, max_num_features-num_features), np.nan)\n",
    "        x_standardized_MAT = np.concatenate((x_with_values_standardized_MAT, nan_MAT), axis=1)        \n",
    "        \n",
    "        y = np.array(data['y'])\n",
    "        y_standardized = standardize_data(y)\n",
    "        \n",
    "        mask = np.array([1]*num_features + [0]*(max_num_features-num_features))        \n",
    "        \n",
    "        data_DICT = {'x': x_standardized_MAT, 'y': y_standardized, 'mask': mask}\n",
    "        \n",
    "        tokenized_data.append({\n",
    "            \"tokens\": tokenized_skeleton,\n",
    "            \"data\": data_DICT,\n",
    "            \"skeleton\": skeleton_str\n",
    "        })\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "def pad_sequences(tokenized_data, max_length, pad_embedding, pad_token=PAD_TOKEN):\n",
    "    \"\"\"Pad tokenized sequences to a fixed length.\"\"\" \n",
    "    for dp in tokenized_data:\n",
    "        token_length = len(dp[\"tokens\"])\n",
    "        if token_length < max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"] + [pad_embedding]*(max_length - token_length)\n",
    "        elif token_length > max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"][:max_length]\n",
    "    return tokenized_data\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "def add_TNet_embeddings(preprocessed_data):\n",
    "    tokenized_formulas = torch.tensor([datapoint['tokens'] for datapoint in preprocessed_data]).float()\n",
    "    datasets = torch.tensor([np.concatenate((datapoint['data']['x'], datapoint['data']['y'].reshape(len(datapoint['data']['y']),1)), axis=1) for datapoint in preprocessed_data])\n",
    "    datasets = torch.nan_to_num(datasets,nan=0.0).float()\n",
    "\n",
    "    batch_size, seq_len, embedding_dim = tokenized_formulas.shape\n",
    "    batch_size, num_points, num_features = datasets.shape\n",
    "\n",
    "    config_formula = tNetConfig(num_vars=seq_len, embedding_size=128) \n",
    "    config_data = tNetConfig(num_vars=num_points, embedding_size=128)\n",
    "\n",
    "    TNet_model_formula = tNet(config_formula) \n",
    "    TNet_model_data = tNet(config_data)\n",
    "\n",
    "    formula_embeddings = TNet_model_formula(tokenized_formulas)  # Shape: [num_formulas, embedding_size]\n",
    "    dataset_embeddings = TNet_model_data(datasets)  # Shape: [batch_size, embedding_size]\n",
    "\n",
    "    formula_embeddings = formula_embeddings.detach().cpu().numpy()\n",
    "    dataset_embeddings = dataset_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    # Add embeddings to each datapoint in preprocessed_data\n",
    "    for i, datapoint in enumerate(preprocessed_data):\n",
    "        datapoint['formula_embedding'] = formula_embeddings[i]\n",
    "        datapoint['dataset_embedding'] = dataset_embeddings[i]\n",
    "    return preprocessed_data\n",
    "\n",
    "def preprocess_and_tokenize_dataset(file_path, model=None, noise_type=\"gaussian\", noise_level=NOISE_LEVEL, max_length='max_length'):\n",
    "    # Step 1: Load dataset\n",
    "    dataset = load_dataset(file_path)\n",
    "\n",
    "    # Step 2: Train Word2Vec model if not provided\n",
    "    if model is None:\n",
    "        model, performance_metrics_DICT = train_word2vec_embeddings(dataset)\n",
    "\n",
    "    # Step 3: Build vocabulary with embeddings\n",
    "    vocab_embeddings = build_vocab_and_embeddings(dataset, model)\n",
    "\n",
    "    # Step 4: Tokenize dataset and convert tokens to embeddings\n",
    "    tokenized_data = tokenize_dataset_with_embeddings(dataset, vocab_embeddings)\n",
    "\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in tokenized_data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    \n",
    "    # Step 7: Pad sequences to a fixed length\n",
    "    padded_data = pad_sequences(tokenized_data,MAX_LENGTH,vocab_embeddings['<PAD>'])\n",
    "    \n",
    "    preprocessed_data = add_TNet_embeddings(padded_data)\n",
    "    \n",
    "    return padded_data, vocab_embeddings, model, performance_metrics_DICT\n",
    "\n",
    "# Save preprocessed data\n",
    "def save_preprocessed_data(data, output_path):\n",
    "    \"\"\"Save the preprocessed data to a file with JSON serialization.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        if isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        for dp in data:\n",
    "            json.dump(convert_to_serializable(dp), file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def save_JSON(data, filename):\n",
    "    \"\"\"Save data to a JSON file with support for NumPy arrays.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(convert_to_serializable(data), f)\n",
    "        \n",
    "def load_JSON(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Main script execution\n",
    "file_path = \"Data/combined_dataset_5_variables_dynamic_seed20777980.json\"\n",
    "\n",
    "# Process the data and train Word2Vec embeddings\n",
    "preprocessed_data, vocab_embeddings, model, performance_metrics_DICT = preprocess_and_tokenize_dataset(file_path)\n",
    "\n",
    "# Save the preprocessed data, vocab embeddings, and trained Word2Vec model\n",
    "save_preprocessed_data(preprocessed_data, \"Data/preprocessed_data_with_embeddings.json\")\n",
    "save_JSON(vocab_embeddings, \"Data/vocab_embeddings.json\")\n",
    "\n",
    "print(\"Preprocessing complete with embeddings. Data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Performance Metrics:\n",
      "{'epoch_list': [1], 'train_loss_list': [0.0]}\n"
     ]
    }
   ],
   "source": [
    "performance_metrics_DICT = load_JSON('Data/embeddings_performance_metrics_DICT.json')\n",
    "model = Word2Vec.load(\"Data/embeddings_model.model\")\n",
    "\n",
    "print(\"Embeddings Performance Metrics:\")\n",
    "print(performance_metrics_DICT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
