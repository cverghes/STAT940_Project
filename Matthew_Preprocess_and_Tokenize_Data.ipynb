{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Data saved to Dataset/preprocessed_data.json\n",
      "Vocabulary size: 16\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants for tokenization\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "NOISE_LEVEL = 0.1\n",
    "\n",
    "def tokenize_function(function_str):\n",
    "    \"\"\"Tokenize the function string into a sequence of symbols.\"\"\"\n",
    "    function_str = function_str.replace('**','^')  # Replace '**' with '^' for easier tokenization\n",
    "    # Regular expression to capture all valid tokens (numbers, variables, functions, operators)\n",
    "    pattern = r'[a-zA-Z_][a-zA-Z0-9_]*|[+\\-*/^(),.]|sin|cos|log|exp|sqrt'\n",
    "    tokens = re.findall(pattern, function_str)\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    \"\"\"Build vocabulary for the dataset.\"\"\"\n",
    "    token_set = set()\n",
    "    for entry in dataset:\n",
    "        function_str = entry[\"function\"]\n",
    "        tokens = tokenize_function(function_str)\n",
    "        token_set.update(tokens)\n",
    "    \n",
    "    # Add PAD_TOKEN\n",
    "    token_set.add(PAD_TOKEN)\n",
    "    \n",
    "    # Create a mapping from token to index (integer)\n",
    "    vocab = {token: idx for idx, token in enumerate(sorted(token_set))}\n",
    "    return vocab\n",
    "\n",
    "# Function to convert dataset to tokenized form\n",
    "def tokenize_dataset(dataset, vocab):\n",
    "    \"\"\"Tokenize the dataset of functions into token IDs.\"\"\"\n",
    "    tokenized_data = []\n",
    "    for entry in dataset:\n",
    "        function_str = entry[\"function\"]\n",
    "        tokenized_function = [vocab[token] for token in tokenize_function(function_str)]\n",
    "        \n",
    "        # For each data point, get the tokenized function and its inputs/outputs\n",
    "        for data_point in entry[\"data\"]:\n",
    "            inputs = data_point[\"inputs\"]\n",
    "            output = data_point[\"output\"]\n",
    "            tokenized_data.append({\n",
    "                \"tokens\": tokenized_function,\n",
    "                \"inputs\": inputs,\n",
    "                \"output\": output\n",
    "            })\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Normalize the inputs and outputs\n",
    "def normalize_data(data, range_vals=(-1, 1)):\n",
    "    \"\"\"Normalize the inputs and outputs to the specified range.\"\"\"\n",
    "    # Collect all unique input keys dynamically from the dataset\n",
    "    \n",
    "    input_keys = []\n",
    "    for dp in data:\n",
    "        input_keys.extend(dp[\"inputs\"].keys())\n",
    "    input_keys = sorted(set(input_keys), key=lambda x: (int(x[1:]), x))\n",
    "    \n",
    "    # Calculate the min and max for each input variable and output\n",
    "    all_inputs = [dp[\"inputs\"] for dp in data]\n",
    "    all_outputs = [dp[\"output\"] for dp in data]\n",
    "    \n",
    "    # Find the min and max for each input variable\n",
    "    input_mins = {key: min([inputs.get(key,float('inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    input_maxs = {key: max([inputs.get(key,float('-inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    output_min = min(all_outputs)\n",
    "    output_max = max(all_outputs)\n",
    "    \n",
    "    # Normalize inputs and outputs\n",
    "    for dp in data:\n",
    "        # Normalize inputs\n",
    "        normalized_inputs = {}\n",
    "        for key in input_keys:\n",
    "            if key in dp[\"inputs\"]:  # If the key exists, normalize it\n",
    "                normalized_inputs[key] = 2*(dp[\"inputs\"][key] - input_mins[key])/(input_maxs[key] - input_mins[key]) - 1\n",
    "            else:\n",
    "                # If the key is missing, assign a default value (e.g., 0 or skip normalization)\n",
    "                normalized_inputs[key] = np.nan  # Default value, adjust as needed\n",
    "        \n",
    "        # Normalize output\n",
    "        normalized_output = 2*(dp[\"output\"] - output_min)/(output_max - output_min) - 1\n",
    "        \n",
    "        dp[\"inputs\"] = normalized_inputs\n",
    "        dp[\"output\"] = normalized_output\n",
    "    \n",
    "    return data\n",
    "\n",
    "def add_masking_to_data(data):\n",
    "    \"\"\"Adds a mask to the data where 1 indicates valid data and 0 indicates NaN.\"\"\"\n",
    "    for dp in data:\n",
    "        dp[\"mask\"] = {key: 1 if not np.isnan(value) else 0 for key, value in dp[\"inputs\"].items()}\n",
    "    return data\n",
    "\n",
    "def add_noise_to_data(data, noise_type=\"gaussian\", noise_level=NOISE_LEVEL):\n",
    "    \"\"\"Add noise to the dataset while ensuring NaN values are ignored.\"\"\"\n",
    "    for dp in data:\n",
    "        for key, value in dp[\"inputs\"].items():\n",
    "            if not np.isnan(value):  # Only add noise if the value is not NaN\n",
    "                if noise_type == \"gaussian\":\n",
    "                    dp[\"inputs\"][key] += np.random.normal(0, noise_level)\n",
    "                elif noise_type == \"uniform\":\n",
    "                    dp[\"inputs\"][key] += np.random.uniform(-noise_level, noise_level)\n",
    "        # Add noise to the output as well, but only if it's not NaN\n",
    "        if not np.isnan(dp[\"output\"]):\n",
    "            if noise_type == \"gaussian\":\n",
    "                dp[\"output\"] += np.random.normal(0, noise_level)\n",
    "            elif noise_type == \"uniform\":\n",
    "                dp[\"output\"] += np.random.uniform(-noise_level, noise_level)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Pad sequences to fixed length\n",
    "def pad_sequences(tokenized_data, max_length, pad_token=PAD_TOKEN):    \n",
    "    \"\"\"Pad tokenized sequences to a fixed length.\"\"\"\n",
    "    for dp in tokenized_data:\n",
    "        token_length = len(dp[\"tokens\"])\n",
    "        \n",
    "        if token_length < max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"] + [pad_token]*(max_length - token_length)\n",
    "        elif token_length > max_length:\n",
    "            # Truncate the sequence if it's too long\n",
    "            dp[\"tokens\"] = dp[\"tokens\"][:max_length]\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Load dataset from JSON file\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "# Preprocess and tokenize the dataset\n",
    "def preprocess_and_tokenize_dataset(file_path, noise_type=\"gaussian\", noise_level=NOISE_LEVEL, max_length='max_length'):\n",
    "    # Step 1: Load dataset\n",
    "    dataset = load_dataset(file_path)\n",
    "\n",
    "    # Step 2: Build vocabulary\n",
    "    vocab = build_vocab(dataset)\n",
    "\n",
    "    # Step 3: Tokenize dataset\n",
    "    tokenized_data = tokenize_dataset(dataset, vocab)\n",
    "\n",
    "    # Step 4: Normalize the data and determine the maximum input length\n",
    "    normalized_data = normalize_data(tokenized_data)\n",
    "\n",
    "    masked_data = add_masking_to_data(normalized_data)\n",
    "    \n",
    "    # Step 5: Add noise to the data (pass input_keys)\n",
    "    noisy_data = add_noise_to_data(normalized_data, noise_type, noise_level)\n",
    "\n",
    "    # Calculate the max length dynamically from the tokenized data (or use a fixed value)\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in noisy_data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    \n",
    "    # Step 6: Pad sequences to a fixed length\n",
    "    padded_data = pad_sequences(noisy_data, max_length=MAX_LENGTH)\n",
    "\n",
    "    return padded_data,vocab\n",
    "\n",
    "# Save preprocessed data to a file\n",
    "def save_preprocessed_data(data, output_path):\n",
    "    \"\"\"Save the preprocessed data to a file.\"\"\"\n",
    "    with open(output_path, 'w') as file:\n",
    "        for dp in data:\n",
    "            json.dump(dp,file)\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "def save_JSON(data,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    return\n",
    "\n",
    "def load_JSON(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the original dataset\n",
    "    file_path = \"Dataset/combined_dataset_5_variables_dynamic_seed20777980.json\"\n",
    "    \n",
    "    # Preprocess and tokenize the dataset\n",
    "    preprocessed_data,vocab = preprocess_and_tokenize_dataset(file_path, noise_type=\"gaussian\",noise_level=0.1)\n",
    "\n",
    "    # Save the preprocessed data to a new file\n",
    "    save_preprocessed_data(preprocessed_data, \"Dataset/preprocessed_data.json\")\n",
    "    save_JSON(vocab, \"Dataset/vocab.json\")\n",
    "    \n",
    "    print(\"Preprocessing complete. Data saved to Dataset/preprocessed_data.json\")\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
