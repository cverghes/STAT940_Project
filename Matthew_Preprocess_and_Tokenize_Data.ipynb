{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Data saved to Data/preprocessed_data.json\n",
      "Vocabulary size: 15\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants for tokenization\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "NOISE_LEVEL = 0.1\n",
    "\n",
    "def tokenize_skeleton(skeleton_str):\n",
    "    \"\"\"Tokenize the skeleton string into a sequence of symbols.\"\"\"\n",
    "    # Replace common symbols for consistent tokenization\n",
    "    skeleton_str = skeleton_str.replace(\"**\", \"^\")  # Replace '**' with '^'\n",
    "    # Define a pattern to match variables, operators, and specific tokens\n",
    "    pattern = r'[a-zA-Z_][a-zA-Z0-9_]*|[+\\-*/^(),.]|C|sin|cos|log|exp|sqrt'\n",
    "    tokens = re.findall(pattern, skeleton_str)\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    \"\"\"Build vocabulary for the dataset.\"\"\"\n",
    "    token_set = set()\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]  # Replace \"function\" with \"skeleton\"\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        token_set.update(tokens)\n",
    "    \n",
    "    # Add PAD_TOKEN\n",
    "    token_set.add(PAD_TOKEN)\n",
    "    \n",
    "    # Create a mapping from token to index (integer)\n",
    "    vocab = {token: idx for idx, token in enumerate(sorted(token_set))}\n",
    "    return vocab\n",
    "\n",
    "# Function to convert dataset to tokenized form\n",
    "def tokenize_dataset(dataset, vocab):\n",
    "    \"\"\"Tokenize the dataset of skeletons into token IDs.\"\"\"\n",
    "    tokenized_data = []\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]  # Replace \"function\" with \"skeleton\"\n",
    "        tokenized_skeleton = [vocab[token] for token in tokenize_skeleton(skeleton_str)]\n",
    "        \n",
    "        # For each data point, get the tokenized skeleton and its inputs/outputs\n",
    "        for data_point in entry[\"data\"]:\n",
    "            inputs = data_point[\"inputs\"]\n",
    "            output = data_point[\"output\"]\n",
    "            tokenized_data.append({\n",
    "                \"tokens\": tokenized_skeleton,\n",
    "                \"inputs\": inputs,\n",
    "                \"output\": output\n",
    "            })\n",
    "    \n",
    "    return tokenized_data\n",
    "# Normalize the inputs and outputs\n",
    "def normalize_data(data, range_vals=(-1, 1)):\n",
    "    \"\"\"Normalize the inputs and outputs to the specified range.\"\"\"\n",
    "    # Collect all unique input keys dynamically from the dataset\n",
    "    \n",
    "    input_keys = []\n",
    "    for dp in data:\n",
    "        input_keys.extend(dp[\"inputs\"].keys())\n",
    "    input_keys = sorted(set(input_keys), key=lambda x: (int(x[1:]), x))\n",
    "    \n",
    "    # Calculate the min and max for each input variable and output\n",
    "    all_inputs = [dp[\"inputs\"] for dp in data]\n",
    "    all_outputs = [dp[\"output\"] for dp in data]\n",
    "    \n",
    "    # Find the min and max for each input variable\n",
    "    input_mins = {key: min([inputs.get(key,float('inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    input_maxs = {key: max([inputs.get(key,float('-inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    output_min = min(all_outputs)\n",
    "    output_max = max(all_outputs)\n",
    "    \n",
    "    # Normalize inputs and outputs\n",
    "    for dp in data:\n",
    "        # Normalize inputs\n",
    "        normalized_inputs = {}\n",
    "        for key in input_keys:\n",
    "            if key in dp[\"inputs\"]:  # If the key exists, normalize it\n",
    "                normalized_inputs[key] = 2*(dp[\"inputs\"][key] - input_mins[key])/(input_maxs[key] - input_mins[key]) - 1\n",
    "            else:\n",
    "                # If the key is missing, assign a default value (e.g., 0 or skip normalization)\n",
    "                normalized_inputs[key] = np.nan  # Default value, adjust as needed\n",
    "        \n",
    "        # Normalize output\n",
    "        normalized_output = 2*(dp[\"output\"] - output_min)/(output_max - output_min) - 1\n",
    "        \n",
    "        dp[\"inputs\"] = normalized_inputs\n",
    "        dp[\"output\"] = normalized_output\n",
    "    \n",
    "    return data\n",
    "\n",
    "def add_masking_to_data(data):\n",
    "    \"\"\"Adds a mask to the data where 1 indicates valid data and 0 indicates NaN.\"\"\"\n",
    "    for dp in data:\n",
    "        dp[\"mask\"] = {key: 1 if not np.isnan(value) else 0 for key, value in dp[\"inputs\"].items()}\n",
    "    return data\n",
    "\n",
    "def add_noise_to_data(data, noise_type=\"gaussian\", noise_level=NOISE_LEVEL):\n",
    "    \"\"\"Add noise to the dataset while ensuring NaN values are ignored.\"\"\"\n",
    "    for dp in data:\n",
    "        for key, value in dp[\"inputs\"].items():\n",
    "            if not np.isnan(value):  # Only add noise if the value is not NaN\n",
    "                if noise_type == \"gaussian\":\n",
    "                    dp[\"inputs\"][key] += np.random.normal(0, noise_level)\n",
    "                elif noise_type == \"uniform\":\n",
    "                    dp[\"inputs\"][key] += np.random.uniform(-noise_level, noise_level)\n",
    "        # Add noise to the output as well, but only if it's not NaN\n",
    "        if not np.isnan(dp[\"output\"]):\n",
    "            if noise_type == \"gaussian\":\n",
    "                dp[\"output\"] += np.random.normal(0, noise_level)\n",
    "            elif noise_type == \"uniform\":\n",
    "                dp[\"output\"] += np.random.uniform(-noise_level, noise_level)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Pad sequences to fixed length\n",
    "def pad_sequences(tokenized_data, max_length, pad_token=PAD_TOKEN):    \n",
    "    \"\"\"Pad tokenized sequences to a fixed length.\"\"\"\n",
    "    for dp in tokenized_data:\n",
    "        token_length = len(dp[\"tokens\"])\n",
    "        \n",
    "        if token_length < max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"] + [pad_token]*(max_length - token_length)\n",
    "        elif token_length > max_length:\n",
    "            # Truncate the sequence if it's too long\n",
    "            dp[\"tokens\"] = dp[\"tokens\"][:max_length]\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Load dataset from JSON file\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "# Preprocess and tokenize the dataset\n",
    "def preprocess_and_tokenize_dataset(file_path, noise_type=\"gaussian\", noise_level=NOISE_LEVEL, max_length='max_length'):\n",
    "    # Step 1: Load dataset\n",
    "    dataset = load_dataset(file_path)\n",
    "\n",
    "    # Step 2: Build vocabulary\n",
    "    vocab = build_vocab(dataset)\n",
    "\n",
    "    # Step 3: Tokenize dataset\n",
    "    tokenized_data = tokenize_dataset(dataset, vocab)\n",
    "\n",
    "    # Step 4: Normalize the data and determine the maximum input length\n",
    "    normalized_data = normalize_data(tokenized_data)\n",
    "\n",
    "    masked_data = add_masking_to_data(normalized_data)\n",
    "    \n",
    "    # Step 5: Add noise to the data (pass input_keys)\n",
    "    noisy_data = add_noise_to_data(normalized_data, noise_type, noise_level)\n",
    "\n",
    "    # Calculate the max length dynamically from the tokenized data (or use a fixed value)\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in noisy_data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    \n",
    "    # Step 6: Pad sequences to a fixed length\n",
    "    padded_data = pad_sequences(noisy_data, max_length=MAX_LENGTH)\n",
    "\n",
    "    return padded_data,vocab\n",
    "\n",
    "# Save preprocessed data to a file\n",
    "def save_preprocessed_data(data, output_path):\n",
    "    \"\"\"Save the preprocessed data to a file.\"\"\"\n",
    "    with open(output_path, 'w') as file:\n",
    "        for dp in data:\n",
    "            json.dump(dp,file)\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "def save_JSON(data,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    return\n",
    "\n",
    "def load_JSON(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Main script execution\n",
    "# Path to the original dataset\n",
    "file_path = \"Data/combined_dataset_5_variables_dynamic_seed20777980.json\"\n",
    "\n",
    "# Preprocess and tokenize the dataset\n",
    "preprocessed_data,vocab = preprocess_and_tokenize_dataset(file_path, noise_type=\"gaussian\",noise_level=0.1)\n",
    "\n",
    "# Save the preprocessed data to a new file\n",
    "save_preprocessed_data(preprocessed_data, \"Data/preprocessed_data.json\")\n",
    "save_JSON(vocab, \"Data/vocab.json\")\n",
    "\n",
    "print(\"Preprocessing complete. Data saved to Data/preprocessed_data.json\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using continuous vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.0\n",
      "Epoch 2/50, Train Loss: 0.0\n",
      "Epoch 3/50, Train Loss: 0.0\n",
      "Epoch 4/50, Train Loss: 0.0\n",
      "Early stopping occurred at epoch 4\n",
      "Preprocessing complete with embeddings. Data saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants for tokenization\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "NOISE_LEVEL = 0.1\n",
    "EMBEDDING_DIM = 100  # Set the dimension of the embeddings\n",
    "EPOCHS = 50  # Number of epochs for Word2Vec training\n",
    "PATIENCE = 3  # Patience for early stopping (number of epochs to wait for improvement)\n",
    "MIN_DELTA = 0.0001  # Minimum change in loss to count as an improvement\n",
    "\n",
    "def tokenize_skeleton(skeleton_str):\n",
    "    \"\"\"Tokenize the skeleton string into a sequence of symbols.\"\"\"\n",
    "    skeleton_str = skeleton_str.replace(\"**\", \"^\")  # Replace '**' with '^'\n",
    "    pattern = r'[a-zA-Z_][a-zA-Z0-9_]*|[+\\-*/^(),.]|C|sin|cos|log|exp|sqrt'\n",
    "    tokens = re.findall(pattern, skeleton_str)\n",
    "    return tokens\n",
    "\n",
    "def train_word2vec_embeddings(dataset, embedding_dim=EMBEDDING_DIM, epochs=EPOCHS, patience_num_epochs=PATIENCE):\n",
    "    \"\"\"Train Word2Vec embeddings on the dataset over multiple epochs with early stopping.\"\"\"\n",
    "    sentences = []\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        sentences.append(tokens)\n",
    "    \n",
    "    # Initialize the Word2Vec model\n",
    "    model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, epochs=1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    num_epochs_without_improvement = 0\n",
    "    early_stopping = False\n",
    "\n",
    "    performance_metrics_DICT = {\n",
    "        \"epoch_list\": [],\n",
    "        \"train_loss_list\": [],\n",
    "    }\n",
    "\n",
    "    # Train Word2Vec model with early stopping\n",
    "    for epoch in range(epochs-1):\n",
    "        # Train for one epoch\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "        # Get the current loss (Word2Vec model loss is stored in the 'trainables' attribute)\n",
    "        current_loss = model.get_latest_training_loss()\n",
    "\n",
    "        # Record loss\n",
    "        performance_metrics_DICT['epoch_list'].append(epoch+1)\n",
    "        performance_metrics_DICT['train_loss_list'].append(current_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {current_loss}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            model.save(\"Data/embeddings_model.model\")\n",
    "            num_epochs_without_improvement = 0\n",
    "            save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "        else:\n",
    "            num_epochs_without_improvement += 1\n",
    "\n",
    "        if num_epochs_without_improvement >= patience_num_epochs:\n",
    "            print(f'Early stopping occurred at epoch {epoch + 1}')\n",
    "            early_stopping = True\n",
    "            break\n",
    "        \n",
    "    if early_stopping == False:\n",
    "        model.save(\"Data/embeddings_model.model\")\n",
    "        save_JSON(performance_metrics_DICT,'Data/embeddings_performance_metrics_DICT.json')\n",
    "    return model, performance_metrics_DICT\n",
    "\n",
    "def build_vocab_and_embeddings(dataset, model=None):\n",
    "    \"\"\"Build vocabulary and convert tokens to embeddings using Word2Vec model.\"\"\"\n",
    "    token_set = set()\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokens = tokenize_skeleton(skeleton_str)\n",
    "        token_set.update(tokens)\n",
    "    \n",
    "    # Convert tokens to embeddings using the Word2Vec model\n",
    "    vocab_embeddings = {}\n",
    "    if model:\n",
    "        for token in token_set:\n",
    "            if token in model.wv:\n",
    "                embedding = model.wv[token]\n",
    "                # Normalize the embedding to have a unit norm (length = 1)\n",
    "                embedding = normalize([embedding])[0]\n",
    "                vocab_embeddings[token] = embedding\n",
    "            else:\n",
    "                # If the token isn't in the model, use a random vector\n",
    "                vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    else:\n",
    "        # If no pre-trained model is provided, use random embeddings for all tokens\n",
    "        for token in token_set:\n",
    "            vocab_embeddings[token] = np.random.uniform(-0.1, 0.1, size=EMBEDDING_DIM)\n",
    "    \n",
    "    # Add PAD_TOKEN to the vocabulary and set its embedding to zero\n",
    "    vocab_embeddings[PAD_TOKEN] = np.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    return vocab_embeddings\n",
    "\n",
    "def tokenize_dataset_with_embeddings(dataset, vocab_embeddings):\n",
    "    \"\"\"Tokenize the dataset and convert tokens to embeddings.\"\"\"\n",
    "    tokenized_data = []\n",
    "    for entry in dataset:\n",
    "        skeleton_str = entry[\"skeleton\"]\n",
    "        tokenized_skeleton = [vocab_embeddings[token] for token in tokenize_skeleton(skeleton_str)]\n",
    "        \n",
    "        for data_point in entry[\"data\"]:\n",
    "            inputs = data_point[\"inputs\"]\n",
    "            output = data_point[\"output\"]\n",
    "            tokenized_data.append({\n",
    "                \"tokens\": tokenized_skeleton,\n",
    "                \"inputs\": inputs,\n",
    "                \"output\": output\n",
    "            })\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "def normalize_data(data, range_vals=(-1, 1)):\n",
    "    \"\"\"Normalize the inputs and outputs to the specified range.\"\"\"\n",
    "    input_keys = []\n",
    "    for dp in data:\n",
    "        input_keys.extend(dp[\"inputs\"].keys())\n",
    "    input_keys = sorted(set(input_keys), key=lambda x: (int(x[1:]), x))\n",
    "    \n",
    "    all_inputs = [dp[\"inputs\"] for dp in data]\n",
    "    all_outputs = [dp[\"output\"] for dp in data]\n",
    "    \n",
    "    input_mins = {key: min([inputs.get(key, float('inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    input_maxs = {key: max([inputs.get(key, float('-inf')) for inputs in all_inputs]) for key in input_keys}\n",
    "    output_min = min(all_outputs)\n",
    "    output_max = max(all_outputs)\n",
    "    \n",
    "    for dp in data:\n",
    "        normalized_inputs = {}\n",
    "        for key in input_keys:\n",
    "            if key in dp[\"inputs\"]:\n",
    "                normalized_inputs[key] = 2 * (dp[\"inputs\"][key] - input_mins[key]) / (input_maxs[key] - input_mins[key]) - 1\n",
    "            else:\n",
    "                normalized_inputs[key] = np.nan  \n",
    "        \n",
    "        normalized_output = 2 * (dp[\"output\"] - output_min) / (output_max - output_min) - 1\n",
    "        \n",
    "        dp[\"inputs\"] = normalized_inputs\n",
    "        dp[\"output\"] = normalized_output\n",
    "    \n",
    "    return data\n",
    "\n",
    "def add_masking_to_data(data):\n",
    "    \"\"\"Adds a mask to the data where 1 indicates valid data and 0 indicates NaN.\"\"\"\n",
    "    for dp in data:\n",
    "        dp[\"mask\"] = {key: 1 if not np.isnan(value) else 0 for key, value in dp[\"inputs\"].items()}\n",
    "    return data\n",
    "\n",
    "def add_noise_to_data(data, noise_type=\"gaussian\", noise_level=NOISE_LEVEL):\n",
    "    \"\"\"Add noise to the dataset while ensuring NaN values are ignored.\"\"\"\n",
    "    for dp in data:\n",
    "        for key, value in dp[\"inputs\"].items():\n",
    "            if not np.isnan(value):\n",
    "                if noise_type == \"gaussian\":\n",
    "                    dp[\"inputs\"][key] += np.random.normal(0, noise_level)\n",
    "                elif noise_type == \"uniform\":\n",
    "                    dp[\"inputs\"][key] += np.random.uniform(-noise_level, noise_level)\n",
    "        if not np.isnan(dp[\"output\"]):\n",
    "            if noise_type == \"gaussian\":\n",
    "                dp[\"output\"] += np.random.normal(0, noise_level)\n",
    "            elif noise_type == \"uniform\":\n",
    "                dp[\"output\"] += np.random.uniform(-noise_level, noise_level)\n",
    "    return data\n",
    "\n",
    "def pad_sequences(tokenized_data, max_length, pad_token=PAD_TOKEN):\n",
    "    \"\"\"Pad tokenized sequences to a fixed length.\"\"\" \n",
    "    for dp in tokenized_data:\n",
    "        token_length = len(dp[\"tokens\"])\n",
    "        if token_length < max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"] + [pad_token] * (max_length - token_length)\n",
    "        elif token_length > max_length:\n",
    "            dp[\"tokens\"] = dp[\"tokens\"][:max_length]\n",
    "    return tokenized_data\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "def preprocess_and_tokenize_dataset(file_path, model=None, noise_type=\"gaussian\", noise_level=NOISE_LEVEL, max_length='max_length'):\n",
    "    # Step 1: Load dataset\n",
    "    dataset = load_dataset(file_path)\n",
    "\n",
    "    # Step 2: Train Word2Vec model if not provided\n",
    "    if model is None:\n",
    "        model, performance_metrics_DICT = train_word2vec_embeddings(dataset)\n",
    "\n",
    "    # Step 3: Build vocabulary with embeddings\n",
    "    vocab_embeddings = build_vocab_and_embeddings(dataset, model)\n",
    "\n",
    "    # Step 4: Tokenize dataset and convert tokens to embeddings\n",
    "    tokenized_data = tokenize_dataset_with_embeddings(dataset, vocab_embeddings)\n",
    "\n",
    "    # Step 5: Normalize the data and determine the maximum input length\n",
    "    normalized_data = normalize_data(tokenized_data)\n",
    "\n",
    "    masked_data = add_masking_to_data(normalized_data)\n",
    "    \n",
    "    # Step 6: Add noise to the data\n",
    "    noisy_data = add_noise_to_data(normalized_data, noise_type, noise_level)\n",
    "\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in noisy_data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    \n",
    "    # Step 7: Pad sequences to a fixed length\n",
    "    padded_data = pad_sequences(noisy_data, max_length=MAX_LENGTH)\n",
    "\n",
    "    return padded_data, vocab_embeddings, model, performance_metrics_DICT\n",
    "\n",
    "# Save preprocessed data\n",
    "def save_preprocessed_data(data, output_path):\n",
    "    \"\"\"Save the preprocessed data to a file with JSON serialization.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        if isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        for dp in data:\n",
    "            json.dump(convert_to_serializable(dp), file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def save_JSON(data, filename):\n",
    "    \"\"\"Save data to a JSON file with support for NumPy arrays.\"\"\"\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        return obj  # Return the object as-is if it's already serializable\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(convert_to_serializable(data), f)\n",
    "        \n",
    "def load_JSON(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Main script execution\n",
    "file_path = \"Data/combined_dataset_5_variables_dynamic_seed20777980.json\"\n",
    "\n",
    "# Process the data and train Word2Vec embeddings\n",
    "preprocessed_data, vocab_embeddings, model, performance_metrics_DICT = preprocess_and_tokenize_dataset(file_path)\n",
    "\n",
    "# Save the preprocessed data, vocab embeddings, and trained Word2Vec model\n",
    "save_preprocessed_data(preprocessed_data, \"Data/preprocessed_data_with_embeddings.json\")\n",
    "save_JSON(vocab_embeddings, \"Data/vocab_embeddings.json\")\n",
    "\n",
    "print(\"Preprocessing complete with embeddings. Data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Performance Metrics:\n",
      "{'epoch_list': [1], 'train_loss_list': [0.0]}\n"
     ]
    }
   ],
   "source": [
    "performance_metrics_DICT = load_JSON('Data/embeddings_performance_metrics_DICT.json')\n",
    "\n",
    "model = Word2Vec.load(\"Data/embeddings_model.model\")\n",
    "\n",
    "print(\"Embeddings Performance Metrics:\")\n",
    "print(performance_metrics_DICT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
