{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Diffusion Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_32456\\1103819058.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  token_ids = torch.tensor(token_ids, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 246\u001b[0m\n\u001b[0;32m    243\u001b[0m schedule \u001b[38;5;241m=\u001b[39m CosineNoiseSchedule(timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m \u001b[43mtrain_diffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 178\u001b[0m, in \u001b[0;36mtrain_diffusion_model\u001b[1;34m(model, train_loader, val_loader, optimizer, schedule, num_epochs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Loss computation using CrossEntropyLoss (you can switch to another loss if needed)\u001b[39;00m\n\u001b[0;32m    177\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(logits, target_tokens)\n\u001b[1;32m--> 178\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    181\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Define a dummy PAD token (you can set this to a proper value based on your vocab)\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def determine_max_seq_len(data, max_length='max_length'):\n",
    "    \"\"\"Calculate the max sequence length dynamically if 'max_length' is used as argument.\"\"\"\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    return MAX_LENGTH\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "def load_JSON(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Define a Noise Schedule using a Cosine schedule (commonly used in DDPMs)\n",
    "class CosineNoiseSchedule:\n",
    "    def __init__(self, timesteps=1000):\n",
    "        self.timesteps = timesteps\n",
    "        self.alphas = torch.cos(torch.linspace(0, math.pi / 2, timesteps))**2\n",
    "        self.betas = 1.0 - self.alphas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)  # Cumulative product of alphas\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return self.alphas[t]\n",
    "\n",
    "    def get_beta(self, t):\n",
    "        return self.betas[t]\n",
    "\n",
    "    def get_variance(self, t):\n",
    "        return self.get_beta(t) * (1 - self.get_alpha(t))\n",
    "    \n",
    "    def get_alpha_bar(self, t):\n",
    "        return self.alpha_bar[t]\n",
    "\n",
    "# Define the Diffusion Model (symbolic regression version)\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=4, num_timesteps=1000):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)  # For token prediction (symbolic generation)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        embedded_tokens = self.embedding(tokens)\n",
    "        embedded_tokens = embedded_tokens.transpose(0, 1)  # Change to (seq_len, batch_size, embedding_dim)\n",
    "        transformer_output = self.transformer(embedded_tokens, embedded_tokens)\n",
    "        logits = self.fc_out(transformer_output)\n",
    "        return logits\n",
    "    \n",
    "    def add_noise(self, token_ids, t, schedule):\n",
    "        \"\"\"Add noise to the token sequence during the forward diffusion process.\"\"\"\n",
    "        noisy_token_ids = token_ids.clone()\n",
    "        noise_level = schedule.get_variance(t)\n",
    "\n",
    "        # Apply noise as per variance (e.g., Gaussian noise)\n",
    "        noise = torch.normal(mean=0, std=noise_level, size=noisy_token_ids.shape)\n",
    "        noisy_token_ids = noisy_token_ids + noise.long()  # Convert to integer token IDs\n",
    "\n",
    "        return noisy_token_ids\n",
    "\n",
    "    def reverse_diffusion(self, noisy_input, schedule):\n",
    "        \"\"\"Apply reverse diffusion to denoise the sequence.\"\"\"\n",
    "        x_t = noisy_input\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            predicted_noise = self.forward(x_t)  # Predict noise at timestep t\n",
    "            alpha_t = schedule.get_alpha(t)\n",
    "            beta_t = schedule.get_beta(t)\n",
    "            alpha_bar_t = schedule.get_alpha_bar(t)\n",
    "            # Reverse the noise process\n",
    "            x_t = (x_t - beta_t * predicted_noise) / alpha_bar_t\n",
    "        return x_t\n",
    "\n",
    "# Loss function for training: Predict the noise at each timestep\n",
    "def denoising_loss(predicted_noise, noisy_input, clean_input):\n",
    "    \"\"\"Loss function for symbolic regression.\"\"\"\n",
    "    return nn.MSELoss()(predicted_noise, clean_input - noisy_input)\n",
    "\n",
    "# Dataset for symbolic regression\n",
    "class SymbolicRegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, vocab, max_seq_len, noise_schedule):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.noise_schedule = noise_schedule\n",
    "\n",
    "    def add_noise(self, token_ids, t, schedule):\n",
    "        \"\"\"Add noise to the token sequence during the forward diffusion process.\"\"\"\n",
    "        # Convert token_ids from list to a PyTorch tensor\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        noisy_token_ids = token_ids.clone()  # Now .clone() will work on the tensor\n",
    "        noise_level = schedule.get_variance(t)  # Get noise level\n",
    "\n",
    "        if isinstance(noise_level, torch.Tensor):\n",
    "            noise_level = noise_level.item()\n",
    "        \n",
    "        # Apply Gaussian noise\n",
    "        noise = torch.normal(mean=0, std=noise_level, size=noisy_token_ids.shape)\n",
    "        noisy_token_ids = noisy_token_ids + noise.long()  # Convert to integer token IDs\n",
    "\n",
    "        # Ensure the token IDs are within bounds\n",
    "        vocab_size = len(self.vocab)\n",
    "        noisy_token_ids = torch.clamp(noisy_token_ids, 0, vocab_size - 1)\n",
    "        \n",
    "        return noisy_token_ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        token_ids = data_point['tokens']\n",
    "        \n",
    "        # Convert tokens to their corresponding indices using the vocab\n",
    "        token_ids = [self.vocab.get(token, self.vocab[PAD_TOKEN]) for token in token_ids]\n",
    "        \n",
    "        # Pad the token sequence if necessary\n",
    "        pad_token_id = self.vocab.get('<PAD>', 5)\n",
    "        token_ids = token_ids + [pad_token_id] * (self.max_seq_len - len(token_ids))\n",
    "\n",
    "        # Convert to tensor (this step is now safe as token_ids contains integers)\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        t = torch.randint(0, 1000, (1,))  # Sample a random timestep\n",
    "        noisy_token_ids = self.add_noise(token_ids, t, self.noise_schedule)\n",
    "\n",
    "        # Return noisy token IDs and clean token IDs for loss computation\n",
    "        return noisy_token_ids, token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "# Training loop for the diffusion model\n",
    "def train_diffusion_model(model, train_loader, val_loader, optimizer, schedule, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (noisy_tokens, target_tokens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Randomly pick a timestep for each batch\n",
    "            t = random.randint(0, model.num_timesteps - 1)\n",
    "\n",
    "            # Add noise at the forward diffusion process\n",
    "            noisy_batch = torch.stack([model.add_noise(seq, t, schedule) for seq in noisy_tokens])\n",
    "            \n",
    "            # Forward pass through the model (predict denoised expression at timestep t)\n",
    "            logits = model(noisy_batch)  # Predict logits (denoised tokens)\n",
    "\n",
    "            # Flatten the logits and target tokens for loss computation\n",
    "            logits = logits.view(-1, logits.size(-1))  # Flatten the logits (seq_len * batch_size, vocab_size)\n",
    "            target_tokens = target_tokens.view(-1)  # Flatten the target tokens (seq_len * batch_size)\n",
    "\n",
    "            # Loss computation using CrossEntropyLoss (you can switch to another loss if needed)\n",
    "            loss = nn.CrossEntropyLoss()(logits, target_tokens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation step\n",
    "        validate_diffusion_model(model, val_loader, schedule)\n",
    "    return\n",
    "\n",
    "def validate_diffusion_model(model, val_loader, schedule):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for noisy_tokens, target_tokens in val_loader:\n",
    "            logits = model(noisy_tokens)  # Predict logits (denoised tokens)\n",
    "            \n",
    "            # Flatten logits and target tokens for loss computation\n",
    "            logits = logits.view(-1, logits.size(-1))  # Flatten (seq_len * batch_size, vocab_size)\n",
    "            target_tokens = target_tokens.view(-1)  # Flatten target tokens (seq_len * batch_size)\n",
    "            \n",
    "            # CrossEntropyLoss for token prediction\n",
    "            loss = nn.CrossEntropyLoss()(logits, target_tokens)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_loss / len(val_loader)}\")\n",
    "    model.train()\n",
    "    return\n",
    "\n",
    "# Sample generation\n",
    "def sample(model, schedule, batch_size=16):\n",
    "    # Start with random noise\n",
    "    noisy_input = torch.randint(0, len(vocab), (batch_size, model.num_timesteps))\n",
    "    denoised_output = model.reverse_diffusion(noisy_input, schedule)\n",
    "    return denoised_output\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Load preprocessed data (assuming `preprocessed_data.json` exists)\n",
    "dataset = load_dataset('Dataset/preprocessed_data.json')\n",
    "vocab = load_JSON(\"Dataset/vocab.json\")\n",
    "\n",
    "MAX_LENGTH = determine_max_seq_len(dataset)  # Determine the max length dynamically\n",
    "\n",
    "schedule = CosineNoiseSchedule(timesteps=1000)\n",
    "\n",
    "# Now pass the schedule when initializing SymbolicRegressionDataset\n",
    "full_dataset = SymbolicRegressionDataset(dataset, vocab, MAX_LENGTH, schedule)\n",
    "\n",
    "train_size = int(0.7 * len(full_dataset))  # 70% for training\n",
    "val_size = int(0.15 * len(full_dataset))  # 15% for validation\n",
    "test_size = len(full_dataset) - train_size - val_size  # 15% for testing\n",
    "\n",
    "# Perform random split\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader objects for each subset\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = DiffusionModel(vocab_size=len(vocab), embedding_dim=128, hidden_dim=256, num_layers=4, num_timesteps=1000)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "schedule = CosineNoiseSchedule(timesteps=1000)\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_model(model, train_loader, val_loader, optimizer, schedule, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
