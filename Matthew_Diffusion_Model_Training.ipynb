{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Diffusion Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Set the random seed (for replicability)\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def determine_max_seq_len(data, max_length='max_length'):\n",
    "    \"\"\"Calculate the max sequence length dynamically if 'max_length' is used as argument.\"\"\"\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    return MAX_LENGTH\n",
    "\n",
    "def setup_device():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    return model\n",
    "\n",
    "def load_model(model,filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()\n",
    "    \n",
    "    device = setup_device()\n",
    "    model.to(device)\n",
    "    return model,device\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "def save_JSON(data,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    return\n",
    "\n",
    "def load_JSON(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Define a Noise Schedule using a Cosine schedule (commonly used in DDPMs)\n",
    "class CosineNoiseSchedule:\n",
    "    def __init__(self, timesteps=1000):\n",
    "        self.timesteps = timesteps\n",
    "        self.alphas = torch.cos(torch.linspace(0, math.pi/2, timesteps))**2\n",
    "        self.betas = 1.0 - self.alphas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)  # Cumulative product of alphas\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return self.alphas[t]\n",
    "\n",
    "    def get_beta(self, t):\n",
    "        return self.betas[t]\n",
    "\n",
    "    def get_variance(self, t):\n",
    "        return self.get_beta(t) * (1 - self.get_alpha(t))\n",
    "    \n",
    "    def get_alpha_bar(self, t):\n",
    "        return self.alpha_bar[t]\n",
    "\n",
    "# Define the Diffusion Model (symbolic regression version)\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=4, num_timesteps=1000):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)  # For token prediction (symbolic generation)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        embedded_tokens = self.embedding(tokens)\n",
    "        embedded_tokens = embedded_tokens.transpose(0, 1)  # Change to (seq_len, batch_size, embedding_dim)\n",
    "        transformer_output = self.transformer(embedded_tokens, embedded_tokens)\n",
    "        logits = self.fc_out(transformer_output)\n",
    "        return logits\n",
    "    \n",
    "    def add_noise(self, token_ids, t, schedule):\n",
    "        \"\"\"Add noise to the token sequence during the forward diffusion process.\"\"\"\n",
    "        noisy_token_ids = token_ids.clone()\n",
    "        noise_level = schedule.get_variance(t)\n",
    "\n",
    "        # Apply noise as per variance (e.g., Gaussian noise)\n",
    "        noise = torch.normal(mean=0, std=noise_level, size=noisy_token_ids.shape)\n",
    "        noisy_token_ids = noisy_token_ids + noise.long()  # Convert to integer token IDs\n",
    "\n",
    "        # Ensure the token IDs are within bounds\n",
    "        vocab_size = self.vocab_size\n",
    "        noisy_token_ids = torch.clamp(noisy_token_ids, 0, vocab_size - 1)\n",
    "        return noisy_token_ids\n",
    "\n",
    "    def reverse_diffusion(self, noisy_input, schedule):\n",
    "        \"\"\"Apply reverse diffusion to denoise the sequence.\"\"\"\n",
    "        x_t = noisy_input\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            predicted_noise = self.forward(x_t)  # Predict noise at timestep t\n",
    "            alpha_t = schedule.get_alpha(t)\n",
    "            beta_t = schedule.get_beta(t)\n",
    "            alpha_bar_t = schedule.get_alpha_bar(t)\n",
    "            # Reverse the noise process\n",
    "            x_t = (x_t - beta_t * predicted_noise)/alpha_bar_t\n",
    "        return x_t\n",
    "\n",
    "# Loss function for training: Predict the noise at each timestep\n",
    "def denoising_loss(predicted_noise, noisy_input, clean_input):\n",
    "    \"\"\"Loss function for symbolic regression.\"\"\"\n",
    "    return nn.MSELoss()(predicted_noise, clean_input - noisy_input)\n",
    "\n",
    "# Dataset for symbolic regression\n",
    "class SymbolicRegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, vocab, max_seq_len, noise_schedule):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.noise_schedule = noise_schedule\n",
    "\n",
    "    def add_noise(self, token_ids, t, schedule):\n",
    "        \"\"\"Add noise to the token sequence during the forward diffusion process.\"\"\"\n",
    "        noisy_token_ids = token_ids.clone()\n",
    "        noise_level = schedule.get_variance(t)\n",
    "\n",
    "        # Apply noise as per variance (e.g., Gaussian noise)\n",
    "        noise = torch.normal(mean=0, std=noise_level, size=noisy_token_ids.shape)\n",
    "        noisy_token_ids = noisy_token_ids + noise.long()  # Convert to integer token IDs\n",
    "\n",
    "        # Ensure the token IDs are within bounds\n",
    "        vocab_size = self.vocab_size\n",
    "        noisy_token_ids = torch.clamp(noisy_token_ids, 0, vocab_size - 1)\n",
    "        return noisy_token_ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        temp_token_ids = data_point['tokens']\n",
    "        filtered_token_ids = [token for token in temp_token_ids if token != '<PAD>']\n",
    "\n",
    "        # Pad the token sequence if necessary\n",
    "        pad_token_id = self.vocab.get('<PAD>', 5)\n",
    "        token_ids = filtered_token_ids + [pad_token_id]*(self.max_seq_len - len(filtered_token_ids))\n",
    "\n",
    "        # Convert to tensor (this step is now safe as token_ids contains integers)\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        t = torch.randint(0, 1000, (1,))  # Sample a random timestep\n",
    "        noisy_token_ids = self.add_noise(token_ids, t, self.noise_schedule)\n",
    "\n",
    "        # Return noisy token IDs and clean token IDs for loss computation\n",
    "        return noisy_token_ids, token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "# Training loop for the diffusion model\n",
    "def train_diffusion_model(model,train_loader,val_loader,num_epochs=10,patience_num_epochs=3):\n",
    "    device = setup_device()\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    schedule = CosineNoiseSchedule(timesteps=1000)\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    num_epochs_without_improvement = 0\n",
    "    early_stopping = False\n",
    "    \n",
    "    performance_metrics_DICT = {\n",
    "        \"epoch_list\": [],\n",
    "        \"train_loss_list\": [],\n",
    "        \"val_loss_list\": []    \n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (noisy_tokens, target_tokens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Randomly pick a timestep for each batch\n",
    "            t = random.randint(0, model.num_timesteps - 1)\n",
    "\n",
    "            # Add noise at the forward diffusion process\n",
    "            noisy_batch = torch.stack([model.add_noise(seq, t, schedule) for seq in noisy_tokens])\n",
    "            \n",
    "            # Forward pass through the model (predict denoised expression at timestep t)\n",
    "            logits = model(noisy_batch)  # Predict logits (denoised tokens)\n",
    "\n",
    "            # Flatten the logits and target tokens for loss computation\n",
    "            logits = logits.view(-1, logits.size(-1))  # Flatten the logits (seq_len * batch_size, vocab_size)\n",
    "            target_tokens = target_tokens.view(-1)  # Flatten the target tokens (seq_len * batch_size)\n",
    "\n",
    "            # Loss computation using CrossEntropyLoss (you can switch to another loss if needed)\n",
    "            loss = nn.CrossEntropyLoss()(logits, target_tokens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        train_loss = total_loss/len(train_loader)\n",
    "        performance_metrics_DICT['train_loss_list'].append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for noisy_tokens, target_tokens in val_loader:\n",
    "                logits = model(noisy_tokens)  # Predict logits (denoised tokens)\n",
    "                \n",
    "                # Flatten logits and target tokens for loss computation\n",
    "                logits = logits.view(-1, logits.size(-1))  # Flatten (seq_len * batch_size, vocab_size)\n",
    "                target_tokens = target_tokens.view(-1)  # Flatten target tokens (seq_len * batch_size)\n",
    "                \n",
    "                # CrossEntropyLoss for token prediction\n",
    "                loss = nn.CrossEntropyLoss()(logits, target_tokens)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss = val_loss/len(val_loader)\n",
    "        performance_metrics_DICT['val_loss_list'].append(val_loss)\n",
    "\n",
    "        performance_metrics_DICT['epoch_list'].append(epoch+1)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(),\"Data/best_diffusion_model.pt\")\n",
    "            save_JSON(performance_metrics_DICT,'Data/diffusion_performance_metrics_DICT.json')\n",
    "            num_epochs_without_improvement = 0\n",
    "            best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            num_epochs_without_improvement += 1\n",
    "\n",
    "        if num_epochs_without_improvement >= patience_num_epochs:\n",
    "            print(f'Early stopping occurred at epoch {epoch + 1}')\n",
    "            early_stopping = True\n",
    "            break\n",
    "\n",
    "        if early_stopping:\n",
    "            model.load_state_dict(best_model_weights)\n",
    "            print(\"Selected the best model weights from early stopping.\")\n",
    "    \n",
    "    if early_stopping == False:\n",
    "        torch.save(model.state_dict(),\"Data/best_diffusion_model.pt\")\n",
    "        save_JSON(performance_metrics_DICT,'Data/diffusion_performance_metrics_DICT.json')\n",
    "    return model,performance_metrics_DICT\n",
    "\n",
    "# Sample generation\n",
    "def sample(model, schedule, vocab, batch_size=16):\n",
    "    # Start with random noise\n",
    "    noisy_input = torch.randint(0, len(vocab), (batch_size, model.num_timesteps))\n",
    "    denoised_output = model.reverse_diffusion(noisy_input, schedule)\n",
    "    return denoised_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('Data/preprocessed_data_with_embeddings.json')\n",
    "vocab = load_JSON(\"Data/vocab_embeddings.json\")\n",
    "\n",
    "MAX_LENGTH = determine_max_seq_len(dataset)  # Determine the max length dynamically\n",
    "\n",
    "schedule = CosineNoiseSchedule(timesteps=1000)\n",
    "\n",
    "# First, perform the split on the raw dataset\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "val_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # 15% for testing\n",
    "\n",
    "# Perform random split\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Initialize the SymbolicRegressionDataset with the schedule for each subset\n",
    "train_dataset_SR = SymbolicRegressionDataset(train_dataset, vocab, MAX_LENGTH, schedule)\n",
    "val_dataset_SR = SymbolicRegressionDataset(val_dataset, vocab, MAX_LENGTH, schedule)\n",
    "test_dataset_SR = SymbolicRegressionDataset(test_dataset, vocab, MAX_LENGTH, schedule)\n",
    "\n",
    "# Create DataLoader objects for each subset\n",
    "train_loader = DataLoader(train_dataset_SR, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_SR, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_SR, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = DiffusionModel(vocab_size=len(vocab), embedding_dim=128, hidden_dim=256, num_layers=4, num_timesteps=1000)\n",
    "\n",
    "# Train the model\n",
    "model,performance_metrics_DICT = train_diffusion_model(model,train_loader,val_loader,num_epochs=10,patience_num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# #Visualize the train and validation loss\n",
    "# def plot_train_valid(model_name,performance_metrics_DICT):\n",
    "#     plt.figure();\n",
    "#     plt.plot(performance_metrics_DICT['epoch_list'], performance_metrics_DICT['train_loss_list'], label=f'Train Loss', color='blue', linestyle='--', marker='o');\n",
    "#     plt.plot(performance_metrics_DICT['epoch_list'], performance_metrics_DICT['val_loss_list'], label=f'Validation Loss', color='green', linestyle='-', marker='x');\n",
    "#     plt.title(f'{model_name} Training and Validation Loss');\n",
    "#     plt.xlabel('Epochs');\n",
    "#     plt.ylabel('Loss');\n",
    "#     plt.legend();\n",
    "#     plt.grid();\n",
    "#     plt.xlim(0,max(performance_metrics_DICT['epoch_list'])+1);\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('Data/preprocessed_data.json')\n",
    "# vocab = load_JSON(\"Data/vocab.json\")\n",
    "\n",
    "# MAX_LENGTH = determine_max_seq_len(dataset)  # Determine the max length dynamically\n",
    "\n",
    "# schedule = CosineNoiseSchedule(timesteps=1000)\n",
    "\n",
    "# # Now pass the schedule when initializing SymbolicRegressionDataset\n",
    "# full_dataset = SymbolicRegressionDataset(dataset, vocab, MAX_LENGTH, schedule)\n",
    "\n",
    "# # First, perform the split on the raw dataset\n",
    "# train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "# val_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "# test_size = len(dataset) - train_size - val_size  # 15% for testing\n",
    "\n",
    "# # Perform random split\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Initialize the SymbolicRegressionDataset with the schedule for each subset\n",
    "# train_dataset_SR = SymbolicRegressionDataset(train_dataset, vocab, MAX_LENGTH, schedule)\n",
    "# val_dataset_SR = SymbolicRegressionDataset(val_dataset, vocab, MAX_LENGTH, schedule)\n",
    "# test_dataset_SR = SymbolicRegressionDataset(test_dataset, vocab, MAX_LENGTH, schedule)\n",
    "\n",
    "# # Create DataLoader objects for each subset\n",
    "# train_loader = DataLoader(train_dataset_SR, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset_SR, batch_size=16, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset_SR, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModel(vocab_size=len(vocab), embedding_dim=128, hidden_dim=256, num_layers=4, num_timesteps=1000)\n",
    "\n",
    "performance_metrics_DICT = load_JSON('Data/diffusion_performance_metrics_DICT.json')\n",
    "\n",
    "model,device = load_model(model,\"Data/best_diffusion_model.pt\")\n",
    "\n",
    "model_name = 'Diffusion Model'\n",
    "\n",
    "plot_train_valid(model_name,performance_metrics_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Convert token sequence to mathematical expression string\n",
    "def tokens_to_expression(tokens, vocab):\n",
    "    \"\"\"Convert a list of token IDs into a string representation of a mathematical expression.\"\"\"\n",
    "    # Reverse the vocab dictionary to map IDs back to their token strings\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    # Decode the token sequence to the corresponding expression\n",
    "    expression = ''.join([reverse_vocab[t] for t in tokens.squeeze(0).tolist() if reverse_vocab[t] != '<PAD>'])\n",
    "    \n",
    "    return expression\n",
    "\n",
    "# Evaluate expression against inputs using SymPy\n",
    "def evaluate_expression_sympy(expression, inputs):\n",
    "    \"\"\"Evaluate the mathematical expression using sympy for symbolic evaluation.\"\"\"\n",
    "    # Create sympy symbols for each input variable (e.g., x0, x1, etc.)\n",
    "    symbols = {f\"x{i}\": sp.symbols(f\"x{i}\") for i in range(len(inputs))}\n",
    "    \n",
    "    try:\n",
    "        # Parse the expression as a sympy expression\n",
    "        parsed_expression = sp.sympify(expression)\n",
    "        \n",
    "        # Substitute the input values into the expression\n",
    "        substituted_expr = parsed_expression.subs(symbols)\n",
    "        \n",
    "        # Evaluate the substituted expression\n",
    "        result = substituted_expr.evalf()  # Use evalf() to compute the numerical value\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating expression '{expression}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate the model on the dataset\n",
    "def evaluate_model_on_dataset(model, dataset, dataset_SR, vocab, schedule):\n",
    "    \"\"\"Evaluate the model on the dataset by generating symbolic expressions and comparing them to actual outputs.\"\"\"\n",
    "    model.eval()\n",
    "    total_mse = 0\n",
    "    total_mae = 0\n",
    "    num_examples = len(dataset)\n",
    "    \n",
    "    for data_point, data_point_SR in zip(dataset,dataset_SR):\n",
    "        # Extract the token sequence (post-denoising)\n",
    "        noisy_tokens = data_point_SR[0]  # Noisy tokens (output of the model)\n",
    "        true_output = data_point_SR[1]  # Ground truth output (target tokens)\n",
    "\n",
    "        # Decode the noisy token sequence to the corresponding expression\n",
    "        noisy_expression = tokens_to_expression(noisy_tokens, vocab)\n",
    "        true_expression = tokens_to_expression(true_output, vocab)\n",
    "\n",
    "        print(f\"noisy_expression: {noisy_expression}, true_expression = {true_expression}\")\n",
    "        \n",
    "        # For simplicity, assuming inputs are a predefined set of values (e.g., {'x0': 1, 'x1': 2, ...})\n",
    "        # Alternatively, you could replace this with actual input values if they exist in your dataset\n",
    "        inputs = data_point['inputs']\n",
    "        outputs = data_point['output']\n",
    "        \n",
    "        # Evaluate the expression on the input values\n",
    "        predicted_output = evaluate_expression_sympy(noisy_expression, inputs)\n",
    "        \n",
    "        if predicted_output is not None:\n",
    "            # Compute the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for prediction accuracy\n",
    "            mse = (predicted_output - true_output.item()) ** 2\n",
    "            mae = abs(predicted_output - true_output.item())\n",
    "            \n",
    "            total_mse += mse\n",
    "            total_mae += mae\n",
    "    \n",
    "    # Compute average metrics\n",
    "    avg_mse = total_mse/num_examples\n",
    "    avg_mae = total_mae/num_examples\n",
    "    \n",
    "    print(f\"Average MSE: {avg_mse}\")\n",
    "    print(f\"Average MAE: {avg_mae}\")\n",
    "    \n",
    "    return avg_mse, avg_mae\n",
    "\n",
    "avg_mse, avg_mae = evaluate_model_on_dataset(model, test_dataset, test_dataset_SR, vocab, schedule)\n",
    "\n",
    "print(f\"Test MSE: {avg_mse}\")\n",
    "print(f\"Test MAE: {avg_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    131\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 132\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnoisy_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass: Predict clean embeddings\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - University of Waterloo\\Documents\\Python Files\\Environments\\STAT940_Final_Project_VENV\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[179], line 100\u001b[0m, in \u001b[0;36mSymbolicRegressionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     98\u001b[0m data_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m     99\u001b[0m tokens \u001b[38;5;241m=\u001b[39m data_point[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 100\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Random timestep for diffusion\u001b[39;00m\n\u001b[0;32m    103\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m, (\u001b[38;5;241m1\u001b[39m,))\n",
      "Cell \u001b[1;32mIn[179], line 87\u001b[0m, in \u001b[0;36mSymbolicRegressionDataset.get_input_embeddings\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert tokens to their continuous embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[1;32mIn[179], line 87\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert tokens to their continuous embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens])\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Cosine Noise Schedule (remains the same)\n",
    "class CosineNoiseSchedule:\n",
    "    def __init__(self, timesteps=1000):\n",
    "        self.timesteps = timesteps\n",
    "        self.alphas = torch.cos(torch.linspace(0, math.pi / 2, timesteps)) ** 2\n",
    "        self.betas = 1.0 - self.alphas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)  # Cumulative product of alphas\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return self.alphas[t]\n",
    "\n",
    "    def get_beta(self, t):\n",
    "        return self.betas[t]\n",
    "\n",
    "    def get_variance(self, t):\n",
    "        return self.get_beta(t) * (1 - self.get_alpha(t))\n",
    "\n",
    "    def get_alpha_bar(self, t):\n",
    "        return self.alpha_bar[t]\n",
    "\n",
    "\n",
    "# Define the Diffusion Model (with continuous embeddings)\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=4, num_timesteps=1000):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)  # Output layer to predict token embeddings\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.transpose(0, 1)  # Change to (seq_len, batch_size, embedding_dim)\n",
    "        transformer_output = self.transformer(embeddings, embeddings)\n",
    "        logits = self.fc_out(transformer_output)  # Logits from output layer\n",
    "        return logits\n",
    "\n",
    "    def add_noise(self, token_embeddings, t, schedule):\n",
    "        \"\"\"Add noise to the token embeddings during the forward diffusion process.\"\"\"\n",
    "        noisy_embeddings = token_embeddings.clone()\n",
    "        noise_level = schedule.get_variance(t)\n",
    "        \n",
    "        # Apply Gaussian noise to the embeddings\n",
    "        noise = torch.normal(mean=0, std=noise_level, size=noisy_embeddings.shape)\n",
    "        noisy_embeddings = noisy_embeddings + noise\n",
    "\n",
    "        return noisy_embeddings\n",
    "\n",
    "    def reverse_diffusion(self, noisy_input, schedule):\n",
    "        \"\"\"Apply reverse diffusion to denoise the embedding sequence.\"\"\"\n",
    "        x_t = noisy_input\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            predicted_noise = self.forward(x_t)  # Predict noise at timestep t\n",
    "            alpha_t = schedule.get_alpha(t)\n",
    "            beta_t = schedule.get_beta(t)\n",
    "            alpha_bar_t = schedule.get_alpha_bar(t)\n",
    "            # Reverse the noise process\n",
    "            x_t = (x_t - beta_t * predicted_noise) / alpha_bar_t\n",
    "        return x_t\n",
    "\n",
    "\n",
    "# Loss function for continuous embeddings (MSE)\n",
    "def denoising_loss(predicted_embeddings, noisy_embeddings, clean_embeddings):\n",
    "    \"\"\"Loss function for symbolic regression (using MSE).\"\"\"\n",
    "    return nn.MSELoss()(predicted_embeddings, clean_embeddings)\n",
    "\n",
    "\n",
    "# Modify the dataset class to work with continuous embeddings\n",
    "class SymbolicRegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, vocab, max_seq_len, noise_schedule):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.noise_schedule = noise_schedule\n",
    "\n",
    "    def get_input_embeddings(self, tokens):\n",
    "        \"\"\"Convert tokens to their continuous embeddings.\"\"\"\n",
    "        return torch.stack([self.vocab[token] for token in tokens])  # Convert tokens to embeddings\n",
    "\n",
    "    def add_noise(self, token_embeddings, t, schedule):\n",
    "        \"\"\"Add noise to the token embeddings.\"\"\"\n",
    "        noisy_embeddings = token_embeddings.clone()\n",
    "        noise_level = schedule.get_variance(t)\n",
    "        noise = torch.normal(mean=0, std=noise_level, size=noisy_embeddings.shape)\n",
    "        noisy_embeddings = noisy_embeddings + noise\n",
    "        return noisy_embeddings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        tokens = data_point['tokens']\n",
    "        token_embeddings = self.get_input_embeddings(tokens)\n",
    "\n",
    "        # Random timestep for diffusion\n",
    "        t = torch.randint(0, 1000, (1,))\n",
    "        noisy_token_embeddings = self.add_noise(token_embeddings, t, self.noise_schedule)\n",
    "\n",
    "        return noisy_token_embeddings, token_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# Example of loading and preparing the dataset\n",
    "dataset = load_dataset('Data/preprocessed_data_with_embeddings.json')\n",
    "vocab = load_JSON(\"Data/vocab_embeddings.json\")  # Vocabulary is a dictionary of continuous embeddings\n",
    "\n",
    "MAX_LENGTH = determine_max_seq_len(dataset)\n",
    "\n",
    "# Create dataset and dataloaders for training\n",
    "schedule = CosineNoiseSchedule(timesteps=1000)\n",
    "train_dataset_SR = SymbolicRegressionDataset(dataset, vocab, MAX_LENGTH, schedule)\n",
    "train_loader = DataLoader(train_dataset_SR, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = DiffusionModel(vocab_size=len(vocab), embedding_dim=128, hidden_dim=256, num_layers=4, num_timesteps=1000)\n",
    "\n",
    "# Training loop (simplified for illustration)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for noisy_embeddings, target_embeddings in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Predict clean embeddings\n",
    "        pred_embeddings = model(noisy_embeddings)\n",
    "\n",
    "        # Compute the loss (using MSE)\n",
    "        loss = denoising_loss(pred_embeddings, noisy_embeddings, target_embeddings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
