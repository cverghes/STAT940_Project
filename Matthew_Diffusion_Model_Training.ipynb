{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Diffusion Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Set the random seed for replicability\n",
    "seed = 20777980\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def determine_max_seq_len(data, max_length='max_length'):\n",
    "    \"\"\"Calculate the max sequence length dynamically if 'max_length' is used as an argument.\"\"\"\n",
    "    if max_length == 'max_length':\n",
    "        MAX_LENGTH = max(len(dp[\"tokens\"]) for dp in data)\n",
    "    else:\n",
    "        MAX_LENGTH = max_length\n",
    "    return MAX_LENGTH\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"Set up the device for training.\"\"\"\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    \"\"\"Save the model's state dictionary.\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    return model\n",
    "\n",
    "def load_model(model, filepath):\n",
    "    \"\"\"Load a saved model state dictionary.\"\"\"\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()\n",
    "    device = setup_device()\n",
    "    model.to(device)\n",
    "    return model, device\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    return dataset\n",
    "\n",
    "def save_JSON(data, filename):\n",
    "    \"\"\"Save data to a JSON file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    return\n",
    "\n",
    "def load_JSON(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "class CosineNoiseSchedule:\n",
    "    def __init__(self, timesteps=1000, epsilon=1e-6, device=None):\n",
    "        self.timesteps = timesteps\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "        \n",
    "        # Create alphas using a cosine schedule\n",
    "        self.alphas = torch.cos(torch.linspace(0, math.pi / 2, timesteps, device=device)) ** 2\n",
    "        self.betas = 1.0 - self.alphas\n",
    "        self.alpha_bar = torch.maximum(torch.cumprod(self.alphas, dim=0), torch.tensor(self.epsilon, device=self.alphas.device))\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return self.alphas[t]\n",
    "\n",
    "    def get_beta(self, t):\n",
    "        return self.betas[t]\n",
    "\n",
    "    def get_variance(self, t):\n",
    "        return self.get_beta(t) * (1 - self.get_alpha(t))\n",
    "\n",
    "    def get_alpha_bar(self, t):\n",
    "        return self.alpha_bar[t]\n",
    "\n",
    "class SymbolicRegressionDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_seq_len, noise_schedule):\n",
    "        self.data = data\n",
    "        self.vocab = vocab  # Add vocab here\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.noise_schedule = noise_schedule\n",
    "\n",
    "    def get_input_embeddings(self, tokens):\n",
    "        embeddings = torch.stack([torch.tensor(token) for token in tokens])\n",
    "        padded_embeddings = nn.functional.pad(embeddings, (0, self.max_seq_len - embeddings.size(0)))\n",
    "        padded_embeddings = padded_embeddings.transpose(0,1)\n",
    "        return padded_embeddings\n",
    "\n",
    "    def add_noise(self, token_ids, t, schedule):\n",
    "        noisy_embeddings = token_ids.clone()\n",
    "        noise_level = torch.sqrt(schedule.get_variance(t))  # Use the schedule's variance\n",
    "        noise = torch.normal(mean=0.0, std=noise_level, size=noisy_embeddings.shape).to(noisy_embeddings.device)\n",
    "        noisy_token_embeddings = noisy_embeddings + noise\n",
    "        return torch.clamp(noisy_token_embeddings, min=0.0, max=1.0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        tokens = data_point['tokens']\n",
    "        current_data = data_point['data']\n",
    "        # Map symbols to embeddings using vocab\n",
    "        x = torch.tensor(current_data['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(current_data['y'], dtype=torch.float32)\n",
    "        mask = torch.tensor(current_data['mask'], dtype=torch.float32)\n",
    "        token_embeddings = self.get_input_embeddings(tokens)\n",
    "        \n",
    "        t = random.randint(0, self.noise_schedule.timesteps - 1)\n",
    "        noisy_token_embeddings = self.add_noise(token_embeddings, t, self.noise_schedule)\n",
    "        \n",
    "        noisy_x = self.add_noise(x, t, self.noise_schedule)  # Fix: using the dataset's add_noise method\n",
    "        noisy_y = self.add_noise(y, t, self.noise_schedule)  # Fix: using the dataset's add_noise method\n",
    "        \n",
    "        return token_embeddings, noisy_token_embeddings, noisy_x, noisy_y, t, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads, num_timesteps, max_seq_len=5000, pretrained_embeddings=None):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        if pretrained_embeddings is not None:\n",
    "            pretrained_embeddings = torch.tensor(list(pretrained_embeddings.values()), dtype=torch.float32)\n",
    "            if pretrained_embeddings.size(1) != embedding_dim:\n",
    "                raise ValueError(\n",
    "                    f\"Pretrained embeddings size {pretrained_embeddings.size(1)} does not match the required embedding_dim {embedding_dim}.\"\n",
    "                )\n",
    "            self.embedding = nn.Parameter(pretrained_embeddings)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.projection = nn.Linear(embedding_dim, hidden_dim) if embedding_dim != hidden_dim else nn.Identity()\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=False\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        batch_size, hidden_dim, seq_len = embeddings.shape\n",
    "        if self.embedding_dim != self.hidden_dim:\n",
    "            embeddings = self.projection(embeddings)\n",
    "        embeddings = embeddings.transpose(1, 2)\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = embeddings.transpose(0, 1)\n",
    "        embeddings = self.transformer(embeddings, embeddings)\n",
    "        embeddings = embeddings.transpose(0, 1)\n",
    "        logits = self.fc_out(embeddings)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        return logits\n",
    "\n",
    "    def add_noise(self, token_embeddings, t, schedule):\n",
    "        noise_level = torch.sqrt(schedule.get_variance(t))\n",
    "        noise = torch.normal(mean=0, std=noise_level, size=token_embeddings.shape).to(token_embeddings.device)\n",
    "        return token_embeddings + noise\n",
    "    \n",
    "    def reverse_diffusion(self, noisy_input, schedule):\n",
    "        x_t = noisy_input\n",
    "        device = x_t.device\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            predicted_noise = self.forward(x_t)\n",
    "            alpha_t = schedule.get_alpha(t)\n",
    "            beta_t = schedule.get_beta(t)\n",
    "            mean_x_prev = (x_t - beta_t * predicted_noise) / torch.sqrt(alpha_t)\n",
    "            if t > 0:\n",
    "                std_dev = torch.sqrt(beta_t)\n",
    "                noise = torch.randn_like(x_t, device=device) * std_dev\n",
    "                x_t = mean_x_prev + noise\n",
    "            else:\n",
    "                x_t = mean_x_prev\n",
    "            x_t = torch.clamp(x_t, min=-1.0, max=1.0)\n",
    "        return x_t\n",
    "\n",
    "def denoising_loss(predicted_embeddings, clean_embeddings):\n",
    "    return nn.MSELoss()(predicted_embeddings, clean_embeddings)\n",
    "\n",
    "def train_diffusion_model(model, train_loader, val_loader, num_epochs=10, patience_num_epochs=3):\n",
    "    device = setup_device()\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=2,factor=0.5)\n",
    "    schedule = CosineNoiseSchedule(timesteps=1000, device=device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    num_epochs_without_improvement = 0\n",
    "    early_stopping = False\n",
    "    performance_metrics = {\"epoch_list\": [], \"train_loss_list\": [], \"val_loss_list\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for token_embeddings, noisy_token_embeddings, x, y, t, mask in train_loader:\n",
    "            token_embeddings, noisy_token_embeddings, x, y, mask = token_embeddings.to(device), noisy_token_embeddings.to(device), x.to(device), y.to(device), mask.to(device)\n",
    "            mask = mask.to(device) if mask is not None else None\n",
    "            optimizer.zero_grad()\n",
    "            predicted_embeddings = model(noisy_token_embeddings)\n",
    "            loss = denoising_loss(predicted_embeddings, token_embeddings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_loss = total_loss/len(train_loader)\n",
    "        performance_metrics['train_loss_list'].append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for token_embeddings, noisy_token_embeddings, x, y, t, mask in val_loader:\n",
    "                token_embeddings, noisy_token_embeddings, x, y, mask = token_embeddings.to(device), noisy_token_embeddings.to(device), x.to(device), y.to(device), mask.to(device)\n",
    "                mask = mask.to(device) if mask is not None else None\n",
    "\n",
    "                # Forward pass\n",
    "                predicted_embeddings = model(noisy_token_embeddings)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = denoising_loss(predicted_embeddings, token_embeddings)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        performance_metrics['val_loss_list'].append(val_loss)\n",
    "        performance_metrics['epoch_list'].append(epoch + 1)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, \"best_diffusion_model.pt\")\n",
    "            num_epochs_without_improvement = 0\n",
    "        else:\n",
    "            num_epochs_without_improvement += 1\n",
    "\n",
    "        if num_epochs_without_improvement >= patience_num_epochs:\n",
    "            print(f\"Training stopped early at epoch {epoch + 1}. Best validation loss: {best_val_loss}\")\n",
    "            early_stopping = True\n",
    "            break\n",
    "        \n",
    "    if early_stopping == False:\n",
    "        save_model(model, \"best_diffusion_model.pt\")\n",
    "\n",
    "    return model, performance_metrics\n",
    "\n",
    "# Example of loading and preparing the dataset\n",
    "dataset = load_dataset('Data/preprocessed_data_with_embeddings.json')\n",
    "vocab = load_JSON(\"Data/vocab_embeddings.json\")  # Vocabulary is a dictionary of continuous embeddings\n",
    "\n",
    "MAX_LENGTH = determine_max_seq_len(dataset)  # Determine the max length dynamically\n",
    "\n",
    "schedule = CosineNoiseSchedule(timesteps=1000,device=setup_device())\n",
    "\n",
    "# First, perform the split on the raw dataset\n",
    "train_size = int(0.7*len(dataset))  # 70% for training\n",
    "val_size = int(0.15*len(dataset))  # 15% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # 15% for testing\n",
    "\n",
    "# Perform random split\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataset_SR = SymbolicRegressionDataset(train_dataset, vocab, MAX_LENGTH, schedule)\n",
    "val_dataset_SR = SymbolicRegressionDataset(val_dataset, vocab, MAX_LENGTH, schedule)\n",
    "test_dataset_SR = SymbolicRegressionDataset(test_dataset, vocab, MAX_LENGTH, schedule)\n",
    "\n",
    "# Create DataLoader objects for each subset\n",
    "train_loader = DataLoader(train_dataset_SR, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_SR, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_SR, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "num_heads = 4  # Number of attention heads, ensure this is a divisor of embedding_dim\n",
    "embedding_dim = 100  # The embedding dimension is 100 as per your problem\n",
    "hidden_dim = embedding_dim  # Hidden dimension stays the same for simplicity\n",
    "\n",
    "# Ensure embedding_dim is divisible by num_heads\n",
    "if embedding_dim % num_heads != 0:\n",
    "    raise ValueError(f\"embedding_dim ({embedding_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "\n",
    "model = DiffusionModel(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    num_timesteps=1000,\n",
    "    pretrained_embeddings=vocab\n",
    ")\n",
    "\n",
    "model,performance_metrics_DICT = train_diffusion_model(model,train_loader,val_loader,num_epochs=10,patience_num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# #Visualize the train and validation loss\n",
    "# def plot_train_valid(model_name,performance_metrics_DICT):\n",
    "#     plt.figure();\n",
    "#     plt.plot(performance_metrics_DICT['epoch_list'], performance_metrics_DICT['train_loss_list'], label=f'Train Loss', color='blue', linestyle='--', marker='o');\n",
    "#     plt.plot(performance_metrics_DICT['epoch_list'], performance_metrics_DICT['val_loss_list'], label=f'Validation Loss', color='green', linestyle='-', marker='x');\n",
    "#     plt.title(f'{model_name} Training and Validation Loss');\n",
    "#     plt.xlabel('Epochs');\n",
    "#     plt.ylabel('Loss');\n",
    "#     plt.legend();\n",
    "#     plt.grid();\n",
    "#     plt.xlim(0,max(performance_metrics_DICT['epoch_list'])+1);\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'Diffusion Model'\n",
    "\n",
    "# plot_train_valid(model_name,performance_metrics_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_embeddings_to_tokens(embeddings, vocab):\n",
    "#     vocab_embeddings = torch.stack([torch.tensor(embed) for embed in vocab.values()])\n",
    "    \n",
    "#     decoded_tokens = []\n",
    "#     for embedding in embeddings:\n",
    "#         # Compute the distance between the embedding and all vocab embeddings\n",
    "#         distances = torch.norm(embedding - vocab_embeddings, dim=1)\n",
    "#         closest_token_idx = torch.argmin(distances).item()\n",
    "#         closest_token = list(vocab.keys())[closest_token_idx]\n",
    "#         decoded_tokens.append(closest_token)\n",
    "    \n",
    "#     return decoded_tokens\n",
    "\n",
    "# def evaluate_diffusion_model(model, test_loader, vocab, schedule, device):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     total_test_loss = 0.0\n",
    "#     decoded_formulas = []\n",
    "#     actual_formulas = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for noisy_embeddings, target_embeddings in test_loader:\n",
    "#             # Get the predicted denoised embeddings\n",
    "#             t = random.randint(0, model.num_timesteps - 1)  # Random timestep for diffusion\n",
    "#             pred_embeddings = model.reverse_diffusion(noisy_embeddings, schedule)\n",
    "\n",
    "#             # Calculate the loss (MSE between predicted and target embeddings)\n",
    "#             loss = denoising_loss(pred_embeddings, target_embeddings)\n",
    "#             total_test_loss += loss.item()\n",
    "\n",
    "#             # Now, we need to decode the denoised embeddings back to tokens\n",
    "#             decoded_tokens = decode_embeddings_to_tokens(pred_embeddings, vocab)\n",
    "\n",
    "#             # Convert the decoded tokens to a formula string\n",
    "#             predicted_formula = \" \".join(decoded_tokens)\n",
    "#             decoded_formulas.append(predicted_formula)\n",
    "\n",
    "#             # Assuming target embeddings have a corresponding ground truth formula (you can adjust this part)\n",
    "#             actual_formula = decode_embeddings_to_tokens(target_embeddings, vocab)\n",
    "#             actual_formulas.append(\" \".join(actual_formula))\n",
    "\n",
    "#     # Calculate average test loss\n",
    "#     avg_test_loss = total_test_loss / len(test_loader)\n",
    "#     return avg_test_loss, decoded_formulas, actual_formulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of loading and preparing the dataset\n",
    "# dataset = load_dataset('Data/preprocessed_data_with_embeddings.json')\n",
    "# vocab = load_JSON(\"Data/vocab_embeddings.json\")  # Vocabulary is a dictionary of continuous embeddings\n",
    "\n",
    "# MAX_LENGTH = determine_max_seq_len(dataset)  # Determine the max length dynamically\n",
    "\n",
    "# schedule = CosineNoiseSchedule(timesteps=1000,device=setup_device())\n",
    "\n",
    "# # First, perform the split on the raw dataset\n",
    "# train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "# val_size = int(0.15 * len(dataset))  # 15% for validation\n",
    "# test_size = len(dataset) - train_size - val_size  # 15% for testing\n",
    "\n",
    "# # Perform random split\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Initialize the SymbolicRegressionDataset with the schedule for each subset\n",
    "# train_dataset_SR = SymbolicRegressionDataset(train_dataset, vocab, MAX_LENGTH, schedule)\n",
    "# val_dataset_SR = SymbolicRegressionDataset(val_dataset, vocab, MAX_LENGTH, schedule)\n",
    "# test_dataset_SR = SymbolicRegressionDataset(test_dataset, vocab, MAX_LENGTH, schedule)\n",
    "\n",
    "# # Create DataLoader objects for each subset\n",
    "# train_loader = DataLoader(train_dataset_SR, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset_SR, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset_SR, batch_size=16, shuffle=True)\n",
    "\n",
    "# # Initialize the model\n",
    "# num_heads = 4  # Number of attention heads, ensure this is a divisor of embedding_dim\n",
    "# embedding_dim = 100  # The embedding dimension is 100 as per your problem\n",
    "# hidden_dim = embedding_dim  # Hidden dimension stays the same for simplicity\n",
    "\n",
    "# # Ensure embedding_dim is divisible by num_heads\n",
    "# if embedding_dim % num_heads != 0:\n",
    "#     raise ValueError(f\"embedding_dim ({embedding_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "\n",
    "# model = DiffusionModel(\n",
    "#     vocab_size=len(vocab),\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     hidden_dim=hidden_dim,\n",
    "#     num_layers=6,\n",
    "#     num_heads=num_heads,\n",
    "#     num_timesteps=1000,\n",
    "#     pretrained_embeddings=vocab\n",
    "# )\n",
    "\n",
    "# model, device = load_model(model, 'Data/best_diffusion_model.pt')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Evaluate the model on the test set\n",
    "# test_loss, decoded_formulas, actual_formulas = evaluate_diffusion_model(model, test_loader, vocab, schedule, device)\n",
    "\n",
    "# # Print out the average test loss\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Print out the first few decoded formulas and their corresponding actual formulas\n",
    "# for predicted, actual in zip(decoded_formulas[:5], actual_formulas[:5]):\n",
    "#     print(f\"Predicted Formula: {predicted}\")\n",
    "#     print(f\"Actual Formula: {actual}\")\n",
    "#     print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
