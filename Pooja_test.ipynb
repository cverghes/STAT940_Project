{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the folder path containing the JSON files\n",
    "folder_path = \"data_symbolic_regression/train\"\n",
    "\n",
    "# Function to tokenize a formula\n",
    "def tokenize_formula(formula):\n",
    "    # Define a regex pattern to extract tokens (identifiers, operators, parentheses, etc.)\n",
    "    token_pattern = r\"[a-zA-Z_][a-zA-Z0-9_]*|[()+\\-*/]|\\d+\\.?\\d*\"\n",
    "    tokens = re.findall(token_pattern, formula)\n",
    "    return tokens\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Open and read the JSON file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            # Extract the human-readable formula\n",
    "            formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "            \n",
    "            if formula_human_readable:\n",
    "                # Tokenize the formula\n",
    "                tokens = tokenize_formula(formula_human_readable)\n",
    "                \n",
    "                # Print or store the tokens for further analysis\n",
    "                print(f\"File: {file_name}\")\n",
    "                print(f\"Formula: {formula_human_readable}\")\n",
    "                print(f\"Tokens: {tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class TextDiffusionModel:\n",
    "    def __init__(self, vocab_size, seq_len, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the text diffusion model.\n",
    "\n",
    "        Parameters:\n",
    "        - vocab_size: Size of the vocabulary (number of unique tokens).\n",
    "        - seq_len: Length of the token sequence.\n",
    "        - device: Device to use (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        self.noise_schedule = torch.linspace(0.01, 0.1, steps=1000).to(device)  # Noise variance per timestep\n",
    "\n",
    "    def add_noise(self, tokens, t):\n",
    "        \"\"\"\n",
    "        Add noise to a sequence of tokens based on timestep t.\n",
    "\n",
    "        Parameters:\n",
    "        - tokens: A tensor of token indices with shape (batch_size, seq_len).\n",
    "        - t: A tensor of timesteps with shape (batch_size,).\n",
    "\n",
    "        Returns:\n",
    "        - noisy_tokens: The tokens with added noise.\n",
    "        - noise: The noise added to the tokens.\n",
    "        \"\"\"\n",
    "        noise_std = self.noise_schedule[t].view(-1, 1, 1)  # Shape: (batch_size, 1, 1)\n",
    "\n",
    "        # Convert tokens to one-hot vectors\n",
    "        one_hot = F.one_hot(tokens, num_classes=self.vocab_size).float()\n",
    "        \n",
    "        # Add Gaussian noise to the one-hot vectors\n",
    "        noise = torch.randn_like(one_hot) * noise_std\n",
    "        noisy_one_hot = one_hot + noise\n",
    "\n",
    "        # Compute softmax to normalize the noisy one-hot vectors\n",
    "        noisy_tokens = F.softmax(noisy_one_hot, dim=-1)\n",
    "        return noisy_tokens, noise\n",
    "\n",
    "    def sample_from_noisy_tokens(self, noisy_tokens):\n",
    "        \"\"\"\n",
    "        Sample discrete tokens from the noisy token distribution.\n",
    "\n",
    "        Parameters:\n",
    "        - noisy_tokens: A tensor of noisy token distributions with shape (batch_size, seq_len, vocab_size).\n",
    "\n",
    "        Returns:\n",
    "        - sampled_tokens: A tensor of sampled token indices with shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        sampled_tokens = torch.argmax(noisy_tokens, dim=-1)\n",
    "        return sampled_tokens\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    vocab_size = 100  # Example vocabulary size\n",
    "    seq_len = 10  # Example sequence length\n",
    "    batch_size = 4  # Example batch size\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "\n",
    "    # Generate a batch of random tokens\n",
    "    tokens = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "\n",
    "    # Choose random timesteps for each batch\n",
    "    t = torch.randint(0, 1000, (batch_size,), device=device)\n",
    "\n",
    "    # Add noise to the tokens\n",
    "    noisy_tokens, noise = model.add_noise(tokens, t)\n",
    "\n",
    "    # Sample from noisy tokens\n",
    "    sampled_tokens = model.sample_from_noisy_tokens(noisy_tokens)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Original Tokens:\", tokens)\n",
    "    print(\"Noisy Tokens (probabilities):\", noisy_tokens)\n",
    "    print(\"Sampled Tokens:\", sampled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Define a function to tokenize a formula\n",
    "def tokenize_formula(formula):\n",
    "    token_pattern = r\"[a-zA-Z_][a-zA-Z0-9_]*|[()+\\-*/]|\\d+\\.?\\d*\"\n",
    "    tokens = re.findall(token_pattern, formula)\n",
    "    return tokens\n",
    "\n",
    "@dataclass\n",
    "class tNetConfig:\n",
    "    num_vars: int\n",
    "    embedding_size: int\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    def __init__(self, config: tNetConfig):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.num_vars = config.num_vars\n",
    "        self.n_embd = config.embedding_size\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "\n",
    "        self.conv1 = nn.Conv1d(self.num_vars + 1, self.n_embd, 1)\n",
    "        self.conv2 = nn.Conv1d(self.n_embd, 2 * self.n_embd, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.n_embd, 4 * self.n_embd, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * self.n_embd, 2 * self.n_embd)\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        self.input_batch_norm = nn.GroupNorm(1, self.num_vars + 1)\n",
    "\n",
    "        self.bn1 = nn.GroupNorm(1, self.n_embd)\n",
    "        self.bn2 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn3 = nn.GroupNorm(1, 4 * self.n_embd)\n",
    "        self.bn4 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn5 = nn.GroupNorm(1, self.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features + 1, #points]\n",
    "        :return: logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.n_embd\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class TextDiffusionModel:\n",
    "    def __init__(self, vocab_size, seq_len, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the text diffusion model.\n",
    "\n",
    "        Parameters:\n",
    "        - vocab_size: Size of the vocabulary (number of unique tokens).\n",
    "        - seq_len: Length of the token sequence.\n",
    "        - device: Device to use (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        # self.noise_schedule = torch.linspace(0.01, 0.1, steps=1000).to(device)  # Noise variance per timestep\n",
    "        self.noise_schedule = torch.linspace(1e-4, 2e-2, steps=1000).to(device)  # Noise variance per timestep\n",
    "\n",
    "    def add_noise(self, tokens, t):\n",
    "        \"\"\"\n",
    "        Add noise to a sequence of tokens based on timestep t.\n",
    "\n",
    "        Parameters:\n",
    "        - tokens: A tensor of token indices with shape (batch_size, seq_len).\n",
    "        - t: A tensor of timesteps with shape (batch_size,).\n",
    "\n",
    "        Returns:\n",
    "        - noisy_tokens: The tokens with added noise.\n",
    "        - noise: The noise added to the tokens.\n",
    "        \"\"\"\n",
    "        noise_std = self.noise_schedule[t].view(-1, 1, 1)  # Shape: (batch_size, 1, 1)\n",
    "\n",
    "        # Convert tokens to one-hot vectors\n",
    "        one_hot = F.one_hot(tokens.long(), num_classes=self.vocab_size).float()\n",
    "        \n",
    "        # Add Gaussian noise to the one-hot vectors\n",
    "        noise = torch.randn_like(one_hot) * noise_std\n",
    "        noisy_one_hot = one_hot + noise\n",
    "\n",
    "        # Compute softmax to normalize the noisy one-hot vectors\n",
    "        noisy_tokens = F.softmax(noisy_one_hot, dim=-1)\n",
    "        return noisy_tokens, noise\n",
    "\n",
    "    def sample_from_noisy_tokens(self, noisy_tokens):\n",
    "        \"\"\"\n",
    "        Sample discrete tokens from the noisy token distribution.\n",
    "\n",
    "        Parameters:\n",
    "        - noisy_tokens: A tensor of noisy token distributions with shape (batch_size, seq_len, vocab_size).\n",
    "\n",
    "        Returns:\n",
    "        - sampled_tokens: A tensor of sampled token indices with shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        sampled_tokens = torch.argmax(noisy_tokens, dim=-1)\n",
    "        return sampled_tokens\n",
    "\n",
    "class ReverseProcessModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_vars, seq_len):\n",
    "        super(ReverseProcessModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_vars = num_vars\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Calculate the correct input size for fc1\n",
    "        input_size = embedding_size + (seq_len * vocab_size) + 1  # embeddings + noisy_tokens + timestep\n",
    "\n",
    "        # Define layers for the reverse process model\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, seq_len * vocab_size)  # Output for all tokens in the sequence\n",
    "\n",
    "    def forward(self, noisy_tokens, embeddings, t):\n",
    "        \"\"\"\n",
    "        Forward pass for the reverse process model.\n",
    "\n",
    "        :param noisy_tokens: Tensor of noisy tokens with shape [batch_size, seq_len, vocab_size].\n",
    "        :param embeddings: Tensor of embeddings with shape [batch_size, embedding_size].\n",
    "        :param t: Tensor of timesteps with shape [batch_size].\n",
    "        :return: Predicted noise.\n",
    "        \"\"\"\n",
    "        # Flatten noisy tokens to [batch_size, seq_len * vocab_size]\n",
    "        noisy_tokens_flat = noisy_tokens.view(noisy_tokens.size(0), -1)\n",
    "\n",
    "        # Concatenate embeddings, flattened noisy tokens, and timestep information\n",
    "        timestep_embedding = torch.cat([embeddings, noisy_tokens_flat, t.unsqueeze(1).float()], dim=-1)\n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = F.relu(self.fc1(timestep_embedding))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        predicted_noise = self.fc3(x)\n",
    "\n",
    "        # Reshape to [batch_size, seq_len, vocab_size]\n",
    "        predicted_noise = predicted_noise.view(-1, self.seq_len, self.vocab_size)\n",
    "        \n",
    "        return predicted_noise\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    folder_path = \"data_symbolic_regression/train\"\n",
    "    val_folder_path = \"data_symbolic_regression/val\"\n",
    "\n",
    "    # Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "    tokenized_formulas = []\n",
    "    points_list = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "                if formula_human_readable:\n",
    "                    tokens = tokenize_formula(formula_human_readable)\n",
    "                    tokenized_formulas.append(tokens)\n",
    "                \n",
    "                points = data.get(\"points\")\n",
    "                if points:\n",
    "                    points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                    points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                    points_list.append(points_tensor)\n",
    "                    # Need below line if points_array is transposed\n",
    "                    # points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0).permute(0, 2, 1)  # Add batch dimension and transpose\n",
    "\n",
    "    # # Create the vocabulary from the tokens\n",
    "    # vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas for t in tokens))}\n",
    "    # vocab_size = len(vocab_mapping)\n",
    "\n",
    "    # token_sequences = [[vocab_mapping[token] for token in tokens] for tokens in tokenized_formulas]\n",
    "\n",
    "    # formula_lengths = [len(tokens) for tokens in tokenized_formulas]\n",
    "    # seq_len = int(np.percentile(formula_lengths, 95))  # Use 95th percentile\n",
    "\n",
    "    val_tokenized_formulas = []\n",
    "    val_points_list = []\n",
    "\n",
    "    # Create the vocabulary from the tokens\n",
    "    for file_name in os.listdir(val_folder_path):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                val_data = json.load(file)\n",
    "\n",
    "                val_formula_human_readable = val_data.get(\"formula_human_readable\", \"\")\n",
    "                if val_formula_human_readable:\n",
    "                    val_tokens = tokenize_formula(val_formula_human_readable)\n",
    "                    val_tokenized_formulas.append(val_tokens)\n",
    "                \n",
    "                val_points = val_data.get(\"points\")\n",
    "                if val_points:\n",
    "                    val_points_array = np.array([val_points[\"var_0\"], val_points[\"var_1\"], val_points[\"var_2\"], val_points[\"target\"]])\n",
    "                    val_points_tensor = torch.tensor(val_points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                    val_points_list.append(val_points_tensor)\n",
    "\n",
    "    vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas for t in tokens))}\n",
    "    vocab_size = len(vocab_mapping)\n",
    "\n",
    "    # Define EOS and PAD token IDs\n",
    "    eos_token_id = vocab_size - 1  # Assuming the last ID in the vocabulary is for EOS\n",
    "    pad_token_id = vocab_size - 2  # Assuming the second-to-last ID in the vocabulary is for PAD\n",
    "\n",
    "    # Add EOS and PAD tokens to vocab_mapping if not already present\n",
    "    if eos_token_id not in vocab_mapping.values():\n",
    "        vocab_mapping['<EOS>'] = eos_token_id\n",
    "    if pad_token_id not in vocab_mapping.values():\n",
    "        vocab_mapping['<PAD>'] = pad_token_id\n",
    "\n",
    "    # Tokenize and map tokens to vocabulary indices\n",
    "    token_sequences = [[vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in tokenized_formulas]\n",
    "\n",
    "    # Calculate sequence length based on the 95th percentile of formula lengths\n",
    "    formula_lengths = [len(tokens) for tokens in tokenized_formulas]\n",
    "    seq_len = int(np.percentile(formula_lengths, 95))  # Use 95th percentile\n",
    "    batch_size = 100  # Example batch size\n",
    "\n",
    "    # Pad or truncate sequences to seq_len, adding EOS token last\n",
    "    token_sequences = [\n",
    "        seq[:seq_len] + [pad_token_id] * max(0, seq_len - len(seq)) + [eos_token_id] \n",
    "        if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "        for seq in token_sequences\n",
    "    ]\n",
    "\n",
    "    # Convert to tensor\n",
    "    token_tensor = torch.tensor(token_sequences, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    diffusion_model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "    \n",
    "    # Pad or truncate sequences to seq_len\n",
    "    token_sequences = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in token_sequences]\n",
    "    token_tensor = torch.tensor(token_sequences, device=device)\n",
    "\n",
    "    val_vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in val_tokenized_formulas for t in tokens))}\n",
    "    val_vocab_size = len(val_vocab_mapping)\n",
    "\n",
    "    # Add EOS and PAD tokens to vocab_mapping if not already present\n",
    "    if eos_token_id not in val_vocab_mapping.values():\n",
    "        val_vocab_mapping['<EOS>'] = eos_token_id\n",
    "    if pad_token_id not in val_vocab_mapping.values():\n",
    "        val_vocab_mapping['<PAD>'] = pad_token_id\n",
    "\n",
    "    # Tokenize and map tokens to vocabulary indices\n",
    "    val_token_sequences = [[val_vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in val_tokenized_formulas]\n",
    "\n",
    "    # Calculate sequence length based on the 95th percentile of formula lengths\n",
    "    val_formula_lengths = [len(tokens) for tokens in val_tokenized_formulas]\n",
    "    val_seq_len = int(np.percentile(val_formula_lengths, 95))  # Use 95th percentile\n",
    "\n",
    "    # Pad or truncate sequences to seq_len, adding EOS token last\n",
    "    val_token_sequences = [\n",
    "        seq[:seq_len] + [pad_token_id] * max(0, seq_len - len(seq)) + [eos_token_id] \n",
    "        if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "        for seq in val_token_sequences\n",
    "    ]\n",
    "\n",
    "    # Convert to tensor\n",
    "    val_token_tensor = torch.tensor(val_token_sequences, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    diffusion_model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "    \n",
    "    # Pad or truncate sequences to seq_len\n",
    "    val_token_sequences = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in val_token_sequences]\n",
    "    val_token_tensor = torch.tensor(val_token_sequences, device=device)\n",
    "\n",
    "    # # Add EOS token and pad sequences\n",
    "    # # Define EOS and PAD token IDs\n",
    "    # eos_token_id = vocab_size - 1  # Assuming the last ID in the vocabulary is for EOS\n",
    "    # pad_token_id = vocab_size - 2  # Assuming the second-to-last ID is for PAD\n",
    "\n",
    "    # # Add EOS token and pad sequences\n",
    "    # max_seq_len = max(len(seq) for seq in token_tensor)\n",
    "    # padded_token_tensor = []\n",
    "    # for seq in token_tensor:\n",
    "    #     seq = torch.cat([seq, torch.tensor([eos_token_id])])  # Add EOS token\n",
    "    #     padding = torch.tensor([pad_token_id] * (max_seq_len - len(seq)))  # Add padding\n",
    "    #     seq = torch.cat([seq, padding])  # Concatenate the sequence and padding\n",
    "    #     padded_token_tensor.append(seq)\n",
    "\n",
    "    # # Convert to a tensor\n",
    "    # token_tensor = torch.stack(padded_token_tensor)\n",
    "\n",
    "    # Choose random timesteps for each sequence\n",
    "    t = torch.randint(0, 1000, (len(token_tensor),), device=device)\n",
    "\n",
    "    # Add noise to the tokens\n",
    "    noisy_tokens, noise = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "    # Sample from noisy tokens\n",
    "    sampled_tokens = diffusion_model.sample_from_noisy_tokens(noisy_tokens)\n",
    "\n",
    "    # Configuration for tNet\n",
    "    num_vars = 3\n",
    "    embedding_size = 128  # Example embedding size\n",
    "    config = tNetConfig(num_vars=num_vars, embedding_size=embedding_size)\n",
    "\n",
    "    # Instantiate the model\n",
    "    tnet_model = tNet(config)\n",
    "\n",
    "    # Input: batch_size x (num_vars + 1) x num_points\n",
    "    batch_size = 1\n",
    "\n",
    "    # Generate embeddings\n",
    "    # input_tensor = torch.rand(batch_size, num_vars, 100)\n",
    "\n",
    "    output_embeddings = []\n",
    "    for pt in points_list:\n",
    "        output_embedding = tnet_model(pt)\n",
    "        output_embeddings.append(output_embedding)\n",
    "    \n",
    "    points_tensors = torch.cat(points_list, dim=0)\n",
    "    \n",
    "    output_embeddings_tensor = torch.cat(output_embeddings, dim=0)\n",
    "    # Print the output\n",
    "    print(\"Input shape:\", points_tensors.shape)\n",
    "    print(\"Output shape:\", output_embeddings_tensor.shape)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Original Tokens shape:\", token_tensor.shape)\n",
    "    print(\"Noisy Tokens (probabilities) shape:\", noisy_tokens.shape)\n",
    "    print(\"Sampled Tokens shape:\", sampled_tokens.shape)\n",
    "\n",
    "    # Initialize reverse model (denoiser)\n",
    "    reverse_model = ReverseProcessModel(vocab_size, embedding_size, num_vars, seq_len).to(device)\n",
    "\n",
    "    # Cross-entropy loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the reverse process model\n",
    "    epochs = 1000  # Define the number of epochs for training\n",
    "    batch_size = 100  # Example batch size\n",
    "\n",
    "    # Optimizer for the reverse process model\n",
    "    optimizer = torch.optim.Adam(reverse_model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        reverse_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training Phase\n",
    "        for batch_idx in range(0, len(points_list), batch_size):\n",
    "            batch_points = points_list[batch_idx:batch_idx + batch_size]\n",
    "            batch_token_tensor = token_tensor[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Random timesteps\n",
    "            t_batch = torch.randint(0, 1000, (len(batch_points),), device=device)\n",
    "\n",
    "            # Add noise\n",
    "            noisy_tokens, _ = diffusion_model.add_noise(batch_token_tensor, t_batch)\n",
    "\n",
    "            # Get embeddings\n",
    "            batch_embeddings = [tnet_model(pt) for pt in batch_points]\n",
    "            embeddings_tensor = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "            # Predict logits\n",
    "            logits = reverse_model(noisy_tokens, embeddings_tensor, t_batch)\n",
    "\n",
    "            # Reshape logits and target tokens for CrossEntropyLoss\n",
    "            logits_flat = logits.view(-1, vocab_size)\n",
    "            target_tokens = batch_token_tensor.view(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits_flat, target_tokens)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        reverse_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx in range(0, len(val_points_list), batch_size):\n",
    "                val_batch_points = val_points_list[val_batch_idx:val_batch_idx + batch_size]\n",
    "                val_batch_token_tensor = val_token_tensor[val_batch_idx:val_batch_idx + batch_size]\n",
    "\n",
    "                val_t_batch = torch.randint(0, 1000, (len(val_batch_points),), device=device)\n",
    "\n",
    "                val_noisy_tokens, _ = diffusion_model.add_noise(val_batch_token_tensor, val_t_batch)\n",
    "\n",
    "                val_embeddings = [tnet_model(pt) for pt in val_batch_points]\n",
    "                val_embeddings_tensor = torch.cat(val_embeddings, dim=0)\n",
    "\n",
    "                val_logits = reverse_model(val_noisy_tokens, val_embeddings_tensor, val_t_batch)\n",
    "                val_logits_flat = val_logits.view(-1, vocab_size)\n",
    "                val_target_tokens = val_batch_token_tensor.view(-1)\n",
    "\n",
    "                val_loss += loss_fn(val_logits_flat, val_target_tokens).item()\n",
    "\n",
    "        val_loss /= len(val_points_list)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(points_list):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(reverse_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered. Restoring best model...\")\n",
    "            reverse_model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break\n",
    "\n",
    "    # for epoch in range(epochs):\n",
    "    #     reverse_model.train()\n",
    "        \n",
    "    #     total_loss = 0\n",
    "    #     for batch_idx in range(0, len(points_list), batch_size):\n",
    "    #         # Select batch of noisy tokens and corresponding points\n",
    "    #         batch_points = points_list[batch_idx:batch_idx + batch_size]\n",
    "    #         batch_token_tensor = token_tensor[batch_idx:batch_idx + batch_size]\n",
    "            \n",
    "    #         # Choose random timesteps for the batch\n",
    "    #         t_batch = torch.randint(0, 1000, (len(batch_points),), device=device)\n",
    "            \n",
    "    #         # Add noise to the tokens (forward diffusion)\n",
    "    #         noisy_tokens, _ = diffusion_model.add_noise(batch_token_tensor, t_batch)\n",
    "            \n",
    "    #         # Get embeddings from tNet model\n",
    "    #         batch_embeddings = []\n",
    "    #         for pt in batch_points:\n",
    "    #             embedding = tnet_model(pt)\n",
    "    #             batch_embeddings.append(embedding)\n",
    "            \n",
    "    #         embeddings_tensor = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "    #         # Pass noisy tokens, embeddings, and timestep through reverse model to predict logits\n",
    "    #         logits = reverse_model(noisy_tokens, embeddings_tensor, t_batch)\n",
    "\n",
    "        #     # Reshape logits and target tokens for CrossEntropyLoss\n",
    "        #     # Logits shape: (batch_size, seq_len, vocab_size) -> (batch_size * seq_len, vocab_size)\n",
    "        #     logits_flat = logits.view(-1, vocab_size)\n",
    "\n",
    "        #     # Target tokens shape: (batch_size, seq_len) -> (batch_size * seq_len)\n",
    "        #     target_tokens = batch_token_tensor.view(-1)\n",
    "\n",
    "        #     # Compute cross-entropy loss\n",
    "        #     loss = loss_fn(logits_flat, target_tokens)\n",
    "            \n",
    "        #     # Backpropagate and optimize\n",
    "        #     optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "        #     total_loss += loss.item()\n",
    "\n",
    "        # # Print progress every epoch\n",
    "        # avg_loss = total_loss / len(points_list)\n",
    "        # print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Optimizer for the reverse process model\n",
    "    # optimizer = torch.optim.Adam(reverse_model.parameters(), lr=1e-4)\n",
    "\n",
    "    # # Loss function: MSE between predicted noise and actual noise\n",
    "    # loss_fn = nn.MSELoss()\n",
    "\n",
    "    # # Train the reverse process\n",
    "    # epochs = 100  # Define the number of epochs for training\n",
    "\n",
    "    # for epoch in range(epochs):\n",
    "    #     reverse_model.train()\n",
    "        \n",
    "    #     total_loss = 0\n",
    "    #     for batch_idx in range(0, len(points_list), batch_size):\n",
    "    #         # Select batch of noisy tokens and corresponding points\n",
    "    #         batch_points = points_list[batch_idx:batch_idx + batch_size]\n",
    "    #         batch_token_tensor = token_tensor[batch_idx:batch_idx + batch_size]\n",
    "            \n",
    "    #         # Choose random timesteps for the batch\n",
    "    #         t_batch = torch.randint(0, 1000, (len(batch_points),), device=device)\n",
    "            \n",
    "    #         # Add noise to the tokens (forward diffusion)\n",
    "    #         noisy_tokens, noise = diffusion_model.add_noise(batch_token_tensor, t_batch)\n",
    "            \n",
    "    #         # Get embeddings from tNet model\n",
    "    #         batch_embeddings = []\n",
    "    #         for pt in batch_points:\n",
    "    #             embedding = tnet_model(pt)\n",
    "    #             batch_embeddings.append(embedding)\n",
    "            \n",
    "        #     embeddings_tensor = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "        #     # Pass noisy tokens, embeddings, and timestep through reverse model to predict noise\n",
    "        #     predicted_noise = reverse_model(noisy_tokens, embeddings_tensor, t_batch)\n",
    "            \n",
    "        #     # Compute loss (MSE between predicted noise and actual noise)\n",
    "        #     loss = loss_fn(predicted_noise, noise.view(-1, vocab_size))\n",
    "            \n",
    "        #     # Backpropagate and optimize\n",
    "        #     optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "        #     total_loss += loss.item()\n",
    "\n",
    "        # # Print progress every epoch\n",
    "        # print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(points_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_diffusion_model(test_folder, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device):\n",
    "    \"\"\"\n",
    "    Evaluate the diffusion model on the test set.\n",
    "\n",
    "    Parameters:\n",
    "    - test_folder: Path to the folder containing the test JSON files.\n",
    "    - diffusion_model: Instance of the TextDiffusionModel.\n",
    "    - reverse_model: Instance of the ReverseProcessModel.\n",
    "    - tnet_model: Instance of the tNet model for generating embeddings.\n",
    "    - vocab_mapping: Dictionary mapping tokens to indices.\n",
    "    - seq_len: Length of the token sequence.\n",
    "    - device: Device to use (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "    - results: List of tuples (actual_formula, reconstructed_formula).\n",
    "    \"\"\"\n",
    "    reverse_vocab_mapping = {idx: token for token, idx in vocab_mapping.items()}\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file_name in os.listdir(test_folder):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(test_folder, file_name)\n",
    "\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "                tokens = tokenize_formula(formula_human_readable)\n",
    "\n",
    "                # Convert tokens to indices\n",
    "                token_indices = [vocab_mapping.get(token, 0) for token in tokens]\n",
    "\n",
    "                # Pad or truncate to seq_len\n",
    "                token_indices = token_indices[:seq_len] + [0] * max(0, seq_len - len(token_indices))\n",
    "                token_tensor = torch.tensor(token_indices, device=device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                points = data.get(\"points\")\n",
    "                if points:\n",
    "                    points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                    points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                    # Generate embeddings using tNet model\n",
    "                    embedding = tnet_model(points_tensor)\n",
    "\n",
    "                    # Choose random timestep\n",
    "                    t = torch.randint(0, 1000, (1,), device=device)\n",
    "\n",
    "                    # Add noise to the tokens\n",
    "                    noisy_tokens, _ = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "                    # Use reverse model to reconstruct the clean tokens\n",
    "                    reconstructed_noise = reverse_model(noisy_tokens, embedding, t)\n",
    "\n",
    "                    # Convert reconstructed noise to token indices\n",
    "                    reconstructed_tokens = torch.argmax(reconstructed_noise, dim=-1)\n",
    "\n",
    "                    # Ensure reconstructed_tokens is a list\n",
    "                    if reconstructed_tokens.dim() == 2:  # Case: (batch_size, seq_len)\n",
    "                        reconstructed_tokens = reconstructed_tokens.squeeze(0)  # Remove batch dimension\n",
    "                    elif reconstructed_tokens.dim() == 1:  # Case: (seq_len,)\n",
    "                        pass  # Already correct\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected shape for reconstructed_tokens: {reconstructed_tokens.shape}\")\n",
    "\n",
    "                    # Remove padding (0s) and EOS token\n",
    "                    reconstructed_tokens = reconstructed_tokens[reconstructed_tokens != 0]  # Remove PAD tokens\n",
    "                    reconstructed_tokens = reconstructed_tokens[reconstructed_tokens != 22]  # Remove EOS tokens\n",
    "\n",
    "                    # Map token indices back to tokens\n",
    "                    reconstructed_formula = \" \".join(\n",
    "                        reverse_vocab_mapping[idx] if idx in reverse_vocab_mapping else \"<UNK>\" for idx in reconstructed_tokens.tolist()\n",
    "                    )\n",
    "\n",
    "                    actual_formula = \" \".join(tokens)\n",
    "\n",
    "                    results.append((actual_formula, reconstructed_formula))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "folder_path_test = \"data_symbolic_regression/test\"\n",
    "\n",
    "# Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "tokenized_formulas_test = []\n",
    "points_list_test = []\n",
    "\n",
    "for file_name in os.listdir(folder_path_test):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            formula_human_readable_test = data.get(\"formula_human_readable\", \"\")\n",
    "            if formula_human_readable_test:\n",
    "                tokens_test = tokenize_formula(formula_human_readable_test)\n",
    "                tokenized_formulas_test.append(tokens_test)\n",
    "                \n",
    "            points_test = data.get(\"points\")\n",
    "            if points_test:\n",
    "                points_array_test = np.array([points_test[\"var_0\"], points_test[\"var_1\"], points_test[\"var_2\"], points_test[\"target\"]])\n",
    "                points_tensor_test = torch.tensor(points_array_test, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                points_list_test.append(points_tensor_test)\n",
    "                # Need below line if points_array is transposed\n",
    "                # points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0).permute(0, 2, 1)  # Add batch dimension and transpose\n",
    "\n",
    "# Create the vocabulary from the tokens\n",
    "# vocab_mapping_test = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas_test for t in tokens))}\n",
    "# vocab_size_test = len(vocab_mapping_test)\n",
    "\n",
    "# token_sequences_test = [[vocab_mapping_test[token] for token in tokens] for tokens in tokenized_formulas_test]\n",
    "\n",
    "# formula_lengths_test = [len(tokens) for tokens in tokenized_formulas_test]\n",
    "# seq_len_test = int(np.percentile(formula_lengths_test, 95))  # Use 95th percentile\n",
    "\n",
    "# # Initialize the model\n",
    "# diffusion_model_test = TextDiffusionModel(vocab_size_test, seq_len_test, device=device)\n",
    "    \n",
    "# # Pad or truncate sequences to seq_len\n",
    "# token_sequences_test = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in token_sequences_test]\n",
    "# token_tensor_test = torch.tensor(token_sequences_test, device=device)\n",
    "\n",
    "# # Choose random timesteps for each sequence\n",
    "# t = torch.randint(0, 1000, (len(token_tensor_test),), device=device)\n",
    "\n",
    "# # Configuration for tNet\n",
    "# num_vars_test = 3\n",
    "# embedding_size_test = 32  # Example embedding size\n",
    "# config_test = tNetConfig(num_vars=num_vars_test, embedding_size=embedding_size_test)\n",
    "\n",
    "# # Instantiate the model\n",
    "# tnet_model_test = tNet(config_test)\n",
    "\n",
    "# reverse_model = ReverseProcessModel(vocab_size_test, embedding_size_test, num_vars_test, seq_len_test).to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_diffusion_model(folder_path_test, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device)\n",
    "\n",
    "# Display example results\n",
    "example_idx = 1  # Index of the example to display\n",
    "\n",
    "if results:\n",
    "    actual, reconstructed = results[example_idx]\n",
    "    print(f\"Actual Formula: {actual}\")\n",
    "    print(f\"Reconstructed Formula: {reconstructed}\")\n",
    "\n",
    "# Calculate accuracy or similarity score (optional)\n",
    "# accuracies = [accuracy_score(list(actual), list(reconstructed)) for actual, reconstructed in results]\n",
    "# print(f\"Average Reconstruction Accuracy: {np.mean(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_diffusion_model(test_folder, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device):\n",
    "    \"\"\"\n",
    "    Evaluate the diffusion model on the test set.\n",
    "\n",
    "    Parameters:\n",
    "    - test_folder: Path to the folder containing the test JSON files.\n",
    "    - diffusion_model: Instance of the TextDiffusionModel.\n",
    "    - reverse_model: Instance of the ReverseProcessModel.\n",
    "    - tnet_model: Instance of the tNet model for generating embeddings.\n",
    "    - vocab_mapping: Dictionary mapping tokens to indices.\n",
    "    - seq_len: Length of the token sequence.\n",
    "    - device: Device to use (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "    - results: List of tuples (actual_formula, reconstructed_formula).\n",
    "    \"\"\"\n",
    "    reverse_vocab_mapping = {idx: token for token, idx in vocab_mapping.items()}\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file_name in os.listdir(test_folder):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(test_folder, file_name)\n",
    "\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "                tokens = tokenize_formula(formula_human_readable)\n",
    "\n",
    "                # Convert tokens to indices\n",
    "                token_indices = [vocab_mapping.get(token, 0) for token in tokens]\n",
    "\n",
    "                # Pad or truncate to seq_len\n",
    "                token_indices = token_indices[:seq_len] + [0] * max(0, seq_len - len(token_indices))\n",
    "                token_tensor = torch.tensor(token_indices, device=device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                points = data.get(\"points\")\n",
    "                if points:\n",
    "                    points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                    points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                    # Generate embeddings using tNet model\n",
    "                    embedding = tnet_model(points_tensor)\n",
    "\n",
    "                    # Choose random timestep\n",
    "                    t = torch.randint(0, 1000, (1,), device=device)\n",
    "\n",
    "                    # Add noise to the tokens\n",
    "                    noisy_tokens, _ = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "                    # Use reverse model to reconstruct the clean tokens\n",
    "                    reconstructed_noise = reverse_model(noisy_tokens, embedding, t)\n",
    "                    # print(f\"Reconstructed Noise Shape: {reconstructed_noise.shape}\")\n",
    "                    # Convert reconstructed noise to token indices\n",
    "                    # reconstructed_tokens = torch.argmax(reconstructed_noise, dim=-1).squeeze(0)\n",
    "\n",
    "                    # Ensure reconstructed_tokens is a list\n",
    "                    reconstructed_tokens = torch.argmax(reconstructed_noise, dim=-1)\n",
    "                    # print(reconstructed_tokens)\n",
    "                    if reconstructed_tokens.dim() == 2:  # Case: (batch_size, seq_len)\n",
    "                        reconstructed_tokens = reconstructed_tokens.squeeze(0)  # Remove batch dimension\n",
    "                    elif reconstructed_tokens.dim() == 1:  # Case: (seq_len,)\n",
    "                        pass  # Already correct\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected shape for reconstructed_tokens: {reconstructed_tokens.shape}\")\n",
    "\n",
    "                    # print(reconstructed_tokens)\n",
    "                    # Map token indices back to tokens\n",
    "                    reconstructed_formula = \" \".join(\n",
    "                        reverse_vocab_mapping[idx] if idx in reverse_vocab_mapping else \"<UNK>\" for idx in reconstructed_tokens.tolist()\n",
    "                    )\n",
    "                    \n",
    "                    actual_formula = \" \".join(tokens)\n",
    "\n",
    "                    results.append((actual_formula, reconstructed_formula))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "folder_path_test = \"data_symbolic_regression/test\"\n",
    "\n",
    "# Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "tokenized_formulas_test = []\n",
    "points_list_test = []\n",
    "\n",
    "for file_name in os.listdir(folder_path_test):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            formula_human_readable_test = data.get(\"formula_human_readable\", \"\")\n",
    "            if formula_human_readable_test:\n",
    "                tokens_test = tokenize_formula(formula_human_readable_test)\n",
    "                tokenized_formulas_test.append(tokens_test)\n",
    "                \n",
    "            points_test = data.get(\"points\")\n",
    "            if points_test:\n",
    "                points_array_test = np.array([points_test[\"var_0\"], points_test[\"var_1\"], points_test[\"var_2\"], points_test[\"target\"]])\n",
    "                points_tensor_test = torch.tensor(points_array_test, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                points_list_test.append(points_tensor_test)\n",
    "                # Need below line if points_array is transposed\n",
    "                # points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0).permute(0, 2, 1)  # Add batch dimension and transpose\n",
    "\n",
    "# Create the vocabulary from the tokens\n",
    "# vocab_mapping_test = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas_test for t in tokens))}\n",
    "# vocab_size_test = len(vocab_mapping_test)\n",
    "\n",
    "# token_sequences_test = [[vocab_mapping_test[token] for token in tokens] for tokens in tokenized_formulas_test]\n",
    "\n",
    "# formula_lengths_test = [len(tokens) for tokens in tokenized_formulas_test]\n",
    "# seq_len_test = int(np.percentile(formula_lengths_test, 95))  # Use 95th percentile\n",
    "\n",
    "# # Initialize the model\n",
    "# diffusion_model_test = TextDiffusionModel(vocab_size_test, seq_len_test, device=device)\n",
    "    \n",
    "# # Pad or truncate sequences to seq_len\n",
    "# token_sequences_test = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in token_sequences_test]\n",
    "# token_tensor_test = torch.tensor(token_sequences_test, device=device)\n",
    "\n",
    "# # Choose random timesteps for each sequence\n",
    "# t = torch.randint(0, 1000, (len(token_tensor_test),), device=device)\n",
    "\n",
    "# # Configuration for tNet\n",
    "# num_vars_test = 3\n",
    "# embedding_size_test = 32  # Example embedding size\n",
    "# config_test = tNetConfig(num_vars=num_vars_test, embedding_size=embedding_size_test)\n",
    "\n",
    "# # Instantiate the model\n",
    "# tnet_model_test = tNet(config_test)\n",
    "\n",
    "# reverse_model = ReverseProcessModel(vocab_size_test, embedding_size_test, num_vars_test, seq_len_test).to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_diffusion_model(folder_path_test, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device)\n",
    "\n",
    "# Display example results\n",
    "example_idx = 0  # Index of the example to display\n",
    "\n",
    "if results:\n",
    "    actual, reconstructed = results[example_idx]\n",
    "    print(f\"Actual Formula: {actual}\")\n",
    "    print(f\"Reconstructed Formula: {reconstructed}\")\n",
    "\n",
    "# Calculate accuracy or similarity score (optional)\n",
    "# accuracies = [accuracy_score(list(actual), list(reconstructed)) for actual, reconstructed in results]\n",
    "# print(f\"Average Reconstruction Accuracy: {np.mean(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data_symbolic_regression/train\"\n",
    "\n",
    "# Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "tokenized_formulas = []\n",
    "points_list = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "            if formula_human_readable:\n",
    "                tokens = tokenize_formula(formula_human_readable)\n",
    "                tokenized_formulas.append(tokens)\n",
    "                \n",
    "            points = data.get(\"points\")\n",
    "            if points:\n",
    "                points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                points_list.append(points_tensor)\n",
    "\n",
    "vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas for t in tokens))}\n",
    "vocab_size = len(vocab_mapping)\n",
    "\n",
    "# Define EOS and PAD token IDs\n",
    "eos_token_id = vocab_size - 1  # Assuming the last ID in the vocabulary is for EOS\n",
    "pad_token_id = vocab_size - 2  # Assuming the second-to-last ID in the vocabulary is for PAD\n",
    "\n",
    "print(eos_token_id, pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([747, 4, 100])\n",
      "Output shape: torch.Size([747, 128])\n",
      "Original Tokens shape: torch.Size([747, 24])\n",
      "Noisy Tokens (probabilities) shape: torch.Size([747, 24, 23])\n",
      "Sampled Tokens shape: torch.Size([747, 24])\n",
      "Epoch [1/1000], Loss: 0.0430, Val Loss: 0.0426\n",
      "Epoch [2/1000], Loss: 0.0325, Val Loss: 0.0351\n",
      "Epoch [3/1000], Loss: 0.0277, Val Loss: 0.0314\n",
      "Epoch [4/1000], Loss: 0.0251, Val Loss: 0.0292\n",
      "Epoch [5/1000], Loss: 0.0235, Val Loss: 0.0272\n",
      "Epoch [6/1000], Loss: 0.0222, Val Loss: 0.0265\n",
      "Epoch [7/1000], Loss: 0.0212, Val Loss: 0.0258\n",
      "Epoch [8/1000], Loss: 0.0207, Val Loss: 0.0253\n",
      "Epoch [9/1000], Loss: 0.0200, Val Loss: 0.0251\n",
      "Epoch [10/1000], Loss: 0.0198, Val Loss: 0.0251\n",
      "Epoch [11/1000], Loss: 0.0196, Val Loss: 0.0246\n",
      "Epoch [12/1000], Loss: 0.0190, Val Loss: 0.0242\n",
      "Epoch [13/1000], Loss: 0.0189, Val Loss: 0.0243\n",
      "Epoch [14/1000], Loss: 0.0185, Val Loss: 0.0239\n",
      "Epoch [15/1000], Loss: 0.0181, Val Loss: 0.0236\n",
      "Epoch [16/1000], Loss: 0.0180, Val Loss: 0.0233\n",
      "Epoch [17/1000], Loss: 0.0178, Val Loss: 0.0231\n",
      "Epoch [18/1000], Loss: 0.0176, Val Loss: 0.0229\n",
      "Epoch [19/1000], Loss: 0.0176, Val Loss: 0.0227\n",
      "Epoch [20/1000], Loss: 0.0175, Val Loss: 0.0229\n",
      "Epoch [21/1000], Loss: 0.0175, Val Loss: 0.0229\n",
      "Epoch [22/1000], Loss: 0.0174, Val Loss: 0.0227\n",
      "Epoch [23/1000], Loss: 0.0174, Val Loss: 0.0228\n",
      "Epoch [24/1000], Loss: 0.0174, Val Loss: 0.0228\n",
      "Epoch [25/1000], Loss: 0.0174, Val Loss: 0.0229\n",
      "Epoch [26/1000], Loss: 0.0174, Val Loss: 0.0227\n",
      "Epoch [27/1000], Loss: 0.0173, Val Loss: 0.0228\n",
      "Epoch [28/1000], Loss: 0.0173, Val Loss: 0.0228\n",
      "Epoch [29/1000], Loss: 0.0173, Val Loss: 0.0228\n",
      "Epoch [30/1000], Loss: 0.0173, Val Loss: 0.0229\n",
      "Epoch [31/1000], Loss: 0.0173, Val Loss: 0.0228\n",
      "Epoch [32/1000], Loss: 0.0173, Val Loss: 0.0230\n",
      "Early stopping triggered. Restoring best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19232\\3150425863.py:449: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reverse_model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import sympy\n",
    "\n",
    "# Define a function to tokenize a formula\n",
    "def tokenize_formula(formula):\n",
    "    token_pattern = r\"[a-zA-Z_][a-zA-Z0-9_]*|[()+\\-*/]|\\d+\\.?\\d*\"\n",
    "    tokens = re.findall(token_pattern, formula)\n",
    "    return tokens\n",
    "\n",
    "@dataclass\n",
    "class tNetConfig:\n",
    "    num_vars: int\n",
    "    embedding_size: int\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    def __init__(self, config: tNetConfig):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.num_vars = config.num_vars\n",
    "        self.n_embd = config.embedding_size\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "\n",
    "        self.conv1 = nn.Conv1d(self.num_vars + 1, self.n_embd, 1)\n",
    "        self.conv2 = nn.Conv1d(self.n_embd, 2 * self.n_embd, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.n_embd, 4 * self.n_embd, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * self.n_embd, 2 * self.n_embd)\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        self.input_batch_norm = nn.GroupNorm(1, self.num_vars + 1)\n",
    "\n",
    "        self.bn1 = nn.GroupNorm(1, self.n_embd)\n",
    "        self.bn2 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn3 = nn.GroupNorm(1, 4 * self.n_embd)\n",
    "        self.bn4 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn5 = nn.GroupNorm(1, self.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features + 1, #points]\n",
    "        :return: logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.n_embd\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class TextDiffusionModel:\n",
    "    def __init__(self, vocab_size, seq_len, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the text diffusion model.\n",
    "\n",
    "        Parameters:\n",
    "        - vocab_size: Size of the vocabulary (number of unique tokens).\n",
    "        - seq_len: Length of the token sequence.\n",
    "        - device: Device to use (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        # self.noise_schedule = torch.linspace(0.01, 0.1, steps=1000).to(device)  # Noise variance per timestep\n",
    "        self.noise_schedule = torch.linspace(1e-4, 2e-2, steps=1000).to(device)  # Noise variance per timestep\n",
    "\n",
    "    def add_noise(self, tokens, t):\n",
    "        \"\"\"\n",
    "        Add noise to a sequence of tokens based on timestep t.\n",
    "\n",
    "        Parameters:\n",
    "        - tokens: A tensor of token indices with shape (batch_size, seq_len).\n",
    "        - t: A tensor of timesteps with shape (batch_size,).\n",
    "\n",
    "        Returns:\n",
    "        - noisy_tokens: The tokens with added noise.\n",
    "        - noise: The noise added to the tokens.\n",
    "        \"\"\"\n",
    "        noise_std = self.noise_schedule[t].view(-1, 1, 1)  # Shape: (batch_size, 1, 1)\n",
    "\n",
    "        # Convert tokens to one-hot vectors\n",
    "        one_hot = F.one_hot(tokens.long(), num_classes=self.vocab_size).float()\n",
    "        \n",
    "        # Add Gaussian noise to the one-hot vectors\n",
    "        noise = torch.randn_like(one_hot) * noise_std\n",
    "        noisy_one_hot = one_hot + noise\n",
    "\n",
    "        # Compute softmax to normalize the noisy one-hot vectors\n",
    "        noisy_tokens = F.softmax(noisy_one_hot, dim=-1)\n",
    "        return noisy_tokens, noise\n",
    "\n",
    "    def sample_from_noisy_tokens(self, noisy_tokens):\n",
    "        \"\"\"\n",
    "        Sample discrete tokens from the noisy token distribution.\n",
    "\n",
    "        Parameters:\n",
    "        - noisy_tokens: A tensor of noisy token distributions with shape (batch_size, seq_len, vocab_size).\n",
    "\n",
    "        Returns:\n",
    "        - sampled_tokens: A tensor of sampled token indices with shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        sampled_tokens = torch.argmax(noisy_tokens, dim=-1)\n",
    "        return sampled_tokens\n",
    "\n",
    "class ReverseProcessModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_vars, seq_len):\n",
    "        super(ReverseProcessModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_vars = num_vars\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Calculate the correct input size for fc1\n",
    "        input_size = embedding_size + (seq_len * vocab_size) + 1  # embeddings + noisy_tokens + timestep\n",
    "\n",
    "        # Define layers for the reverse process model\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, seq_len * vocab_size)  # Output for all tokens in the sequence\n",
    "\n",
    "    def forward(self, noisy_tokens, embeddings, t):\n",
    "        \"\"\"\n",
    "        Forward pass for the reverse process model.\n",
    "\n",
    "        :param noisy_tokens: Tensor of noisy tokens with shape [batch_size, seq_len, vocab_size].\n",
    "        :param embeddings: Tensor of embeddings with shape [batch_size, embedding_size].\n",
    "        :param t: Tensor of timesteps with shape [batch_size].\n",
    "        :return: Predicted noise.\n",
    "        \"\"\"\n",
    "        # Flatten noisy tokens to [batch_size, seq_len * vocab_size]\n",
    "        noisy_tokens_flat = noisy_tokens.view(noisy_tokens.size(0), -1)\n",
    "\n",
    "        # Concatenate embeddings, flattened noisy tokens, and timestep information\n",
    "        timestep_embedding = torch.cat([embeddings, noisy_tokens_flat, t.unsqueeze(1).float()], dim=-1)\n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = F.relu(self.fc1(timestep_embedding))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        predicted_noise = self.fc3(x)\n",
    "\n",
    "        # Reshape to [batch_size, seq_len, vocab_size]\n",
    "        predicted_noise = predicted_noise.view(-1, self.seq_len, self.vocab_size)\n",
    "        \n",
    "        return predicted_noise\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    folder_path = \"data_symbolic_regression/train\"\n",
    "    val_folder_path = \"data_symbolic_regression/val\"\n",
    "\n",
    "    # Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "    tokenized_formulas = []\n",
    "    points_list = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "                if formula_human_readable:\n",
    "                    tokens = tokenize_formula(formula_human_readable)\n",
    "                    tokenized_formulas.append(tokens)\n",
    "                \n",
    "                points = data.get(\"points\")\n",
    "                if points:\n",
    "                    points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                    points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                    points_list.append(points_tensor)\n",
    "                    # Need below line if points_array is transposed\n",
    "                    # points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0).permute(0, 2, 1)  # Add batch dimension and transpose\n",
    "\n",
    "    # # Create the vocabulary from the tokens\n",
    "    # vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas for t in tokens))}\n",
    "    # vocab_size = len(vocab_mapping)\n",
    "\n",
    "    # token_sequences = [[vocab_mapping[token] for token in tokens] for tokens in tokenized_formulas]\n",
    "\n",
    "    # formula_lengths = [len(tokens) for tokens in tokenized_formulas]\n",
    "    # seq_len = int(np.percentile(formula_lengths, 95))  # Use 95th percentile\n",
    "\n",
    "    val_tokenized_formulas = []\n",
    "    val_points_list = []\n",
    "\n",
    "    # Create the vocabulary from the tokens\n",
    "    for file_name in os.listdir(val_folder_path):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(val_folder_path, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                val_data = json.load(file)\n",
    "\n",
    "                val_formula_human_readable = val_data.get(\"formula_human_readable\", \"\")\n",
    "                if val_formula_human_readable:\n",
    "                    val_tokens = tokenize_formula(val_formula_human_readable)\n",
    "                    val_tokenized_formulas.append(val_tokens)\n",
    "                \n",
    "                val_points = val_data.get(\"points\")\n",
    "                if val_points:\n",
    "                    val_points_array = np.array([val_points[\"var_0\"], val_points[\"var_1\"], val_points[\"var_2\"], val_points[\"target\"]])\n",
    "                    val_points_tensor = torch.tensor(val_points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                    val_points_list.append(val_points_tensor)\n",
    "\n",
    "    vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas for t in tokens))}\n",
    "    vocab_size = len(vocab_mapping)\n",
    "\n",
    "    # Define EOS and PAD token IDs\n",
    "    eos_token_id = vocab_size - 1  # Assuming the last ID in the vocabulary is for EOS\n",
    "    pad_token_id = vocab_size - 2  # Assuming the second-to-last ID in the vocabulary is for PAD\n",
    "\n",
    "    # Add EOS and PAD tokens to vocab_mapping if not already present\n",
    "    if eos_token_id not in vocab_mapping.values():\n",
    "        vocab_mapping['<EOS>'] = eos_token_id\n",
    "    if pad_token_id not in vocab_mapping.values():\n",
    "        vocab_mapping['<PAD>'] = pad_token_id\n",
    "\n",
    "    # Tokenize and map tokens to vocabulary indices\n",
    "    token_sequences = [[vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in tokenized_formulas]\n",
    "\n",
    "    # Calculate sequence length based on the 95th percentile of formula lengths\n",
    "    formula_lengths = [len(tokens) for tokens in tokenized_formulas]\n",
    "    seq_len = int(np.percentile(formula_lengths, 95))  # Use 95th percentile\n",
    "    batch_size = 100  # Example batch size\n",
    "\n",
    "    # Pad or truncate sequences to seq_len, adding EOS token last\n",
    "    token_sequences = [\n",
    "        seq[:seq_len] + [pad_token_id] * max(0, seq_len - len(seq)) + [eos_token_id] \n",
    "        if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "        for seq in token_sequences\n",
    "    ]\n",
    "\n",
    "    # Convert to tensor\n",
    "    token_tensor = torch.tensor(token_sequences, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    diffusion_model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "    \n",
    "    # Pad or truncate sequences to seq_len\n",
    "    token_sequences = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in token_sequences]\n",
    "    token_tensor = torch.tensor(token_sequences, device=device)\n",
    "\n",
    "    val_vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in val_tokenized_formulas for t in tokens))}\n",
    "    val_vocab_size = len(val_vocab_mapping)\n",
    "\n",
    "    # Add EOS and PAD tokens to vocab_mapping if not already present\n",
    "    if eos_token_id not in val_vocab_mapping.values():\n",
    "        val_vocab_mapping['<EOS>'] = eos_token_id\n",
    "    if pad_token_id not in val_vocab_mapping.values():\n",
    "        val_vocab_mapping['<PAD>'] = pad_token_id\n",
    "\n",
    "    # Tokenize and map tokens to vocabulary indices\n",
    "    val_token_sequences = [[val_vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in val_tokenized_formulas]\n",
    "\n",
    "    # Calculate sequence length based on the 95th percentile of formula lengths\n",
    "    val_formula_lengths = [len(tokens) for tokens in val_tokenized_formulas]\n",
    "    val_seq_len = int(np.percentile(val_formula_lengths, 100))  # Use 95th percentile\n",
    "\n",
    "    # Pad or truncate sequences to seq_len, adding EOS token last\n",
    "    val_token_sequences = [\n",
    "        seq[:seq_len] + [pad_token_id] * max(0, seq_len - len(seq)) + [eos_token_id] \n",
    "        if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "        for seq in val_token_sequences\n",
    "    ]\n",
    "\n",
    "    # Convert to tensor\n",
    "    val_token_tensor = torch.tensor(val_token_sequences, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    diffusion_model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "    \n",
    "    # Pad or truncate sequences to seq_len\n",
    "    val_token_sequences = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in val_token_sequences]\n",
    "    val_token_tensor = torch.tensor(val_token_sequences, device=device)\n",
    "\n",
    "    # # Add EOS token and pad sequences\n",
    "    # # Define EOS and PAD token IDs\n",
    "    # eos_token_id = vocab_size - 1  # Assuming the last ID in the vocabulary is for EOS\n",
    "    # pad_token_id = vocab_size - 2  # Assuming the second-to-last ID is for PAD\n",
    "\n",
    "    # # Add EOS token and pad sequences\n",
    "    # max_seq_len = max(len(seq) for seq in token_tensor)\n",
    "    # padded_token_tensor = []\n",
    "    # for seq in token_tensor:\n",
    "    #     seq = torch.cat([seq, torch.tensor([eos_token_id])])  # Add EOS token\n",
    "    #     padding = torch.tensor([pad_token_id] * (max_seq_len - len(seq)))  # Add padding\n",
    "    #     seq = torch.cat([seq, padding])  # Concatenate the sequence and padding\n",
    "    #     padded_token_tensor.append(seq)\n",
    "\n",
    "    # # Convert to a tensor\n",
    "    # token_tensor = torch.stack(padded_token_tensor)\n",
    "\n",
    "    # Choose random timesteps for each sequence\n",
    "    t = torch.randint(0, 1000, (len(token_tensor),), device=device)\n",
    "\n",
    "    # Add noise to the tokens\n",
    "    noisy_tokens, noise = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "    # Sample from noisy tokens\n",
    "    sampled_tokens = diffusion_model.sample_from_noisy_tokens(noisy_tokens)\n",
    "\n",
    "    # Configuration for tNet\n",
    "    num_vars = 3\n",
    "    embedding_size = 128  # Example embedding size\n",
    "    config = tNetConfig(num_vars=num_vars, embedding_size=embedding_size)\n",
    "\n",
    "    # Instantiate the model\n",
    "    tnet_model = tNet(config)\n",
    "\n",
    "    # Input: batch_size x (num_vars + 1) x num_points\n",
    "    batch_size = 1\n",
    "\n",
    "    # Generate embeddings\n",
    "    # input_tensor = torch.rand(batch_size, num_vars, 100)\n",
    "\n",
    "    output_embeddings = []\n",
    "    for pt in points_list:\n",
    "        output_embedding = tnet_model(pt)\n",
    "        output_embeddings.append(output_embedding)\n",
    "    \n",
    "    points_tensors = torch.cat(points_list, dim=0)\n",
    "    \n",
    "    output_embeddings_tensor = torch.cat(output_embeddings, dim=0)\n",
    "    # Print the output\n",
    "    print(\"Input shape:\", points_tensors.shape)\n",
    "    print(\"Output shape:\", output_embeddings_tensor.shape)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Original Tokens shape:\", token_tensor.shape)\n",
    "    print(\"Noisy Tokens (probabilities) shape:\", noisy_tokens.shape)\n",
    "    print(\"Sampled Tokens shape:\", sampled_tokens.shape)\n",
    "\n",
    "    # Initialize reverse model (denoiser)\n",
    "    reverse_model = ReverseProcessModel(vocab_size, embedding_size, num_vars, seq_len).to(device)\n",
    "\n",
    "    # Cross-entropy loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the reverse process model\n",
    "    epochs = 1000  # Define the number of epochs for training\n",
    "    batch_size = 100  # Example batch size\n",
    "\n",
    "    # Optimizer for the reverse process model\n",
    "    optimizer = torch.optim.Adam(reverse_model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Initialize lists to store training and validation losses\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        reverse_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training Phase\n",
    "        for batch_idx in range(0, len(points_list), batch_size):\n",
    "            batch_points = points_list[batch_idx:batch_idx + batch_size]\n",
    "            batch_token_tensor = token_tensor[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Random timesteps\n",
    "            t_batch = torch.randint(0, 1000, (len(batch_points),), device=device)\n",
    "\n",
    "            # Add noise\n",
    "            noisy_tokens, _ = diffusion_model.add_noise(batch_token_tensor, t_batch)\n",
    "\n",
    "            # Get embeddings\n",
    "            batch_embeddings = [tnet_model(pt) for pt in batch_points]\n",
    "            embeddings_tensor = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "            # Predict logits\n",
    "            logits = reverse_model(noisy_tokens, embeddings_tensor, t_batch)\n",
    "\n",
    "            # Reshape logits and target tokens for CrossEntropyLoss\n",
    "            logits_flat = logits.view(-1, vocab_size)\n",
    "            target_tokens = batch_token_tensor.view(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits_flat, target_tokens)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        reverse_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx in range(0, len(val_points_list), batch_size):\n",
    "                val_batch_points = val_points_list[val_batch_idx:val_batch_idx + batch_size]\n",
    "                val_batch_token_tensor = val_token_tensor[val_batch_idx:val_batch_idx + batch_size]\n",
    "\n",
    "                val_t_batch = torch.randint(0, 1000, (len(val_batch_points),), device=device)\n",
    "\n",
    "                val_noisy_tokens, _ = diffusion_model.add_noise(val_batch_token_tensor, val_t_batch)\n",
    "\n",
    "                val_embeddings = [tnet_model(pt) for pt in val_batch_points]\n",
    "                val_embeddings_tensor = torch.cat(val_embeddings, dim=0)\n",
    "\n",
    "                val_logits = reverse_model(val_noisy_tokens, val_embeddings_tensor, val_t_batch)\n",
    "                val_logits_flat = val_logits.view(-1, vocab_size)\n",
    "                val_target_tokens = val_batch_token_tensor.view(-1)\n",
    "\n",
    "                val_loss += loss_fn(val_logits_flat, val_target_tokens).item()\n",
    "\n",
    "        val_loss /= len(val_points_list)\n",
    "\n",
    "        # Store losses for plotting\n",
    "        training_losses.append(total_loss / len(points_list))\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {training_losses[-1]:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(reverse_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered. Restoring best model...\")\n",
    "            reverse_model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 272\u001b[0m\n\u001b[0;32m    269\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m, (\u001b[38;5;28mlen\u001b[39m(token_tensor),), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# Add noise to the tokens\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m noisy_tokens, noise \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Sample from noisy tokens\u001b[39;00m\n\u001b[0;32m    275\u001b[0m sampled_tokens \u001b[38;5;241m=\u001b[39m diffusion_model\u001b[38;5;241m.\u001b[39msample_from_noisy_tokens(noisy_tokens)\n",
      "Cell \u001b[1;32mIn[24], line 95\u001b[0m, in \u001b[0;36mTextDiffusionModel.add_noise\u001b[1;34m(self, tokens, t)\u001b[0m\n\u001b[0;32m     92\u001b[0m noise_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_schedule[t]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, 1, 1)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Convert tokens to one-hot vectors\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Add Gaussian noise to the one-hot vectors\u001b[39;00m\n\u001b[0;32m     98\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(one_hot) \u001b[38;5;241m*\u001b[39m noise_std\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to tokenize a formula\n",
    "def tokenize_formula(formula):\n",
    "    token_pattern = r\"[a-zA-Z_][a-zA-Z0-9_]*|[()+\\-*/]|\\d+\\.?\\d*\"\n",
    "    tokens = re.findall(token_pattern, formula)\n",
    "    return tokens\n",
    "\n",
    "@dataclass\n",
    "class tNetConfig:\n",
    "    num_vars: int\n",
    "    embedding_size: int\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    def __init__(self, config: tNetConfig):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.num_vars = config.num_vars\n",
    "        self.n_embd = config.embedding_size\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "\n",
    "        self.conv1 = nn.Conv1d(self.num_vars + 1, self.n_embd, 1)\n",
    "        self.conv2 = nn.Conv1d(self.n_embd, 2 * self.n_embd, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.n_embd, 4 * self.n_embd, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * self.n_embd, 2 * self.n_embd)\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        self.input_batch_norm = nn.GroupNorm(1, self.num_vars + 1)\n",
    "\n",
    "        self.bn1 = nn.GroupNorm(1, self.n_embd)\n",
    "        self.bn2 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn3 = nn.GroupNorm(1, 4 * self.n_embd)\n",
    "        self.bn4 = nn.GroupNorm(1, 2 * self.n_embd)\n",
    "        self.bn5 = nn.GroupNorm(1, self.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features + 1, #points]\n",
    "        :return: logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.n_embd\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class TextDiffusionModel:\n",
    "    def __init__(self, vocab_size, seq_len, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the text diffusion model.\n",
    "\n",
    "        Parameters:\n",
    "        - vocab_size: Size of the vocabulary (number of unique tokens).\n",
    "        - seq_len: Length of the token sequence.\n",
    "        - device: Device to use (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        # self.noise_schedule = torch.linspace(0.01, 0.1, steps=1000).to(device)  # Noise variance per timestep\n",
    "        self.noise_schedule = torch.linspace(1e-4, 2e-2, steps=1000).to(device)  # Noise variance per timestep\n",
    "\n",
    "    def add_noise(self, tokens, t):\n",
    "        \"\"\"\n",
    "        Add noise to a sequence of tokens based on timestep t.\n",
    "\n",
    "        Parameters:\n",
    "        - tokens: A tensor of token indices with shape (batch_size, seq_len).\n",
    "        - t: A tensor of timesteps with shape (batch_size,).\n",
    "\n",
    "        Returns:\n",
    "        - noisy_tokens: The tokens with added noise.\n",
    "        - noise: The noise added to the tokens.\n",
    "        \"\"\"\n",
    "        noise_std = self.noise_schedule[t].view(-1, 1, 1)  # Shape: (batch_size, 1, 1)\n",
    "\n",
    "        # Convert tokens to one-hot vectors\n",
    "        one_hot = F.one_hot(tokens.long(), num_classes=self.vocab_size).float()\n",
    "        \n",
    "        # Add Gaussian noise to the one-hot vectors\n",
    "        noise = torch.randn_like(one_hot) * noise_std\n",
    "        noisy_one_hot = one_hot + noise\n",
    "\n",
    "        # Compute softmax to normalize the noisy one-hot vectors\n",
    "        noisy_tokens = F.softmax(noisy_one_hot, dim=-1)\n",
    "        return noisy_tokens, noise\n",
    "\n",
    "    def sample_from_noisy_tokens(self, noisy_tokens):\n",
    "        \"\"\"\n",
    "        Sample discrete tokens from the noisy token distribution.\n",
    "\n",
    "        Parameters:\n",
    "        - noisy_tokens: A tensor of noisy token distributions with shape (batch_size, seq_len, vocab_size).\n",
    "\n",
    "        Returns:\n",
    "        - sampled_tokens: A tensor of sampled token indices with shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        sampled_tokens = torch.argmax(noisy_tokens, dim=-1)\n",
    "        return sampled_tokens\n",
    "\n",
    "class ReverseProcessModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_vars, seq_len):\n",
    "        super(ReverseProcessModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_vars = num_vars\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Calculate the correct input size for fc1\n",
    "        input_size = embedding_size + (seq_len*vocab_size) + 1 \n",
    "        #input_size = embedding_size + ((seq_len - 1)*(vocab_size-2)) + 1  # embeddings + noisy_tokens + timestep\n",
    "\n",
    "        # Define layers for the reverse process model\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, seq_len * vocab_size)  # Output for all tokens in the sequence\n",
    "\n",
    "    def forward(self, noisy_tokens, embeddings, t):\n",
    "        \"\"\"\n",
    "        Forward pass for the reverse process model.\n",
    "\n",
    "        :param noisy_tokens: Tensor of noisy tokens with shape [batch_size, seq_len, vocab_size].\n",
    "        :param embeddings: Tensor of embeddings with shape [batch_size, embedding_size].\n",
    "        :param t: Tensor of timesteps with shape [batch_size].\n",
    "        :return: Predicted noise.\n",
    "        \"\"\"\n",
    "        # Flatten noisy tokens to [batch_size, seq_len * vocab_size]\n",
    "        noisy_tokens_flat = noisy_tokens.view(noisy_tokens.size(0), -1)\n",
    "\n",
    "        # Concatenate embeddings, flattened noisy tokens, and timestep information\n",
    "        timestep_embedding = torch.cat([embeddings, noisy_tokens_flat, t.unsqueeze(1).float()], dim=-1)\n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = F.relu(self.fc1(timestep_embedding))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        predicted_noise = self.fc3(x)\n",
    "\n",
    "        # Reshape to [batch_size, seq_len, vocab_size]\n",
    "        predicted_noise = predicted_noise.view(-1, self.seq_len, self.vocab_size)\n",
    "        \n",
    "        return predicted_noise\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    folder_path = \"data_symbolic_regression/train\"\n",
    "    val_folder_path = \"data_symbolic_regression/val\"\n",
    "\n",
    "    # Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "    tokenized_formulas = []\n",
    "    all_tokenized_formulas = []\n",
    "    points_list = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "                if formula_human_readable:\n",
    "                    tokens = tokenize_formula(formula_human_readable)\n",
    "                    tokenized_formulas.append(tokens)\n",
    "                    all_tokenized_formulas.append(tokens)\n",
    "                \n",
    "                points = data.get(\"points\")\n",
    "                if points:\n",
    "                    points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                    points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                    points_list.append(points_tensor)\n",
    "                    # Need below line if points_array is transposed\n",
    "                    # points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0).permute(0, 2, 1)  # Add batch dimension and transpose\n",
    "\n",
    "    val_tokenized_formulas = []\n",
    "    val_points_list = []\n",
    "\n",
    "    # Create the vocabulary from the tokens\n",
    "    for file_name in os.listdir(val_folder_path):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(val_folder_path, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                val_data = json.load(file)\n",
    "\n",
    "                val_formula_human_readable = val_data.get(\"formula_human_readable\", \"\")\n",
    "                if val_formula_human_readable:\n",
    "                    val_tokens = tokenize_formula(val_formula_human_readable)\n",
    "                    val_tokenized_formulas.append(val_tokens)\n",
    "                    all_tokenized_formulas.append(val_tokens)\n",
    "                \n",
    "                val_points = val_data.get(\"points\")\n",
    "                if val_points:\n",
    "                    val_points_array = np.array([val_points[\"var_0\"], val_points[\"var_1\"], val_points[\"var_2\"], val_points[\"target\"]])\n",
    "                    val_points_tensor = torch.tensor(val_points_array, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                    val_points_list.append(val_points_tensor)\n",
    "\n",
    "    vocab_mapping = {token: idx for idx, token in enumerate(set(t for tokens in all_tokenized_formulas for t in tokens))}\n",
    "    vocab_size = len(vocab_mapping)\n",
    "\n",
    "    # Define EOS and PAD token IDs\n",
    "    eos_token_id = vocab_size  # Assuming the last ID in the vocabulary is for EOS\n",
    "    pad_token_id = vocab_size + 1 # Assuming the second-to-last ID in the vocabulary is for PAD\n",
    "\n",
    "    # Add EOS and PAD tokens to vocab_mapping if not already present\n",
    "    if eos_token_id not in vocab_mapping.values():\n",
    "        vocab_mapping['<EOS>'] = eos_token_id\n",
    "    if pad_token_id not in vocab_mapping.values():\n",
    "        vocab_mapping['<PAD>'] = pad_token_id\n",
    "    \n",
    "    # Tokenize and map tokens to vocabulary indices\n",
    "    token_sequences = [[vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in tokenized_formulas]\n",
    "\n",
    "    # Calculate sequence length based on the 95th percentile of formula lengths\n",
    "    formula_lengths = [len(tokens) for tokens in tokenized_formulas]\n",
    "    seq_len = int(np.percentile(formula_lengths, 95)) + 1  # Use 95th percentile\n",
    "    batch_size = 100  # Example batch size\n",
    "\n",
    "    # Pad or truncate sequences to seq_len, adding EOS token last\n",
    "    token_sequences = [\n",
    "        seq[:seq_len] + [pad_token_id]*max(0, seq_len - len(seq)) + [eos_token_id] \n",
    "        if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "        for seq in token_sequences\n",
    "    ]\n",
    "\n",
    "    # Convert to tensor\n",
    "    token_tensor = torch.tensor(token_sequences, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    diffusion_model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "\n",
    "    # Tokenize and map tokens to vocabulary indices\n",
    "    val_token_sequences = [[vocab_mapping.get(token, pad_token_id) for token in tokens] for tokens in val_tokenized_formulas]\n",
    "    \n",
    "    val_token_sequences = [\n",
    "        seq[:seq_len] + [eos_token_id] + [pad_token_id]*max(0, seq_len - len(seq))\n",
    "        if len(seq) < seq_len else seq[:seq_len] + [eos_token_id]  # Add EOS token at the end after padding\n",
    "        for seq in val_token_sequences\n",
    "    ]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    val_token_tensor = torch.tensor(val_token_sequences, device=device)\n",
    "\n",
    "    # Initialize the model\n",
    "    diffusion_model = TextDiffusionModel(vocab_size, seq_len, device=device)\n",
    "    \n",
    "    # Pad or truncate sequences to seq_len\n",
    "    val_token_sequences = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in val_token_sequences]\n",
    "    val_token_tensor = torch.tensor(val_token_sequences, device=device)\n",
    "\n",
    "    t = torch.randint(0, 1000, (len(token_tensor),), device=device)\n",
    "\n",
    "    # Add noise to the tokens\n",
    "    noisy_tokens, noise = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "    # Sample from noisy tokens\n",
    "    sampled_tokens = diffusion_model.sample_from_noisy_tokens(noisy_tokens)\n",
    "\n",
    "    # Configuration for tNet\n",
    "    num_vars = 3\n",
    "    embedding_size = 128  # Example embedding size\n",
    "    config = tNetConfig(num_vars=num_vars, embedding_size=embedding_size)\n",
    "\n",
    "    # Instantiate the model\n",
    "    tnet_model = tNet(config)\n",
    "\n",
    "    # Input: batch_size x (num_vars + 1) x num_points\n",
    "    batch_size = 1\n",
    "\n",
    "    # Generate embeddings\n",
    "    # input_tensor = torch.rand(batch_size, num_vars, 100)\n",
    "\n",
    "    output_embeddings = []\n",
    "    for pt in points_list:\n",
    "        output_embedding = tnet_model(pt)\n",
    "        output_embeddings.append(output_embedding)\n",
    "    \n",
    "    points_tensors = torch.cat(points_list, dim=0)\n",
    "    \n",
    "    output_embeddings_tensor = torch.cat(output_embeddings, dim=0)\n",
    "    # Print the output\n",
    "    print(\"Input shape:\", points_tensors.shape)\n",
    "    print(\"Output shape:\", output_embeddings_tensor.shape)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Original Tokens shape:\", token_tensor.shape)\n",
    "    print(\"Noisy Tokens (probabilities) shape:\", noisy_tokens.shape)\n",
    "    print(\"Sampled Tokens shape:\", sampled_tokens.shape)\n",
    "\n",
    "    # Initialize reverse model (denoiser)\n",
    "    reverse_model = ReverseProcessModel(vocab_size, embedding_size, num_vars, seq_len).to(device)\n",
    "\n",
    "    # Cross-entropy loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the reverse process model\n",
    "    epochs = 1000  # Define the number of epochs for training\n",
    "    batch_size = 100  # Example batch size\n",
    "\n",
    "    # Optimizer for the reverse process model\n",
    "    optimizer = torch.optim.Adam(reverse_model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Initialize lists to store training and validation losses\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        reverse_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training Phase\n",
    "        for batch_idx in range(0, len(points_list), batch_size):\n",
    "            batch_points = points_list[batch_idx:batch_idx + batch_size]\n",
    "            batch_token_tensor = token_tensor[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Random timesteps\n",
    "            t_batch = torch.randint(0, 1000, (len(batch_points),), device=device)\n",
    "\n",
    "            # Add noise\n",
    "            noisy_tokens, _ = diffusion_model.add_noise(batch_token_tensor, t_batch)\n",
    "\n",
    "            # Get embeddings\n",
    "            batch_embeddings = [tnet_model(pt) for pt in batch_points]\n",
    "            embeddings_tensor = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "            # Predict logits\n",
    "            logits = reverse_model(noisy_tokens, embeddings_tensor, t_batch)\n",
    "\n",
    "            # Reshape logits and target tokens for CrossEntropyLoss\n",
    "            logits_flat = logits.view(-1, vocab_size)\n",
    "            target_tokens = batch_token_tensor.view(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits_flat, target_tokens)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        reverse_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx in range(0, len(val_points_list), batch_size):\n",
    "                val_batch_points = val_points_list[val_batch_idx:val_batch_idx + batch_size]\n",
    "                val_batch_token_tensor = val_token_tensor[val_batch_idx:val_batch_idx + batch_size]\n",
    "\n",
    "                val_t_batch = torch.randint(0, 1000, (len(val_batch_points),), device=device)\n",
    "\n",
    "                val_noisy_tokens, _ = diffusion_model.add_noise(val_batch_token_tensor, val_t_batch)\n",
    "\n",
    "                val_embeddings = [tnet_model(pt) for pt in val_batch_points]\n",
    "                val_embeddings_tensor = torch.cat(val_embeddings, dim=0)\n",
    "\n",
    "                val_logits = reverse_model(val_noisy_tokens, val_embeddings_tensor, val_t_batch)\n",
    "                val_logits_flat = val_logits.view(-1, vocab_size)\n",
    "                val_target_tokens = val_batch_token_tensor.view(-1)\n",
    "\n",
    "                val_loss += loss_fn(val_logits_flat, val_target_tokens).item()\n",
    "\n",
    "        val_loss /= len(val_points_list)\n",
    "\n",
    "        # Store losses for plotting\n",
    "        training_losses.append(total_loss / len(points_list))\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {training_losses[-1]:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(reverse_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered. Restoring best model...\")\n",
    "            reverse_model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_diffusion_model(test_folder, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device):\n",
    "    \"\"\"\n",
    "    Evaluate the diffusion model on the test set.\n",
    "\n",
    "    Parameters:\n",
    "    - test_folder: Path to the folder containing the test JSON files.\n",
    "    - diffusion_model: Instance of the TextDiffusionModel.\n",
    "    - reverse_model: Instance of the ReverseProcessModel.\n",
    "    - tnet_model: Instance of the tNet model for generating embeddings.\n",
    "    - vocab_mapping: Dictionary mapping tokens to indices.\n",
    "    - seq_len: Length of the token sequence.\n",
    "    - device: Device to use (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "    - results: List of tuples (actual_formula, reconstructed_formula).\n",
    "    \"\"\"\n",
    "    reverse_vocab_mapping = {idx: token for token, idx in vocab_mapping.items()}\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file_name in os.listdir(test_folder):\n",
    "        if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "            file_path = os.path.join(test_folder, file_name)\n",
    "\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                formula_human_readable = data.get(\"formula_human_readable\", \"\")\n",
    "                tokens = tokenize_formula(formula_human_readable)\n",
    "\n",
    "                # Convert tokens to indices\n",
    "                token_indices = [vocab_mapping.get(token, 0) for token in tokens]\n",
    "\n",
    "                # Pad or truncate to seq_len\n",
    "                token_indices = token_indices[:seq_len] + [0] * max(0, seq_len - len(token_indices))\n",
    "                token_tensor = torch.tensor(token_indices, device=device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                points = data.get(\"points\")\n",
    "                if points:\n",
    "                    points_array = np.array([points[\"var_0\"], points[\"var_1\"], points[\"var_2\"], points[\"target\"]])\n",
    "                    points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                    # Generate embeddings using tNet model\n",
    "                    embedding = tnet_model(points_tensor)\n",
    "\n",
    "                    # Choose random timestep\n",
    "                    t = torch.randint(0, 1000, (1,), device=device)\n",
    "\n",
    "                    # Add noise to the tokens\n",
    "                    noisy_tokens, _ = diffusion_model.add_noise(token_tensor, t)\n",
    "\n",
    "                    # Use reverse model to reconstruct the clean tokens\n",
    "                    reconstructed_noise = reverse_model(noisy_tokens, embedding, t)\n",
    "                    # print(f\"Reconstructed Noise Shape: {reconstructed_noise.shape}\")\n",
    "                    # Convert reconstructed noise to token indices\n",
    "                    # reconstructed_tokens = torch.argmax(reconstructed_noise, dim=-1).squeeze(0)\n",
    "\n",
    "                    # Ensure reconstructed_tokens is a list\n",
    "                    reconstructed_tokens = torch.argmax(reconstructed_noise, dim=-1)\n",
    "                    # print(reconstructed_tokens)\n",
    "                    if reconstructed_tokens.dim() == 2:  # Case: (batch_size, seq_len)\n",
    "                        reconstructed_tokens = reconstructed_tokens.squeeze(0)  # Remove batch dimension\n",
    "                    elif reconstructed_tokens.dim() == 1:  # Case: (seq_len,)\n",
    "                        pass  # Already correct\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected shape for reconstructed_tokens: {reconstructed_tokens.shape}\")\n",
    "\n",
    "                    # print(reconstructed_tokens)\n",
    "                    # Map token indices back to tokens\n",
    "                    reconstructed_formula = \" \".join(\n",
    "                        reverse_vocab_mapping[idx] if idx in reverse_vocab_mapping else \"<UNK>\" for idx in reconstructed_tokens.tolist()\n",
    "                    )\n",
    "                    \n",
    "                    actual_formula = \" \".join(tokens)\n",
    "\n",
    "                    results.append((actual_formula, reconstructed_formula))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Formula: ( ( exp ( var_1 ) * var_2 ) + log ( ( var_2 * var_0 ) ) )\n",
      "Reconstructed Formula: ( ( ( ( ( ) ) ) ) * ) ( ) ( ( ) ) ) neg neg neg neg neg neg\n"
     ]
    }
   ],
   "source": [
    "# Define the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "folder_path_test = \"data_symbolic_regression/test\"\n",
    "\n",
    "# Load and tokenize formulas from the training set; Convert the data points to a Pytorch tensor\n",
    "tokenized_formulas_test = []\n",
    "points_list_test = []\n",
    "\n",
    "for file_name in os.listdir(folder_path_test):\n",
    "    if file_name.endswith(\".json\") and not file_name.startswith('properties'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            formula_human_readable_test = data.get(\"formula_human_readable\", \"\")\n",
    "            if formula_human_readable_test:\n",
    "                tokens_test = tokenize_formula(formula_human_readable_test)\n",
    "                tokenized_formulas_test.append(tokens_test)\n",
    "                \n",
    "            points_test = data.get(\"points\")\n",
    "            if points_test:\n",
    "                points_array_test = np.array([points_test[\"var_0\"], points_test[\"var_1\"], points_test[\"var_2\"], points_test[\"target\"]])\n",
    "                points_tensor_test = torch.tensor(points_array_test, dtype=torch.float32, device=device).unsqueeze(0)  # Add batch dimension\n",
    "                points_list_test.append(points_tensor_test)\n",
    "                # Need below line if points_array is transposed\n",
    "                # points_tensor = torch.tensor(points_array, dtype=torch.float32, device=device).unsqueeze(0).permute(0, 2, 1)  # Add batch dimension and transpose\n",
    "\n",
    "# Create the vocabulary from the tokens\n",
    "# vocab_mapping_test = {token: idx for idx, token in enumerate(set(t for tokens in tokenized_formulas_test for t in tokens))}\n",
    "# vocab_size_test = len(vocab_mapping_test)\n",
    "\n",
    "# token_sequences_test = [[vocab_mapping_test[token] for token in tokens] for tokens in tokenized_formulas_test]\n",
    "\n",
    "# formula_lengths_test = [len(tokens) for tokens in tokenized_formulas_test]\n",
    "# seq_len_test = int(np.percentile(formula_lengths_test, 95))  # Use 95th percentile\n",
    "\n",
    "# # Initialize the model\n",
    "# diffusion_model_test = TextDiffusionModel(vocab_size_test, seq_len_test, device=device)\n",
    "    \n",
    "# # Pad or truncate sequences to seq_len\n",
    "# token_sequences_test = [seq[:seq_len] + [0] * max(0, seq_len - len(seq)) for seq in token_sequences_test]\n",
    "# token_tensor_test = torch.tensor(token_sequences_test, device=device)\n",
    "\n",
    "# # Choose random timesteps for each sequence\n",
    "# t = torch.randint(0, 1000, (len(token_tensor_test),), device=device)\n",
    "\n",
    "# # Configuration for tNet\n",
    "# num_vars_test = 3\n",
    "# embedding_size_test = 32  # Example embedding size\n",
    "# config_test = tNetConfig(num_vars=num_vars_test, embedding_size=embedding_size_test)\n",
    "\n",
    "# # Instantiate the model\n",
    "# tnet_model_test = tNet(config_test)\n",
    "\n",
    "# reverse_model = ReverseProcessModel(vocab_size_test, embedding_size_test, num_vars_test, seq_len_test).to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_diffusion_model(folder_path_test, diffusion_model, reverse_model, tnet_model, vocab_mapping, seq_len, device)\n",
    "\n",
    "# Display example results\n",
    "example_idx = 2  # Index of the example to display\n",
    "\n",
    "if results:\n",
    "    actual, reconstructed = results[example_idx]\n",
    "    print(f\"Actual Formula: {actual}\")\n",
    "    print(f\"Reconstructed Formula: {reconstructed}\")\n",
    "\n",
    "# Calculate accuracy or similarity score (optional)\n",
    "# accuracies = [accuracy_score(list(actual), list(reconstructed)) for actual, reconstructed in results]\n",
    "# print(f\"Average Reconstruction Accuracy: {np.mean(accuracies):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT940_Final_Project_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
